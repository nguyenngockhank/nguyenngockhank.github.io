(window.webpackJsonp=window.webpackJsonp||[]).push([[53],{1316:function(e,t,a){"use strict";a.r(t);var r=a(7),s=Object(r.a)({},(function(){var e=this,t=e.$createElement,r=e._self._c||t;return r("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[r("h1",{attrs:{id:"design-typeahead-suggestion"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#design-typeahead-suggestion"}},[e._v("#")]),e._v(" Design Typeahead Suggestion")]),e._v(" "),r("p",[e._v("Let's design a real-time suggestion service, which will recommend terms to users as they enter text for\nsearching. Similar Services: Auto-suggestions, Typeahead search Difficulty: Medium")]),e._v(" "),r("h2",{attrs:{id:"_1-what-is-typeahead-suggestion"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-what-is-typeahead-suggestion"}},[e._v("#")]),e._v(" 1. What is Typeahead Suggestion?")]),e._v(" "),r("p",[e._v("Typeahead suggestions enable users to search for known and frequently searched terms. As the user\ntypes into the search box, it tries to predict the query based on the characters the user has entered and\ngives a list of suggestions to complete the query. Typeahead suggestions help the user to articulate their\nsearch queries better. It’s not about speeding up the search process but rather about guiding the users\nand lending them a helping hand in constructing their search query.")]),e._v(" "),r("h2",{attrs:{id:"_2-requirements-and-goals-of-the-system"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2-requirements-and-goals-of-the-system"}},[e._v("#")]),e._v(" 2. Requirements and Goals of the System")]),e._v(" "),r("p",[e._v("Functional Requirements: As the user types in their query, our service should suggest top 10 terms\nstarting with whatever the user has typed.\nNon-function Requirements: The suggestions should appear in real-time. The user should be able to\nsee the suggestions within 200ms.")]),e._v(" "),r("h2",{attrs:{id:"_3-basic-system-design-and-algorithm"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_3-basic-system-design-and-algorithm"}},[e._v("#")]),e._v(" 3. Basic System Design and Algorithm")]),e._v(" "),r("p",[e._v("The problem we are solving is that we have a lot of ‘strings’ that we need to store in such a way that\nusers can search with any prefix. Our service will suggest next terms that will match the given prefix.\nFor example, if our database contains the following terms: cap, cat, captain, or capital and the user has\ntyped in ‘cap’, our system should suggest ‘cap’, ‘captain’ and ‘capital’.")]),e._v(" "),r("p",[e._v("Since we’ve got to serve a lot of queries with minimum latency, we need to come up with a scheme that\ncan efficiently store our data such that it can be queried quickly. We can’t depend upon some database\nfor this; we need to store our index in memory in a highly efficient data structure.")]),e._v(" "),r("p",[e._v("One of the most appropriate data structures that can serve our purpose is the Trie (pronounced “try”). A\ntrie is a tree-like data structure used to store phrases where each node stores a character of the phrase in\na sequential manner. For example, if we need to store ‘cap, cat, caption, captain, capital’ in the trie, it\nwould look like:")]),e._v(" "),r("p",[r("img",{attrs:{src:a(808),alt:"img"}})]),e._v(" "),r("p",[e._v("Now if the user has typed ‘cap’, our service can traverse the trie to go to the node ‘P’ to find all the\nterms that start with this prefix (e.g., cap-tion, cap-ital etc).")]),e._v(" "),r("p",[e._v("We can merge nodes that have only one branch to save storage space. The above trie can be stored like\nthis:")]),e._v(" "),r("p",[r("img",{attrs:{src:a(809),alt:"img"}})]),e._v(" "),r("p",[r("strong",[e._v("Should we have case insensitive trie?")]),e._v(" For simplicity and search use-case, let’s assume our data is case\ninsensitive.")]),e._v(" "),r("p",[r("strong",[e._v("How to find top suggestion?")]),e._v(" Now that we can find all the terms given a prefix, how can we know\nwhat the top 10 terms are that we should suggest? One simple solution could be to store the count of\nsearches that terminated at each node, e.g., if users have searched about ‘CAPTAIN’ 100 times and\n‘CAPTION’ 500 times, we can store this number with the last character of the phrase. So now if the\nuser has typed ‘CAP’ we know the top most searched word under the prefix ‘CAP’ is ‘CAPTION’. So,\ngiven a prefix, we can traverse the sub-tree under it to find the top suggestions.")]),e._v(" "),r("p",[r("strong",[e._v("Given a prefix, how much time will it take to traverse its sub-tree?")]),e._v(" Given the amount of data we\nneed to index, we should expect a huge tree. Even traversing a sub-tree would take really long, e.g., the\nphrase ‘system design interview questions’ is 30 levels deep. Since we have very strict latency\nrequirements we do need to improve the efficiency of our solution.")]),e._v(" "),r("p",[r("strong",[e._v("Can we store top suggestions with each node?")]),e._v(" This can surely speed up our searches but will require\na lot of extra storage. We can store top 10 suggestions at each node that we can return to the user. We\nhave to bear the big increase in our storage capacity to achieve the required efficiency.")]),e._v(" "),r("p",[e._v("We can optimize our storage by storing only references of the terminal nodes rather than storing the\nentire phrase. To find the suggested terms we need to traverse back using the parent reference from the\nterminal node. We will also need to store the frequency with each reference to keep track of top\nsuggestions.")]),e._v(" "),r("p",[r("strong",[e._v("How would we build this trie?")]),e._v(" We can efficiently build our trie bottom up. Each parent node will\nrecursively call all the child nodes to calculate their top suggestions and their counts. Parent nodes will\ncombine top suggestions from all of their children to determine their top suggestions.")]),e._v(" "),r("p",[r("strong",[e._v("How to update the trie?")]),e._v(" Assuming five billion searches every day, which would give us\napproximately 60K queries per second. If we try to update our trie for every query it’ll be extremely\nresource intensive and this can hamper our read requests, too. One solution to handle this could be to\nupdate our trie offline after a certain interval.")]),e._v(" "),r("p",[e._v("As the new queries come in we can log them and also track their frequencies. Either we can log every\nquery or do sampling and log every 1000th query. For example, if we don’t want to show a term which\nis searched for less than 1000 times, it’s safe to log every 1000th searched term.")]),e._v(" "),r("p",[e._v("We can have a "),r("a",{attrs:{href:"https://en.wikipedia.org/wiki/MapReduce",target:"_blank",rel:"noopener noreferrer"}},[e._v("Map-Reduce (MR)"),r("OutboundLink")],1),e._v(" set-up to process all the logging data periodically say every hour.\nThese MR jobs will calculate frequencies of all searched terms in the past hour. We can then update our\ntrie with this new data. We can take the current snapshot of the trie and update it with all the new terms\nand their frequencies. We should do this offline as we don’t want our read queries to be blocked by\nupdate trie requests. We can have two options:")]),e._v(" "),r("ol",[r("li",[e._v("We can make a copy of the trie on each server to update it offline. Once done we can switch to\nstart using it and discard the old one.")]),e._v(" "),r("li",[e._v("Another option is we can have a master-slave configuration for each trie server. We can update\nslave while the master is serving traffic. Once the update is complete, we can make the slave our\nnew master. We can later update our old master, which can then start serving traffic, too.")])]),e._v(" "),r("p",[e._v("How can we update the frequencies of typeahead suggestions? Since we are storing frequencies of\nour typeahead suggestions with each node, we need to update them too. We can update only differences\nin frequencies rather than recounting all search terms from scratch. If we’re keeping count of all the\nterms searched in last 10 days, we’ll need to subtract the counts from the time period no longer\nincluded and add the counts for the new time period being included. We can add and subtract\nfrequencies based on "),r("a",{attrs:{href:"https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average",target:"_blank",rel:"noopener noreferrer"}},[e._v("Exponential Moving Average (EMA)"),r("OutboundLink")],1),e._v(" of each term. In EMA, we give more weight\nto the latest data. It’s also known as the exponentially weighted moving average.")]),e._v(" "),r("p",[e._v("After inserting a new term in the trie, we’ll go to the terminal node of the phrase and increase its\nfrequency. Since we’re storing the top 10 queries in each node, it is possible that this particular search\nterm jumped into the top 10 queries of a few other nodes. So, we need to update the top 10 queries of\nthose nodes then. We have to traverse back from the node to all the way up to the root. For every\nparent, we check if the current query is part of the top 10. If so, we update the corresponding frequency.\nIf not, we check if the current query’s frequency is high enough to be a part of the top 10. If so, we\ninsert this new term and remove the term with the lowest frequency.")]),e._v(" "),r("p",[r("strong",[e._v("How can we remove a term from the trie?")]),e._v(" Let’s say we have to remove a term from the trie because\nof some legal issue or hate or piracy etc. We can completely remove such terms from the trie when the\nregular update happens, meanwhile, we can add a filtering layer on each server which will remove any\nsuch term before sending them to users.")]),e._v(" "),r("p",[r("strong",[e._v("What could be different ranking criteria for suggestions?")]),e._v(" In addition to a simple count, for terms\nranking, we have to consider other factors too, e.g., freshness, user location, language, demographics,\npersonal history etc.")]),e._v(" "),r("h2",{attrs:{id:"_4-permanent-storage-of-the-trie"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_4-permanent-storage-of-the-trie"}},[e._v("#")]),e._v(" 4. Permanent Storage of the Trie")]),e._v(" "),r("p",[r("strong",[e._v("How to store trie in a file so that we can rebuild our trie easily - this will be needed when a machine restarts?")]),e._v(" We can take a snapshot of our trie periodically and store it in a file. This will enable\nus to rebuild a trie if the server goes down. To store, we can start with the root node and save the trie\nlevel-by-level. With each node, we can store what character it contains and how many children it has.\nRight after each node, we should put all of its children. Let’s assume we have the following trie:")]),e._v(" "),r("p",[r("img",{attrs:{src:a(810),alt:"img"}})]),e._v(" "),r("p",[e._v("If we store this trie in a file with the above-mentioned scheme, we will have: “C2,A2,R1,T,P,O1,D”.\nFrom this, we can easily rebuild our trie.")]),e._v(" "),r("p",[e._v("If you’ve noticed, we are not storing top suggestions and their counts with each node. It is hard to store\nthis information; as our trie is being stored top down, we don’t have child nodes created before the\nparent, so there is no easy way to store their references. For this, we have to recalculate all the top\nterms with counts. This can be done while we are building the trie. Each node will calculate its top\nsuggestions and pass it to its parent. Each parent node will merge results from all of its children to\nfigure out its top suggestions.")]),e._v(" "),r("h2",{attrs:{id:"_5-scale-estimation"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_5-scale-estimation"}},[e._v("#")]),e._v(" 5. Scale Estimation")]),e._v(" "),r("p",[e._v("If we are building a service that has the same scale as that of Google we can expect 5 billion searches\nevery day, which would give us approximately 60K queries per second.")]),e._v(" "),r("p",[e._v("Since there will be a lot of duplicates in 5 billion queries, we can assume that only 20% of these will be\nunique. If we only want to index the top 50% of the search terms, we can get rid of a lot of less\nfrequently searched queries. Let’s assume we will have 100 million unique terms for which we want to\nbuild an index.")]),e._v(" "),r("p",[e._v("Storage Estimation: If on the average each query consists of 3 words and if the average length of a\nword is 5 characters, this will give us 15 characters of average query size. Assuming we need 2 bytes to\nstore a character, we will need 30 bytes to store an average query. So total storage we will need:")]),e._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",{pre:!0,attrs:{class:"language-text"}},[r("code",[e._v("100 million * 30 bytes => 3 GB\n")])])]),r("p",[e._v("We can expect some growth in this data every day, but we should also be removing some terms that are\nnot searched anymore. If we assume we have 2% new queries every day and if we are maintaining our\nindex for the last one year, total storage we should expect:")]),e._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",{pre:!0,attrs:{class:"language-text"}},[r("code",[e._v("3GB + (0.02 * 3 GB * 365 days) => 25 GB\n")])])]),r("h2",{attrs:{id:"_6-data-partition"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_6-data-partition"}},[e._v("#")]),e._v(" 6. Data Partition")]),e._v(" "),r("p",[e._v("Although our index can easily fit on one server, we can still partition it in order to meet our\nrequirements of higher efficiency and lower latencies. How can we efficiently partition our data to\ndistribute it onto multiple servers?")]),e._v(" "),r("p",[r("strong",[e._v("a. Range Based Partitioning")]),e._v(": What if we store our phrases in separate partitions based on their first\nletter. So we save all the terms starting with the letter ‘A’ in one partition and those that start with the\nletter ‘B’ into another partition and so on. We can even combine certain less frequently occurring letters\ninto one database partition. We should come up with this partitioning scheme statically so that we can\nalways store and search terms in a predictable manner.")]),e._v(" "),r("p",[e._v("The main problem with this approach is that it can lead to unbalanced servers, for instance, if we decide\nto put all terms starting with the letter ‘E’ into a DB partition, but later we realize that we have too\nmany terms that start with letter ‘E’ that we can’t fit into one DB partition.")]),e._v(" "),r("p",[e._v("We can see that the above problem will happen with every statically defined scheme. It is not possible\nto calculate if each of our partitions will fit on one server statically.")]),e._v(" "),r("p",[r("strong",[e._v("b. Partition based on the maximum capacity of the server:")]),e._v(" Let’s say we partition our trie based on\nthe maximum memory capacity of the servers. We can keep storing data on a server as long as it has\nmemory available. Whenever a sub-tree cannot fit into a server, we break our partition there to assign\nthat range to this server and move on the next server to repeat this process. Let’s say if our first trie\nserver can store all terms from ‘A’ to ‘AABC’, which mean our next server will store from ‘AABD’\nonwards. If our second server could store up to ‘BXA’, the next server will start from ‘BXB’, and so\non. We can keep a hash table to quickly access this partitioning scheme:\nServer 1, A-AABC"),r("br"),e._v("\nServer 2, AABD-BXA"),r("br"),e._v("\nServer 3, BXB-CDA")]),e._v(" "),r("p",[e._v("For querying, if the user has typed ‘A’ we have to query both server 1 and 2 to find the top suggestions.\nWhen the user has typed ‘AA’, we still have to query server 1 and 2, but when the user has typed\n‘AAA’ we only need to query server 1.")]),e._v(" "),r("p",[e._v("We can have a load balancer in front of our trie servers which can store this mapping and redirect\ntraffic. Also, if we are querying from multiple servers, either we need to merge the results at the server\nside to calculate overall top results or make our clients do that. If we prefer to do this on the server side,\nwe need to introduce another layer of servers between load balancers and trie severs (let’s call them\naggregator). These servers will aggregate results from multiple trie servers and return the top results to\nthe client.")]),e._v(" "),r("p",[e._v("Partitioning based on the maximum capacity can still lead us to hotspots, e.g., if there are a lot of\nqueries for terms starting with ‘cap’, the server holding it will have a high load compared to others.")]),e._v(" "),r("p",[r("strong",[e._v("c. Partition based on the hash of the term")]),e._v(": Each term will be passed to a hash function, which will\ngenerate a server number and we will store the term on that server. This will make our term distribution\nrandom and hence minimize hotspots. To find typeahead suggestions for a term we have to ask all the\nservers and then aggregate the results.")]),e._v(" "),r("h2",{attrs:{id:"_7-cache"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_7-cache"}},[e._v("#")]),e._v(" 7. Cache")]),e._v(" "),r("p",[e._v("We should realize that caching the top searched terms will be extremely helpful in our service. There\nwill be a small percentage of queries that will be responsible for most of the traffic. We can have\nseparate cache servers in front of the trie servers holding most frequently searched terms and their\ntypeahead suggestions. Application servers should check these cache servers before hitting the trie\nservers to see if they have the desired searched terms.")]),e._v(" "),r("p",[e._v("We can also build a simple Machine Learning (ML) model that can try to predict the engagement on\neach suggestion based on simple counting, personalization, or trending data etc., and cache these terms.")]),e._v(" "),r("h2",{attrs:{id:"_8-replication-and-load-balancer"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_8-replication-and-load-balancer"}},[e._v("#")]),e._v(" 8. Replication and Load Balancer")]),e._v(" "),r("p",[e._v("We should have replicas for our trie servers both for load balancing and also for fault tolerance. We\nalso need a load balancer that keeps track of our data partitioning scheme and redirects traffic based on\nthe prefixes.")]),e._v(" "),r("h2",{attrs:{id:"_9-fault-tolerance"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_9-fault-tolerance"}},[e._v("#")]),e._v(" 9. Fault Tolerance")]),e._v(" "),r("p",[e._v("What will happen when a trie server goes down? As discussed above we can have a master-slave\nconfiguration; if the master dies, the slave can take over after failover. Any server that comes back up,\ncan rebuild the trie based on the last snapshot.")]),e._v(" "),r("h2",{attrs:{id:"_10-typeahead-client"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_10-typeahead-client"}},[e._v("#")]),e._v(" 10. Typeahead Client")]),e._v(" "),r("p",[e._v("We can perform the following optimizations on the client to improve user’s experience:")]),e._v(" "),r("ol",[r("li",[e._v("The client should only try hitting the server if the user has not pressed any key for 50ms.")]),e._v(" "),r("li",[e._v("If the user is constantly typing, the client can cancel the in-progress requests.")]),e._v(" "),r("li",[e._v("Initially, the client can wait until the user enters a couple of characters.")]),e._v(" "),r("li",[e._v("Clients can pre-fetch some data from the server to save future requests.")]),e._v(" "),r("li",[e._v("Clients can store the recent history of suggestions locally. Recent history has a very high rate of being reused.")]),e._v(" "),r("li",[e._v("Establishing an early connection with the server turns out to be one of the most important factors. As soon as the user opens the search engine website, the client can open a connection with the server. So when a user types in the first character, the client doesn’t waste time in establishing the connection.")]),e._v(" "),r("li",[e._v("The server can push some part of their cache to CDNs and Internet Service Providers (ISPs) for efficiency.")])]),e._v(" "),r("h2",{attrs:{id:"_11-personalization"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_11-personalization"}},[e._v("#")]),e._v(" 11. Personalization")]),e._v(" "),r("p",[e._v("Users will receive some typeahead suggestions based on their historical searches, location, language,\netc. We can store the personal history of each user separately on the server and cache them on the client\ntoo. The server can add these personalized terms in the final set before sending it to the user.\nPersonalized searches should always come before others.")])])}),[],!1,null,null,null);t.default=s.exports},808:function(e,t,a){e.exports=a.p+"assets/img/15.95e89a0c.png"},809:function(e,t,a){e.exports=a.p+"assets/img/16.c7f13aae.png"},810:function(e,t,a){e.exports=a.p+"assets/img/17.d637127a.png"}}]);