(window.webpackJsonp=window.webpackJsonp||[]).push([[36],{1628:function(e,t,a){"use strict";a.r(t);var s=a(7),n=Object(s.a)({},(function(){var e=this,t=e.$createElement,s=e._self._c||t;return s("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[s("h1",{attrs:{id:"design-dynamo"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#design-dynamo"}},[e._v("#")]),e._v(" Design Dynamo")]),e._v(" "),s("p",[e._v("Dynamo is a "),s("strong",[e._v("highly available key-value store")]),e._v(" developed by Amazon for their internal use. Many Amazon services, such as shopping cart, bestseller lists, sales rank, product catalog, etc., need only primary-key access to data. A multi-table relational database system would be an overkill for such services and would also limit scalability and availability. Dynamo provides a flexible design to let applications choose their desired level of availability and consistency.")]),e._v(" "),s("h2",{attrs:{id:"overview"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#overview"}},[e._v("#")]),e._v(" Overview")]),e._v(" "),s("h3",{attrs:{id:"goal"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#goal"}},[e._v("#")]),e._v(" Goal")]),e._v(" "),s("p",[e._v("Design a distributed key-value store that is highly available (i.e., reliable), highly scalable, and completely decentralized.")]),e._v(" "),s("h3",{attrs:{id:"background"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#background"}},[e._v("#")]),e._v(" Background")]),e._v(" "),s("p",[e._v("Dynamo – not to be confused with DynamoDB, which was inspired by Dynamo’s design – is a distributed key-value storage system that provides an “always-on” (or highly available) experience at a massive scale. In "),s("a",{attrs:{href:"https://www.educative.io/collection/page/5668639101419520/5559029852536832/5998984290631680",target:"_blank",rel:"noopener noreferrer"}},[e._v("CAP theorem"),s("OutboundLink")],1),e._v(" terms, Dynamo "),s("strong",[e._v("falls within the category of AP systems")]),e._v(" (i.e., available and partition tolerant) and is designed for "),s("strong",[e._v("high availability and partition tolerance at the expense of strong consistency.")])]),e._v(" "),s("p",[e._v("The primary motivation for designing Dynamo as a highly available system was the observation that the availability of a system directly correlates to the number of customers served. Therefore, the main goal is that the system, even when it is imperfect, should be available to the customer as it brings more customer satisfaction. On the other hand, inconsistencies can be resolved in the background, and most of the time they will not be noticeable by the customer. Derived from this core principle, Dynamo is aggressively\noptimized for availability.")]),e._v(" "),s("p",[e._v("The Dynamo design was highly influential as it inspired many NoSQL databases, like "),s("a",{attrs:{href:"https://cassandra.apache.org/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Cassandra"),s("OutboundLink")],1),e._v(", "),s("a",{attrs:{href:"https://riak.com/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Riak"),s("OutboundLink")],1),e._v(", and "),s("a",{attrs:{href:"http://www.projectvoldemort.com/voldemort/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Voldemort"),s("OutboundLink")],1),e._v(" – not to mention Amazon’s own "),s("a",{attrs:{href:"https://aws.amazon.com/dynamodb/",target:"_blank",rel:"noopener noreferrer"}},[e._v("DynamoDB"),s("OutboundLink")],1),e._v(".")]),e._v(" "),s("h3",{attrs:{id:"design-goals"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#design-goals"}},[e._v("#")]),e._v(" Design goals")]),e._v(" "),s("p",[e._v("As stated above, the main goal of Dynamo is to be highly available. Here is\nthe summary of its other design goals:")]),e._v(" "),s("ul",[s("li",[s("strong",[e._v("Scalable")]),e._v(": The system should be "),s("strong",[s("em",[e._v("highly scalable")])]),e._v(". We should be able to throw a machine into the system to see proportional improvement.")]),e._v(" "),s("li",[s("strong",[e._v("Decentralized")]),e._v(": To avoid single points of failure and performance bottlenecks, there should not be any central/leader process.")]),e._v(" "),s("li",[s("strong",[e._v("Eventually Consistent")]),e._v(": Data can be "),s("strong",[s("em",[e._v("optimistically replicated")])]),e._v(" to become eventually consistent. This means that instead of incurring write-time costs to ensure data correctness throughout the system (i.e., strong consistency), inconsistencies can be resolved at some other time (e.g., during reads). Eventual consistency is used to achieve high availability.")])]),e._v(" "),s("p",[s("img",{attrs:{src:a(799),alt:"High level design"}})]),e._v(" "),s("h3",{attrs:{id:"dynamo-s-use-cases"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#dynamo-s-use-cases"}},[e._v("#")]),e._v(" Dynamo’s use cases")]),e._v(" "),s("p",[e._v("By default, Dynamo is an eventually consistent database. Therefore, any\napplication where strong consistency is not a concern can utilize Dynamo.\nThough Dynamo can support strong consistency, it comes with a\nperformance impact. Hence, if strong consistency is a requirement for an\napplication, then Dynamo might not be a good option.")]),e._v(" "),s("p",[e._v("Dynamo is used at Amazon to manage services that have very highreliability requirements and need tight control over the trade-offs between\n"),s("strong",[e._v("availability")]),e._v(", "),s("strong",[e._v("consistency")]),e._v(", "),s("strong",[e._v("cost-effectiveness")]),e._v(", and "),s("strong",[e._v("performance")]),e._v(". Amazon’s\nplatform has a very diverse set of applications with different storage\nrequirements. Many applications chose Dynamo because of its flexibility for\nselecting the appropriate trade-offs to achieve high availability and\nguaranteed performance in the most cost-effective manner.")]),e._v(" "),s("p",[e._v("Many services on Amazon’s platform require only primary-key access to a\ndata store. For such services, the common pattern of using a relational\ndatabase would lead to inefficiencies and limit scalability and availability.\nDynamo provides a simple primary-key only interface to meet the\nrequirements of these applications.")]),e._v(" "),s("h3",{attrs:{id:"system-apis"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#system-apis"}},[e._v("#")]),e._v(" System APIs")]),e._v(" "),s("p",[e._v("The Dynamo clients use "),s("code",[e._v("put()")]),e._v(" and "),s("code",[e._v("get()")]),e._v(" operations to write and read data\ncorresponding to a specified key. This key uniquely identifies an object.")]),e._v(" "),s("ul",[s("li",[s("code",[e._v("get(key)")]),e._v(" : The "),s("code",[e._v("get")]),e._v(" operation finds the nodes where the object associated with the given key is located and returns either a single\nobject or a list of objects with conflicting versions along with a "),s("em",[e._v("context")]),e._v(". The "),s("em",[e._v("context")]),e._v(" contains encoded metadata about the object that is meaningless to the caller and includes information such as the version of the object (more on this below).")]),e._v(" "),s("li",[s("code",[e._v("put(key, context, object)")]),e._v(" : The "),s("code",[e._v("put")]),e._v(" operation finds the nodes where the object associated with the given key should be stored and writes the given object to the disk. The context is a value that is returned with a "),s("code",[e._v("get")]),e._v(" operation and then sent back with the "),s("code",[e._v("put")]),e._v(" operation. The context is always stored along with the object and is used like a cookie to verify the validity of the object supplied in the put request.")])]),e._v(" "),s("p",[e._v("Dynamo treats both the object and the key as an arbitrary array of bytes\n(typically less than 1 MB). It applies the MD5 hashing algorithm on the key to\ngenerate a 128-bit identifier which is used to determine the storage nodes\nthat are responsible for serving the key.")]),e._v(" "),s("h2",{attrs:{id:"high-level-architecture"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#high-level-architecture"}},[e._v("#")]),e._v(" High-level Architecture")]),e._v(" "),s("p",[e._v("At a high level, Dynamo is a "),s("strong",[e._v("Distributed Hash Table")]),e._v(" (DHT) that is replicated across the cluster for high availability and fault tolerance.")]),e._v(" "),s("h3",{attrs:{id:"data-distribution"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#data-distribution"}},[e._v("#")]),e._v(" Data distribution")]),e._v(" "),s("p",[e._v("Dynamo uses "),s("strong",[e._v("Consistent Hashing")]),e._v(" to distribute its data among nodes. Consistent hashing also makes it easy to add or remove nodes from a\nDynamo cluster.")]),e._v(" "),s("p",[e._v("Data is replicated optimistically, i.e., Dynamo provides eventual consistency.")]),e._v(" "),s("h3",{attrs:{id:"handling-temporary-failures"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#handling-temporary-failures"}},[e._v("#")]),e._v(" Handling temporary failures")]),e._v(" "),s("p",[e._v("To handle temporary failures, Dynamo replicates data to a "),s("strong",[e._v("sloppy quorum")]),e._v(" of other nodes in the system instead of a strict majority quorum.")]),e._v(" "),s("h3",{attrs:{id:"inter-node-communication-and-failure-detection"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#inter-node-communication-and-failure-detection"}},[e._v("#")]),e._v(" Inter-node communication and failure detection")]),e._v(" "),s("p",[e._v("Dynamo’s nodes use "),s("strong",[e._v("gossip protocol")]),e._v(" to keep track of the cluster state.")]),e._v(" "),s("h3",{attrs:{id:"high-availability"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#high-availability"}},[e._v("#")]),e._v(" High availability")]),e._v(" "),s("p",[e._v("Dynamo makes the system “always writeable” (or highly available) by using "),s("strong",[e._v("hinted handoff")])]),e._v(" "),s("h3",{attrs:{id:"conflict-resolution-and-handling-permanent-failures"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#conflict-resolution-and-handling-permanent-failures"}},[e._v("#")]),e._v(" Conflict resolution and handling permanent failures")]),e._v(" "),s("p",[e._v("Since there are no write-time guarantees that nodes agree on values, Dynamo resolves potential conflicts using other mechanisms:")]),e._v(" "),s("ul",[s("li",[e._v("Use "),s("strong",[e._v("vector clocks")]),e._v(" to keep track of value history and reconcile divergent histories at read time.")]),e._v(" "),s("li",[e._v("In the background, dynamo uses an anti-entropy mechanism like "),s("strong",[e._v("Merkle trees")]),e._v(" to handle permanent failures.")])]),e._v(" "),s("h2",{attrs:{id:"data-partition"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#data-partition"}},[e._v("#")]),e._v(" Data Partition")]),e._v(" "),s("p",[e._v("The act of distributing data across a set of nodes is called data partitioning.\nThere are two challenges when we try to distribute data:")]),e._v(" "),s("ol",[s("li",[e._v("How do we know on which node a particular piece of data will be stored?")]),e._v(" "),s("li",[e._v("When we add or remove nodes, how do we know what data will be moved from existing nodes to the new nodes? Furthermore, how can we minimize data movement when nodes join or leave?")])]),e._v(" "),s("h3",{attrs:{id:"consistent-hashing-dynamo-s-data-distribution"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#consistent-hashing-dynamo-s-data-distribution"}},[e._v("#")]),e._v(" Consistent hashing: Dynamo’s data distribution")]),e._v(" "),s("p",[e._v("Consistent hashing represents the data managed by a cluster as a ring. Each node in the ring is assigned a range of data. Dynamo uses the consistent hashing algorithm to determine what row is stored to what node. Here is an example of the consistent hashing ring:")]),e._v(" "),s("p",[s("img",{attrs:{src:a(800),alt:"rign"}})]),e._v(" "),s("p",[e._v("With consistent hashing, the ring is divided into smaller predefined ranges.")]),e._v(" "),s("p",[e._v("Each node is assigned one of these ranges. In Dynamo’s terminology, the start  of the range is called a token. This means that each node will be assigned one token. The range assigned to each node is computed as follows:")]),e._v(" "),s("ul",[s("li",[s("strong",[e._v("Range start")]),e._v(":  Token value")]),e._v(" "),s("li",[s("strong",[e._v("Range end")]),e._v(":    Next token value - 1")])]),e._v(" "),s("p",[e._v("Here are the tokens and data ranges of the four nodes described in the above\ndiagram:")]),e._v(" "),s("table",[s("thead",[s("tr",[s("th",[e._v("Server")]),e._v(" "),s("th",[e._v("Token")]),e._v(" "),s("th",[e._v("Range Start")]),e._v(" "),s("th",[e._v("Range End")])])]),e._v(" "),s("tbody",[s("tr",[s("td",[e._v("Server 1")]),e._v(" "),s("td",[e._v("1")]),e._v(" "),s("td",[e._v("1")]),e._v(" "),s("td",[e._v("25")])]),e._v(" "),s("tr",[s("td",[e._v("Server 2")]),e._v(" "),s("td",[e._v("26")]),e._v(" "),s("td",[e._v("26")]),e._v(" "),s("td",[e._v("50")])]),e._v(" "),s("tr",[s("td",[e._v("Server 3")]),e._v(" "),s("td",[e._v("51")]),e._v(" "),s("td",[e._v("51")]),e._v(" "),s("td",[e._v("75")])]),e._v(" "),s("tr",[s("td",[e._v("Server 4")]),e._v(" "),s("td",[e._v("76")]),e._v(" "),s("td",[e._v("76")]),e._v(" "),s("td",[e._v("100")])])])]),e._v(" "),s("p",[e._v("Whenever Dynamo is serving a "),s("code",[e._v("put()")]),e._v(" or a "),s("code",[e._v("get()")]),e._v(" request, the first step it performs is to apply the "),s("strong",[e._v("MD5 hashing algorithm")]),e._v(" to the key. The output of this hashing algorithm determines within which range the data lies and hence,\non which node the data will be stored. As we saw above, each node in Dynamo is supposed to store data for a fixed range. Hence, the hash generated from the data key tells us the node where the data will be stored.")]),e._v(" "),s("h3",{attrs:{id:"virtual-nodes"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#virtual-nodes"}},[e._v("#")]),e._v(" Virtual nodes")]),e._v(" "),s("p",[e._v("Adding and removing nodes in any distributed system is quite common.\nExisting nodes can die and may need to be decommissioned. Similarly, new\nnodes may be added to an existing cluster to meet growing demands.\nDynamo efficiently handles these scenarios through the use of virtual nodes\n(or Vnodes).")]),e._v(" "),s("p",[e._v("As we saw above, the basic Consistent Hashing algorithm assigns a single\ntoken (or a consecutive hash range) to each physical node. This was a static\ndivision of ranges that requires calculating tokens based on a given number\nof nodes. This scheme made adding or replacing a node an expensive\noperation, as, in this case, we would like to rebalance and distribute the data\nto all other nodes, resulting in moving a lot of data. Here are a few potential\nissues associated with a manual and fixed division of the ranges:")]),e._v(" "),s("ul",[s("li",[s("strong",[e._v("Adding or removing nodes")]),e._v(": Adding or removing nodes will result in recomputing the tokens causing a significant administrative overhead for a large cluster.")]),e._v(" "),s("li",[s("strong",[e._v("Hotspots")]),e._v(": Since each node is assigned one large range, if the data is not  evenly distributed, some nodes can become hotspots.")]),e._v(" "),s("li",[s("strong",[e._v("Node rebuilding")]),e._v(": Since each node’s data is replicated on a "),s("em",[e._v("fixed number of nodes")]),e._v(", when we need to rebuild a node, only its replica nodes can provide the data. This puts a lot of pressure on the replica nodes and can lead to service degradation.")])]),e._v(" "),s("p",[e._v("To handle these issues, Dynamo introduced a new scheme for distributing\nthe tokens to physical nodes. Instead of assigning a single token to a node,\nthe hash range is divided into multiple smaller ranges, and each physical\nnode is assigned multiple of these smaller ranges. Each of these subranges is\ncalled a Vnode. With Vnodes, instead of a node being responsible for just one\ntoken, it is responsible for many tokens (or subranges).")]),e._v(" "),s("p",[s("img",{attrs:{src:a(801),alt:"vnodes"}})]),e._v(" "),s("p",[e._v("Practically, Vnodes are "),s("strong",[e._v("randomly distributed")]),e._v(" across the cluster and are generally "),s("strong",[e._v("non-contiguous")]),e._v(" so that no two neighboring Vnodes are assigned to the same physical node. Furthermore, nodes do carry replicas of other nodes for fault-tolerance. Also, since there can be heterogeneous machines in the clusters, some servers might hold more Vnodes than others. The figure below shows how physical nodes A, B, C, D, & E are using Vnodes of the Consistent Hash ring. Each physical node is assigned a set of Vnodes and each Vnode is replicated once.")]),e._v(" "),s("h3",{attrs:{id:"advantages-of-vnodes"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#advantages-of-vnodes"}},[e._v("#")]),e._v(" Advantages of Vnodes")]),e._v(" "),s("ol",[s("li",[s("p",[e._v("Vnodes help "),s("strong",[e._v("spread the load more evenly")]),e._v(" across the physical nodes on the cluster by dividing the hash ranges into smaller subranges. This speeds up the rebalancing process after adding or removing nodes.\nWhen a new node is added, it receives many Vnodes from the existing\nnodes to maintain a balanced cluster. Similarly, when a node needs to\nbe rebuilt, instead of getting data from a fixed number of replicas, many\nnodes participate in the rebuild process.")])]),e._v(" "),s("li",[s("p",[e._v("Vnodes make it easier to "),s("strong",[e._v("maintain a cluster containing heterogeneous machines")]),e._v(". This means, with Vnodes, we can assign a high number of ranges to a powerful server and a lower number of\nranges to a less powerful server.")])]),e._v(" "),s("li",[s("p",[e._v("Since Vnodes help assign smaller ranges to each physical node, the "),s("strong",[e._v("probability of hotspots is much less")]),e._v(" than the basic Consistent Hashing scheme which uses one big range per node.")])])]),e._v(" "),s("h2",{attrs:{id:"replication"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#replication"}},[e._v("#")]),e._v(" Replication")]),e._v(" "),s("h3",{attrs:{id:"optimistic-replication"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#optimistic-replication"}},[e._v("#")]),e._v(" Optimistic replication?")]),e._v(" "),s("p",[e._v("To ensure high availability and durability, Dynamo replicates each data item\non multiple N nodes in the system where the value N is equivalent to the\n"),s("em",[e._v("replication factor")]),e._v(" and is configurable per instance of Dynamo. Each key is\nassigned to a "),s("strong",[e._v("coordinator node")]),e._v(" (the node that falls first in the hash range),\nwhich first stores the data locally and then replicates it to N − 1 clockwise\nsuccessor nodes on the ring. This results in each node owning the region on\nthe ring between it and its Nth predecessor. This replication is done\nasynchronously (in the background), and Dynamo provides an "),s("strong",[e._v("eventually consistent")]),e._v(" model. This replication technique is called "),s("strong",[e._v("optimistic replication")]),e._v(", which means that replicas are not guaranteed to be identical at\nall times.")]),e._v(" "),s("p",[s("img",{attrs:{src:a(802),alt:"replica"}})]),e._v(" "),s("p",[e._v("Each node in Dynamo serves as a replica for a different range of data. As Dynamo stores N copies of data spread across different nodes, if one node is down, other replicas can respond to queries for that range of data. If a client\ncannot contact the coordinator node, it sends the request to a node holding a replica.")]),e._v(" "),s("h3",{attrs:{id:"preference-list"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#preference-list"}},[e._v("#")]),e._v(" Preference List")]),e._v(" "),s("p",[e._v("The list of nodes responsible for storing a particular key is called the\npreference list. Dynamo is designed so that every node in the system can\ndetermine which nodes should be in this list for any specific key. This list contains more than N nodes to account for failure and skip\nvirtual nodes on the ring so that the list only contains distinct physical nodes.")]),e._v(" "),s("h3",{attrs:{id:"sloppy-quorum-and-handling-of-temporary-failures"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#sloppy-quorum-and-handling-of-temporary-failures"}},[e._v("#")]),e._v(" Sloppy quorum and handling of temporary failures")]),e._v(" "),s("p",[e._v("Following traditional quorum approaches, any distributed system becomes unavailable during server failures or network partitions and would have reduced availability even under simple failure conditions. To increase the\navailability, Dynamo does not enforce strict quorum requirements, and\ninstead uses something called "),s("strong",[e._v("sloppy quorum")]),e._v(". With this approach, all read/write operations are performed on the first N healthy nodes from the preference list, which may not always be the first N nodes encountered\nwhile moving clockwise on the consistent hashing ring.")]),e._v(" "),s("h4",{attrs:{id:"example"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#example"}},[e._v("#")]),e._v(" Example")]),e._v(" "),s("p",[e._v("Consider the example of Dynamo configuration given in the figure below with "),s("strong",[e._v("N = 3")]),e._v(".")]),e._v(" "),s("p",[e._v("In this example, if "),s("code",[e._v("Server 1")]),e._v(" is temporarily down or unreachable during a write operation, its data will now be stored on "),s("code",[e._v("Server 4")]),e._v(" . Thus, Dynamo transfers the replica stored on the failing node (i.e., "),s("code",[e._v("Server 1")]),e._v(" ) to the next node of the consistent hash ring that does not have the replica (i.e., "),s("code",[e._v("Server 4")]),e._v(").")]),e._v(" "),s("p",[e._v("This is done to avoid unavailability caused by a short-term machine or network failure and to maintain desired availability and durability guarantees.")]),e._v(" "),s("p",[e._v("The replica sent to "),s("code",[e._v("Server 4")]),e._v(" will have a hint in its metadata that suggests which node was the intended recipient of the replica (in this case, "),s("code",[e._v("Server 1")]),e._v(" ).")]),e._v(" "),s("p",[e._v("Nodes that receive hinted replicas will keep them in a separate local database that is scanned periodically. Upon detecting that "),s("code",[e._v("Server 1")]),e._v(" has recovered, "),s("code",[e._v("Server 4")]),e._v(" will attempt to deliver the replica to "),s("code",[e._v("Server 1")]),e._v(" .")]),e._v(" "),s("p",[e._v("Once the transfer succeeds, "),s("code",[e._v("Server 4")]),e._v(" may delete the object from its local store without decreasing the total number of replicas in the system.")]),e._v(" "),s("h3",{attrs:{id:"hinted-handoff"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#hinted-handoff"}},[e._v("#")]),e._v(" Hinted handoff")]),e._v(" "),s("p",[e._v("The interesting trick described above to increase availability is known as\nhinted handoff, i.e., "),s("strong",[e._v("when a node is unreachable, another node can accept writes on its behalf")]),e._v(". The write is then kept in a local buffer and sent out once the destination node is reachable again. This makes Dynamo "),s("strong",[e._v("always writeable.")]),e._v(". Thus, even in the extreme case where only a single node is alive, write requests will still get accepted and eventually processed.")]),e._v(" "),s("p",[e._v("The main problem is that since a sloppy quorum is not a strict majority, the data can and will diverge, i.e., it is possible for two concurrent writes to the same key to be accepted by non-overlapping sets of nodes. This means that multiple conflicting values against the same key can exist in the system, and we can get stale or conflicting data while reading. Dynamo allows this and resolves these conflicts using Vector Clocks.")]),e._v(" "),s("h2",{attrs:{id:"vector-clocks-and-conicting-data"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#vector-clocks-and-conicting-data"}},[e._v("#")]),e._v(" Vector Clocks and Conicting Data")]),e._v(" "),s("h3",{attrs:{id:"clock-skew"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#clock-skew"}},[e._v("#")]),e._v(" Clock Skew")]),e._v(" "),s("p",[e._v("On a single machine, all we need to know about is the absolute or wall clock time: suppose we perform a write to key "),s("code",[e._v("k")]),e._v(" with timestamp "),s("code",[e._v("t1")]),e._v(" and then perform another write to "),s("code",[e._v("k")]),e._v(" with timestamp "),s("code",[e._v("t2")]),e._v(" . Since "),s("code",[e._v("t2 > t1")]),e._v(" , the second write must have been newer than the first write, and therefore the database can safely overwrite the original value.")]),e._v(" "),s("p",[e._v("In a distributed system, this assumption does not hold. The problem is "),s("strong",[e._v("clock skew")]),e._v(", i.e., different clocks tend to run at different rates, so we cannot assume that time "),s("code",[e._v("t")]),e._v(" on node a happened before time "),s("code",[e._v("t + 1")]),e._v(" on node b . The most practical techniques that help with synchronizing clocks, like NTP, still\ndo not guarantee that every clock in a distributed system is synchronized at\nall times. So, without special hardware like GPS units and atomic clocks, just\nusing wall clock timestamps is not enough.")]),e._v(" "),s("h3",{attrs:{id:"vector-clock"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#vector-clock"}},[e._v("#")]),e._v(" Vector Clock")]),e._v(" "),s("p",[e._v("Instead of employing tight synchronization mechanics, Dynamo uses\nsomething called "),s("strong",[e._v("vector clock")]),e._v(" in order to capture "),s("strong",[e._v("causality between different versions of the same object")]),e._v(". A vector clock is effectively a "),s("code",[e._v("(node, counter)")]),e._v(" pair. One vector clock is associated with every version of every object stored in Dynamo. One can determine whether two versions of an object are on parallel branches or have a causal ordering by examining their vector clocks. If the counters on the first object’s clock are less-than-or-equal to all of the nodes in the second clock, then the first is an ancestor of the second and can be forgotten. Otherwise, the two changes are considered to\nbe in conflict and require reconciliation. Dynamo resolves these conflicts at read-time. Let’s understands this with an example:")]),e._v(" "),s("p",[s("img",{attrs:{src:a(803),alt:"img"}})]),e._v(" "),s("ol",[s("li",[e._v("Server A serves a write to key "),s("code",[e._v("k1")]),e._v(" , with value "),s("code",[e._v("foo")]),e._v(" . It assigns it a version of "),s("code",[e._v("[A:1]")]),e._v(". This write gets replicated to server B .")]),e._v(" "),s("li",[e._v("Server A serves a write to key "),s("code",[e._v("k1")]),e._v(" , with value "),s("code",[e._v("bar")]),e._v(" . It assigns it a version of "),s("code",[e._v("[A:2]")]),e._v(". This write also gets replicated to server B .")]),e._v(" "),s("li",[e._v("A network partition occurs. A and B cannot talk to each other.")]),e._v(" "),s("li",[e._v("Server A serves a write to key "),s("code",[e._v("k1")]),e._v(" , with value "),s("code",[e._v("baz")]),e._v(" . It assigns it a version of "),s("code",[e._v("[A:3]")]),e._v(". It cannot replicate it to server B , but it gets stored in a hinted handoff buffer on another server.")]),e._v(" "),s("li",[e._v("Server B sees a write to key "),s("code",[e._v("k1")]),e._v(" , with value "),s("code",[e._v("bax")]),e._v(" . It assigns it a version of "),s("code",[e._v("[B:1]")]),e._v(". It cannot replicate it to server A , but it gets stored in a hinted handoff buffer on another server.")]),e._v(" "),s("li",[e._v("The network heals. Server A and B can talk to each other again.")]),e._v(" "),s("li",[e._v("Either server gets a read request for key "),s("code",[e._v("k1")]),e._v(" . It sees the same key with different versions "),s("code",[e._v("[A:3]")]),e._v(" and "),s("code",[e._v("[A:2]")]),e._v(" "),s("code",[e._v("[B:1]")]),e._v(" , but it does not know which one is newer. It returns both and tells the client to figure out the version and write the newer version back into the system.")])]),e._v(" "),s("p",[e._v("As we saw in the above example, most of the time, new versions subsume the previous version(s), and the system itself can determine the correct version (e.g., "),s("code",[e._v("[A:2]")]),e._v(" is newer than "),s("code",[e._v("[A:1]")]),e._v(" ). However, version branching may happen in the presence of failures combined with concurrent updates, resulting in conflicting versions of an object. In these cases, the system\ncannot reconcile the multiple versions of the same object, and the client must perform the reconciliation to collapse multiple branches of data evolution back into one (this process is called semantic reconciliation). A typical example of a collapse operation is “merging” different versions of a customer’s shopping cart. Using this reconciliation mechanism, an add operation (i.e., adding an item to the cart) is never lost. However, deleted items can resurface.")]),e._v(" "),s("div",{staticClass:"custom-block tip"},[s("p",{staticClass:"custom-block-title"},[e._v("TIP")]),e._v(" "),s("p",[e._v("Resolving conflicts is similar to how Git works. If Git can merge different versions into one, merging is done automatically. If not, the client (i.e., the developer) has to reconcile conflicts manually.")])]),e._v(" "),s("p",[e._v("Dynamo truncates vector clocks ("),s("em",[e._v("oldest first")]),e._v(") when they grow too large. If Dynamo ends up deleting older vector clocks that are required to reconcile an object’s state, Dynamo would not be able to achieve eventual consistency. Dynamo’s authors note that this is a potential problem but do not specify how this may be addressed. They do mention that this problem has not yet\nsurfaced in any of their production systems.")]),e._v(" "),s("h3",{attrs:{id:"conflict-free-replicated-data-types-crdts"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#conflict-free-replicated-data-types-crdts"}},[e._v("#")]),e._v(" Conflict-free replicated data types (CRDTs)")]),e._v(" "),s("p",[e._v("A more straightforward way to handle conflicts is through the use of "),s("strong",[e._v("CRDTs")]),e._v(". To make use of CRDTs, we need to model our data in such a way that concurrent changes can be applied to the data in any order and will produce the same end result. This way, the system does not need to worry about any ordering guarantees. Amazon’s shopping cart is an excellent example of\nCRDT. When a user adds two items (A & B) to the cart, these two operations of adding A & B can be done on any node and with any order, as the end result is the two items are added to the cart. ("),s("em",[e._v("Removing from the shopping cart is modeled as a negative add")]),e._v("). The idea that any two nodes that have received the same set of updates will see the same end result is called strong "),s("strong",[e._v("eventual consistency")]),e._v(". "),s("strong",[e._v("Riak")]),e._v(" has a few built-in "),s("a",{attrs:{href:"https://docs.riak.com/riak/kv/2.2.0/developing/data-types/",target:"_blank",rel:"noopener noreferrer"}},[e._v("CRDTs"),s("OutboundLink")],1),e._v(".")]),e._v(" "),s("h3",{attrs:{id:"last-write-wins-lww"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#last-write-wins-lww"}},[e._v("#")]),e._v(" Last-write-wins (LWW)")]),e._v(" "),s("p",[e._v("Unfortunately, it is not easy to model the data as CRDTs. In many cases, it\ninvolves too much effort. Therefore, vector clocks with client-side resolution\nare considered good enough.")]),e._v(" "),s("p",[e._v("Instead of vector clocks, Dynamo also offers ways to resolve the conflicts\nautomatically on the server-side. Dynamo (and Apache Cassandra) often uses\na simple conflict resolution policy: "),s("strong",[e._v("last-write-wins (LWW)")]),e._v(", based on the\nwall-clock timestamp. LWW can easily end up losing data. For example, if\ntwo conflicting writes happen simultaneously, it is equivalent to flipping a\ncoin on which write to throw away.")]),e._v(" "),s("h2",{attrs:{id:"the-life-of-put-get-operations"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#the-life-of-put-get-operations"}},[e._v("#")]),e._v(" The Life of put() & get() Operations")]),e._v(" "),s("h3",{attrs:{id:"strategies-for-choosing-the-coordinator-node"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#strategies-for-choosing-the-coordinator-node"}},[e._v("#")]),e._v(" Strategies for choosing the coordinator node")]),e._v(" "),s("p",[e._v("Dynamo clients can use one of the two strategies to choose a node for their get() and put() requests:")]),e._v(" "),s("ul",[s("li",[e._v("Clients can route their requests through a generic load balancer.")]),e._v(" "),s("li",[e._v("Clients can use a partition-aware client library that routes the requests to the appropriate coordinator nodes with lower latency")])]),e._v(" "),s("p",[s("img",{attrs:{src:a(804),alt:"img"}})]),e._v(" "),s("p",[e._v("In the first case, the load balancer decides which way the request would be\nrouted, while in the second strategy, the client selects the node to contact.\nBoth approaches are beneficial in their own ways.")]),e._v(" "),s("p",[e._v("In the first strategy, the client is unaware of the Dynamo ring, which helps scalability and makes Dynamo’s architecture "),s("strong",[e._v("loosely coupled")]),e._v(". However, in this case, since the load balancer can forward the request to any node in the\nring, it is possible that the node it selects is not part of the preference list.\nThis will result in an extra hop, as the request will then be forwarded to one of the nodes in the preference list by the intermediate node.")]),e._v(" "),s("p",[e._v("The second strategy helps in achieving "),s("strong",[e._v("lower latency")]),e._v(", as in this case, the\nclient maintains a copy of the ring and forwards the request to an\nappropriate node from the preference list. Because of this option, Dynamo is\nalso called a "),s("strong",[e._v("zero-hop DHT")]),e._v(", as the client can directly contact the node that\nholds the required data. However, in this case, Dynamo does not have much\ncontrol over the load distribution and request handling")]),e._v(" "),s("h3",{attrs:{id:"consistency-protocol"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#consistency-protocol"}},[e._v("#")]),e._v(" Consistency protocol")]),e._v(" "),s("p",[e._v("Dynamo uses a consistency protocol similar to quorum systems. If R/W is the minimum number of nodes that must participate in a successful read/write operation respectively:")]),e._v(" "),s("ul",[s("li",[e._v("Then "),s("code",[e._v("R + W > N")]),e._v(" yields a quorum-like system")]),e._v(" "),s("li",[e._v("A Common (N, R, W) configuration used by Dynamo is (3, 2, 2).\n"),s("ul",[s("li",[e._v("(3, 3, 1): fast W, slow R, not very durable")]),e._v(" "),s("li",[e._v("(3, 1, 3): fast R, slow W, durable")])])]),e._v(" "),s("li",[e._v("In this model, the latency of a get() (or put() ) operation depends upon the slowest of the replicas. For this reason, R and W are usually configured to be less than N to provide better latency.")]),e._v(" "),s("li",[e._v("In general, low values of W and R increase the risk of inconsistency, as write requests are deemed successful and returned to the clients even if a majority of replicas have not processed them. This also introduces a vulnerability window for durability when a write request is successfully returned to the client even though it has been persisted at only a small number of nodes.")]),e._v(" "),s("li",[e._v("For both Read and Write operations, the requests are forwarded to the first ‘N’ healthy nodes.")])]),e._v(" "),s("h3",{attrs:{id:"put-process"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#put-process"}},[e._v("#")]),e._v(" ‘put()’ process")]),e._v(" "),s("ol",[s("li",[e._v("The coordinator generates a new data version and vector clock component.")]),e._v(" "),s("li",[e._v("Saves new data locally.")]),e._v(" "),s("li",[e._v("Sends the write request to "),s("code",[e._v("N − 1")]),e._v(" highest-ranked healthy nodes from the preference list.")]),e._v(" "),s("li",[e._v("The put() operation is considered successful after receiving "),s("code",[e._v("W − 1")]),e._v(" confirmation.")])]),e._v(" "),s("h3",{attrs:{id:"get-process"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#get-process"}},[e._v("#")]),e._v(" ‘get()’ process")]),e._v(" "),s("ol",[s("li",[e._v("The coordinator requests the data version from "),s("code",[e._v("N − 1")]),e._v(" highest-ranked healthy nodes from the preference list.")]),e._v(" "),s("li",[e._v("Waits until "),s("code",[e._v("R − 1")]),e._v(" replies.")]),e._v(" "),s("li",[e._v("Coordinator handles causal data versions through a vector clock.")]),e._v(" "),s("li",[e._v("Returns all relevant data versions to the caller.")])]),e._v(" "),s("h3",{attrs:{id:"request-handling-through-state-machine"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#request-handling-through-state-machine"}},[e._v("#")]),e._v(" Request handling through state machine")]),e._v(" "),s("p",[e._v("Each client request results in creating a state machine on the node that\nreceived the client request. The state machine contains all the logic for\nidentifying the nodes responsible for a key, sending the requests, waiting for\nresponses, potentially doing retries, processing the replies, and packaging\nthe response for the client. Each state machine instance handles exactly one\nclient request. For example, a read operation implements the following state\nmachine:")]),e._v(" "),s("ol",[s("li",[e._v("Send read requests to the nodes.")]),e._v(" "),s("li",[e._v("Wait for the minimum number of required responses.")]),e._v(" "),s("li",[e._v("If too few replies were received within a given time limit, fail the request.")]),e._v(" "),s("li",[e._v("Otherwise, gather all the data versions and determine the ones to be returned.")]),e._v(" "),s("li",[e._v("If versioning is enabled, perform syntactic reconciliation and generate an opaque write context that contains the vector clock that subsumes all the remaining versions")])]),e._v(" "),s("p",[e._v("After the read response has been returned to the caller, the state machine waits for a short period to receive any outstanding responses. If stale versions were returned in any of the responses, the coordinator updates those nodes with the latest version. This process is called "),s("strong",[e._v("Read Repair")]),e._v(" because it repairs replicas that have missed a recent update.")]),e._v(" "),s("p",[e._v("As stated above, put() requests are coordinated by one of the top N nodes\nin the preference list. Although it is always desirable to have the first node\namong the top N to coordinate the writes, thereby serializing all writes at a\nsingle location, this approach has led to uneven load distribution for\nDynamo. This is because the request load is not uniformly distributed across\nobjects. To counter this, any of the top N nodes in the preference list is\nallowed to coordinate the writes. In particular, since each write operation\nusually follows a read operation, the coordinator for a write operation is\nchosen to be the node that replied fastest to the previous read operation,\nwhich is stored in the request’s context information. This optimization\nenables Dynamo to pick the node that has the data that was read by the\npreceding read operation, thereby increasing the chances of getting “"),s("strong",[e._v("read-your-writes”")]),e._v(" consistency.")]),e._v(" "),s("h2",{attrs:{id:"anti-entropy-through-merkle-trees"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#anti-entropy-through-merkle-trees"}},[e._v("#")]),e._v(" Anti-entropy Through Merkle Trees")]),e._v(" "),s("p",[e._v("As we know, Dynamo uses vector clocks to remove conflicts while serving\nread requests. Now, if a replica falls significantly behind others, it might take\na very long time to resolve conflicts using just vector clocks. It would be nice\nto be able to automatically resolve some conflicts in the background. To do\nthis, we need to quickly compare two copies of a range of data residing on\ndifferent replicas and figure out exactly which parts are different.")]),e._v(" "),s("h3",{attrs:{id:"merkle-trees"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#merkle-trees"}},[e._v("#")]),e._v(" Merkle trees")]),e._v(" "),s("p",[e._v("A replica can contain a lot of data. Naively splitting up the entire data range\nfor checksums is not very feasible; there is simply too much data to be\ntransferred. Therefore, Dynamo uses "),s("strong",[e._v("Merkle trees")]),e._v(" to compare replicas of a\nrange. A Merkle tree is a binary tree of hashes, where each internal node is\nthe hash of its two children, and each leaf node is a hash of a portion of the\noriginal data.")]),e._v(" "),s("p",[s("img",{attrs:{src:a(805),alt:"img"}})]),e._v(" "),s("p",[e._v("Comparing Merkle trees is conceptually simple:")]),e._v(" "),s("ol",[s("li",[e._v("Compare the root hashes of both trees.")]),e._v(" "),s("li",[e._v("If they are equal, stop.")]),e._v(" "),s("li",[e._v("Recurse on the left and right children.")])]),e._v(" "),s("p",[e._v("Ultimately, this means that replicas know precisely which parts of the range\nare different, and the amount of data exchanged is minimized.")]),e._v(" "),s("h3",{attrs:{id:"merits-and-demerits-of-merkle-trees"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#merits-and-demerits-of-merkle-trees"}},[e._v("#")]),e._v(" Merits and demerits of Merkle trees")]),e._v(" "),s("p",[e._v("The principal advantage of using a Merkle tree is that each branch of the tree\ncan be checked independently without requiring nodes to download the\nentire tree or the whole data set. Hence, Merkle trees minimize the amount\nof data that needs to be transferred for synchronization and reduce the\nnumber of disk reads performed during the anti-entropy process.")]),e._v(" "),s("p",[e._v("The disadvantage of using Merkle trees is that many key ranges can change\nwhen a node joins or leaves, and as a result, the trees need to be\nrecalculated.")]),e._v(" "),s("h2",{attrs:{id:"gossip-protocol"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#gossip-protocol"}},[e._v("#")]),e._v(" Gossip Protocol")]),e._v(" "),s("p",[e._v("In a Dynamo cluster, since we do not have any central node that keeps track\nof all nodes to know if a node is down or not, how does a node know every\nother node’s current state? The simplest way to do this is to have every node\nmaintain heartbeats with every other node. When a node goes down, it will\nstop sending out heartbeats, and everyone else will find out immediately. But\nthen O(N"),s("sup",[e._v("2")]),e._v(") messages get sent every tick (N being the number of nodes),\nwhich is a ridiculously high amount and not feasible in any sizable cluster.")]),e._v(" "),s("p",[e._v("Dynamo uses "),s("strong",[e._v("gossip protocol")]),e._v(" that enables each node to keep track of state\ninformation about the other nodes in the cluster, like which nodes are\nreachable, what key ranges they are responsible for, and so on (this is\nbasically a copy of the hash ring). Nodes share state information with each\nother to stay in sync. Gossip protocol is a "),s("strong",[e._v("peer-to-peer communication mechanism")]),e._v(" in which nodes periodically exchange state information about\nthemselves and other nodes they know about. Each node initiates a gossip\nround every second to exchange state information about itself and other nodes with one other random node. This means that any new event will\neventually propagate through the system, and all nodes quickly learn about\nall other nodes in a cluster.")]),e._v(" "),s("p",[s("img",{attrs:{src:a(806),alt:"img"}})]),e._v(" "),s("h3",{attrs:{id:"external-discovery-through-seed-nodes"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#external-discovery-through-seed-nodes"}},[e._v("#")]),e._v(" External discovery through seed nodes")]),e._v(" "),s("p",[e._v("As we know, Dynamo nodes use gossip protocol to find the current state of\nthe ring. This can result in a logical partition of the cluster in a particular\nscenario. Let’s understand this with an example:")]),e._v(" "),s("p",[e._v("An administrator joins node A to the ring and then joins node B to the ring.\nNodes A and B consider themselves part of the ring, yet neither would be\nimmediately aware of each other. To prevent these logical partitions,\nDynamo introduced the concept of "),s("strong",[e._v("seed nodes")]),e._v(". Seed nodes are fully\nfunctional nodes and can be obtained either from a static configuration or a\nconfiguration service. This way, all nodes are aware of seed nodes. Each\nnode communicates with seed nodes through gossip protocol to reconcile\nmembership changes; therefore, logical partitions are highly unlikely.")]),e._v(" "),s("h2",{attrs:{id:"dynamo-characteristics-and-criticism"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#dynamo-characteristics-and-criticism"}},[e._v("#")]),e._v(" Dynamo Characteristics and Criticism")]),e._v(" "),s("h3",{attrs:{id:"responsibilities-of-a-dynamo-s-node"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#responsibilities-of-a-dynamo-s-node"}},[e._v("#")]),e._v(" Responsibilities of a Dynamo’s node")]),e._v(" "),s("p",[e._v("Because Dynamo is completely decentralized and does not rely on a central/leader server (unlike GFS, for example), each node serves three functions:")]),e._v(" "),s("ol",[s("li",[s("strong",[e._v("Managing get() and put() requests")]),e._v(": A node may act as a coordinator and manage all operations for a particular key or may forward the request to the appropriate node.")]),e._v(" "),s("li",[s("strong",[e._v("Keeping track of membership and detecting failures")]),e._v(": Every node uses gossip protocol to keep track of other nodes in the system and their associated hash ranges.")]),e._v(" "),s("li",[s("strong",[e._v("Local persistent storage")]),e._v(": Each node is responsible for being either the primary or replica store for keys that hash to a specific range of values. These (key, value) pairs are stored within that node using various storage systems depending on application needs. A few examples of such storage systems are:\n"),s("ul",[s("li",[e._v("BerkeleyDB Transactional Data Store")]),e._v(" "),s("li",[e._v("MySQL (for large objects)")]),e._v(" "),s("li",[e._v("An in-memory buffer (for best performance) backed by persistent storage")])])])]),e._v(" "),s("h3",{attrs:{id:"characteristics-of-dynamo"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#characteristics-of-dynamo"}},[e._v("#")]),e._v(" Characteristics of Dynamo")]),e._v(" "),s("p",[e._v("Here are a few reasons behind Dynamo’s popularity:")]),e._v(" "),s("ul",[s("li",[s("strong",[e._v("Distributed")]),e._v(": Dynamo can run on a large number of machines.")]),e._v(" "),s("li",[s("strong",[e._v("Decentralized")]),e._v(": Dynamo is decentralized; there is no need for any central coordinator to oversee operations. All nodes are identical and can perform all functions of Dynamo.")]),e._v(" "),s("li",[s("strong",[e._v("Scalable")]),e._v(": By adding more nodes to the cluster, Dynamo can easily be scaled horizontally. No manual intervention or rebalancing is required. Additionally, Dynamo achieves linear scalability and proven faulttolerance on commodity hardware.")]),e._v(" "),s("li",[s("strong",[e._v("Highly Available")]),e._v(": Dynamo is fault-tolerant, and the data remains available even if one or several nodes or data centers go down.")]),e._v(" "),s("li",[s("strong",[e._v("Fault-tolerant and reliable")]),e._v(": Since data is replicated to multiple nodes, fault-tolerance is pretty high.")]),e._v(" "),s("li",[s("strong",[e._v("Tunable consistency")]),e._v(": With Dynamo, applications can adjust the tradeoff between availability and consistency of data, typically by configuring replication factor and consistency level settings.")]),e._v(" "),s("li",[s("strong",[e._v("Durable")]),e._v(": Dynamo stores data permanently.")]),e._v(" "),s("li",[s("strong",[e._v("Eventually Consistent")]),e._v(": Dynamo accepts the trade-off of strong consistency in favor of high availability")])]),e._v(" "),s("h3",{attrs:{id:"criticism-on-dynamo"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#criticism-on-dynamo"}},[e._v("#")]),e._v(" Criticism on Dynamo")]),e._v(" "),s("ul",[s("li",[e._v("Each Dynamo node contains the entire Dynamo routing table. This is likely to affect the scalability of the system as this routing table will grow larger and larger as nodes are added to the system.")]),e._v(" "),s("li",[e._v("Dynamo seems to imply that it strives for symmetry, where every node in the system has the same set of roles and responsibilities, but later, it specifies some nodes as seeds. Seeds are special nodes that are externally discoverable. These are used to help prevent logical partitions in the Dynamo ring. This seems like it may violate Dynamo’s symmetry principle.")]),e._v(" "),s("li",[e._v("Although security was not a concern as Dynamo was built for internal use only, DHTs can be susceptible to several different types of attacks. While Amazon can assume a trusted environment, sometimes a buggy software can act in a manner quite similar to a malicious actor.")]),e._v(" "),s("li",[e._v("Dynamo’s design can be described as a “leaky abstraction,” where client applications are often asked to manage inconsistency, and the user experience is not 100% seamless. For example, inconsistencies in the shopping cart items may lead users to think that the website is buggy or unreliable.")])]),e._v(" "),s("h3",{attrs:{id:"datastores-developed-on-the-principles-of-dynamo"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#datastores-developed-on-the-principles-of-dynamo"}},[e._v("#")]),e._v(" Datastores developed on the principles of Dynamo")]),e._v(" "),s("p",[e._v("Dynamo is not open-source and was built for services running within\nAmazon. Two of the most famous datastores built on the principles of\nDynamo are "),s("a",{attrs:{href:"https://riak.com/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Riak"),s("OutboundLink")],1),e._v(" and "),s("a",{attrs:{href:"https://cassandra.apache.org/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Cassandra"),s("OutboundLink")],1),e._v(". Riak is a distributed NoSQL key-value data store that is highly available, scalable, fault-tolerant, and easy to operate.\nCassandra is a distributed, decentralized, scalable, and highly available\nNoSQL wide-column database. Here is how they adopted different\nalgorithms offered by Dynamo:")]),e._v(" "),s("table",[s("thead",[s("tr",[s("th",[e._v("Technique")]),e._v(" "),s("th",[e._v("Apache Cassandra")]),e._v(" "),s("th",[e._v("Riak")])])]),e._v(" "),s("tbody",[s("tr",[s("td",[e._v("Consistent Hashing with virtual nodes")]),e._v(" "),s("td",[e._v("✅")]),e._v(" "),s("td",[e._v("✅")])]),e._v(" "),s("tr",[s("td",[e._v("Hinted Handoff")]),e._v(" "),s("td",[e._v("✅")]),e._v(" "),s("td",[e._v("✅")])]),e._v(" "),s("tr",[s("td",[e._v("Anti-entropy with Merkle trees")]),e._v(" "),s("td",[e._v("✅ (manual repair)")]),e._v(" "),s("td",[e._v("✅")])]),e._v(" "),s("tr",[s("td",[e._v("Vector Clocks")]),e._v(" "),s("td",[e._v("❌ (LWW)")]),e._v(" "),s("td",[e._v("✅")])]),e._v(" "),s("tr",[s("td",[e._v("Gossip-based Protocol")]),e._v(" "),s("td",[e._v("✅")]),e._v(" "),s("td",[e._v("✅")])])])]),e._v(" "),s("h2",{attrs:{id:"summary"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#summary"}},[e._v("#")]),e._v(" Summary")]),e._v(" "),s("ol",[s("li",[e._v("Dynamo is a highly available "),s("strong",[e._v("key-value store")]),e._v(" developed by "),s("strong",[e._v("Amazon")]),e._v(" for their internal use.")]),e._v(" "),s("li",[e._v("Dynamo shows how business requirements can drive system designs. Amazon has chosen to sacrifice strong consistency for "),s("strong",[e._v("higher availability")]),e._v(" based on their business requirements.")]),e._v(" "),s("li",[e._v("Dynamo was designed with the understanding that system/hardware failures can and do occur.")]),e._v(" "),s("li",[e._v("Dynamo is a "),s("strong",[e._v("peer-to-peer")]),e._v(" distributed system, i.e., it does not have any leader or follower nodes. All nodes are equal and have the same set of roles and responsibilities. This also means that there is no single point of failure.")]),e._v(" "),s("li",[e._v("Dynamo uses the Consistent Hashing algorithm to distribute the data among nodes in the cluster automatically.")]),e._v(" "),s("li",[e._v("Data is replicated across nodes for fault tolerance and redundancy. Dynamo replicates writes to a "),s("strong",[e._v("sloppy quorum")]),e._v(" of other nodes in the system instead of a strict maiority quorum.")]),e._v(" "),s("li",[e._v("For anti-entropy and to resolve conflicts, Dynamo uses Merkle trees.")]),e._v(" "),s("li",[e._v("Different storage engines can be plugged into Dynamo's local storage.")]),e._v(" "),s("li",[e._v("Dynamo uses the gossip protocol for inter-node communication.")]),e._v(" "),s("li",[e._v('Dynamo makes the system "'),s("strong",[e._v("always writeable")]),e._v('" by using '),s("strong",[e._v("hinted handoff.")])]),e._v(" "),s("li",[e._v("Dynamo's design philosophy is to ALWAYS allow writes. To support this, Dynamo allows concurrent writes. Writes can be performed by different servers concurrently, resulting in multiple versions of an object.\nDynamo attempts to track and reconcile these changes using vector clocks. When Dynamo cannot reconcile an object's state from its "),s("strong",[e._v("vector clocks")]),e._v(", it sends it to the client application for reconciliation ("),s("em",[e._v("the thought being that the clients have more semantic information on the object and may be able to reconcile it")]),e._v(").")]),e._v(" "),s("li",[e._v("Dynamo is able to successfully pull together several distributed techniques such as consistent hashing, p2p, gossip, vector clocks, and quorum, and combine them into a complex system.")]),e._v(" "),s("li",[e._v("Amazon built Dynamo for internal use only, so "),s("strong",[e._v("no security")]),e._v(" related issues were considered.")])]),e._v(" "),s("table",[s("thead",[s("tr",[s("th",[e._v("Problem")]),e._v(" "),s("th",[e._v("Technique")]),e._v(" "),s("th",[e._v("Advantage")])])]),e._v(" "),s("tbody",[s("tr",[s("td",[e._v("Partitioning")]),e._v(" "),s("td",[e._v("Consistent Hashing")]),e._v(" "),s("td",[e._v("Incremental Scalability")])]),e._v(" "),s("tr",[s("td",[e._v("HA for writes")]),e._v(" "),s("td",[e._v("Vector clocks with reconciliation during reads")]),e._v(" "),s("td",[e._v("Version size is decoupled from update rates")])]),e._v(" "),s("tr",[s("td",[e._v("Handling temporary failures")]),e._v(" "),s("td",[e._v("Sloppy Quorum & Hinted Handoff")]),e._v(" "),s("td",[e._v("Provides HA and durability guarantee when some of the replicas are not available")])]),e._v(" "),s("tr",[s("td",[e._v("Recovering from permanent failures")]),e._v(" "),s("td",[e._v("Anti-entropy using Merkle trees")]),e._v(" "),s("td",[e._v("Synchronizes divergent replicas on the background")])]),e._v(" "),s("tr",[s("td",[e._v("Membership & failure detection")]),e._v(" "),s("td",[e._v("Gossip-based membership protocol & failure detection")]),e._v(" "),s("td",[e._v("Preserves symmetry and avoid centralized monitoring")])])])]),e._v(" "),s("h3",{attrs:{id:"system-design-patterns"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#system-design-patterns"}},[e._v("#")]),e._v(" System design patterns")]),e._v(" "),s("p",[e._v("Here is a summary of system design patterns used in Dynamo:")]),e._v(" "),s("ul",[s("li",[s("strong",[e._v("Consistent Hashing")]),e._v(": Dynamo uses Consistent Hashing to distribute its data across nodes.")]),e._v(" "),s("li",[s("strong",[e._v("Quorum")]),e._v(": To ensure data consistency, each Dynamo write operation can be configured to be successful only if the data has been written to at least a quorum of replica nodes.")]),e._v(" "),s("li",[s("strong",[e._v("Gossip protocol")]),e._v(": Dynamo uses gossip protocol that allows each node to keep track of state information about the other nodes in the cluster.")]),e._v(" "),s("li",[s("strong",[e._v("Hinted Handoff")]),e._v(": Dynamo nodes use Hinted Handoff to remember the write operation for failing nodes.")]),e._v(" "),s("li",[s("strong",[e._v("Read Repair")]),e._v(": Dynamo uses 'Read Repair' to push the latest version of the data to nodes with the older versions.")]),e._v(" "),s("li",[s("strong",[e._v("Vector clocks")]),e._v(": To reconcile concurrent updates on an object Dynamo uses Vector clocks.")]),e._v(" "),s("li",[s("strong",[e._v("Merkle trees")]),e._v(": For anti-entropy and to resolve conflicts in the background, Dynamo uses Merkle trees.")])]),e._v(" "),s("h2",{attrs:{id:"abbr"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#abbr"}},[e._v("#")]),e._v(" Abbr")]),e._v(" "),s("ul",[s("li",[e._v("N: Node")]),e._v(" "),s("li",[e._v("W: Write")]),e._v(" "),s("li",[e._v("R: Read")]),e._v(" "),s("li",[e._v("DHT: Distributed Hash Table")]),e._v(" "),s("li",[e._v("NTP: Network Time Protocol")]),e._v(" "),s("li",[e._v("CRDTs: Conflict-free replicated data types")]),e._v(" "),s("li",[e._v("LWW: Last-write-wins")])])])}),[],!1,null,null,null);t.default=n.exports},799:function(e,t,a){e.exports=a.p+"assets/img/hld.58698744.png"},800:function(e,t,a){e.exports=a.p+"assets/img/consitenthash.1c254622.png"},801:function(e,t,a){e.exports=a.p+"assets/img/vnodes.58aaac1a.png"},802:function(e,t,a){e.exports=a.p+"assets/img/replica.115daa6f.png"},803:function(e,t,a){e.exports=a.p+"assets/img/vector-clock.cac189dd.png"},804:function(e,t,a){e.exports=a.p+"assets/img/coordinator.f74175bd.png"},805:function(e,t,a){e.exports=a.p+"assets/img/merkle-tree.4ed2fcad.png"},806:function(e,t,a){e.exports=a.p+"assets/img/gossip.45461107.png"}}]);