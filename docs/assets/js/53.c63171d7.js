(window.webpackJsonp=window.webpackJsonp||[]).push([[53],{1620:function(e,t,a){"use strict";a.r(t);var n=a(7),i=Object(n.a)({},(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("h1",{attrs:{id:"design-google-file-storage"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#design-google-file-storage"}},[e._v("#")]),e._v(" Design Google File Storage")]),e._v(" "),n("h2",{attrs:{id:"overview"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#overview"}},[e._v("#")]),e._v(" Overview")]),e._v(" "),n("h3",{attrs:{id:"goal"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#goal"}},[e._v("#")]),e._v(" Goal")]),e._v(" "),n("p",[e._v("Design a distributed file system to store huge files (terabyte and larger). The\nsystem should be scalable, reliable, and highly available.")]),e._v(" "),n("h3",{attrs:{id:"what-is-google-file-system-gfs"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#what-is-google-file-system-gfs"}},[e._v("#")]),e._v(" What is Google File System (GFS)?")]),e._v(" "),n("p",[e._v("GFS is a scalable distributed file system developed by Google for its large\ndata-intensive applications.")]),e._v(" "),n("h3",{attrs:{id:"background"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#background"}},[e._v("#")]),e._v(" Background")]),e._v(" "),n("p",[e._v("GFS was built for handling batch processing on large data sets and is\ndesigned for system-to-system interaction, not user-to-system interaction.")]),e._v(" "),n("p",[e._v("Google built GFS keeping the following goals in mind:")]),e._v(" "),n("ul",[n("li",[n("strong",[e._v("Scalable")]),e._v(": GFS should run reliably on a very large system built from\ncommodity hardware.")]),e._v(" "),n("li",[n("strong",[e._v("Fault-tolerant")]),e._v(": The design must be sufficiently tolerant of hardware\nand software failures to enable application-level services to continue\ntheir operation in the face of any likely combination of failure\nconditions.")]),e._v(" "),n("li",[n("strong",[e._v("Large files")]),e._v(": Files stored in GFS will be huge. Multi-GB files are common.\nLarge sequential and small random reads: The workloads primarily\nconsist of two kinds of reads: large, streaming reads and small, random\nreads.")]),e._v(" "),n("li",[n("strong",[e._v("Sequential writes")]),e._v(": The workloads also have many large, sequential\nwrites that append data to files. Typical operation sizes are similar to\nthose for reads. Once written, files are seldom modified again.")]),e._v(" "),n("li",[n("strong",[e._v("Not optimized for small data:")]),e._v(" Small, random reads and writes do\noccur and are supported, but the system is not optimized for such cases.")]),e._v(" "),n("li",[n("strong",[e._v("Concurrent access")]),e._v(": The level of concurrent access will also be high,\nwith large numbers of concurrent appends being particularly prevalent,\noften accompanied by concurrent reads.")]),e._v(" "),n("li",[n("strong",[e._v("High throughput")]),e._v(": GFS should be optimized for high and sustained\nthroughput in reading the data, and this is prioritized over latency. This\nis not to say that latency is unimportant; rather, GFS needs to be\noptimized for high-performance reading and appending large volumes\nof data for the correct operation of the system.")])]),e._v(" "),n("h3",{attrs:{id:"gfs-use-cases"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#gfs-use-cases"}},[e._v("#")]),e._v(" GFS use cases")]),e._v(" "),n("p",[e._v("-GFS is a distributed file system built for large, distributed data-intensive\napplications like "),n("strong",[e._v("Gmail")]),e._v(" or "),n("strong",[e._v("YouTube")]),e._v(".")]),e._v(" "),n("ul",[n("li",[e._v("Originally, it was built to store data generated by Google’s large\n"),n("strong",[e._v("crawling and indexing system")])]),e._v(" "),n("li",[e._v("Google’s "),n("strong",[e._v("BigTable")]),e._v(" uses the distributed Google File System to store log\nand data files.")])]),e._v(" "),n("h3",{attrs:{id:"apis"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#apis"}},[e._v("#")]),e._v(" APIs")]),e._v(" "),n("p",[e._v("GFS does not provide standard POSIX-like APIs; instead, user-level APIs are\nprovided. In GFS, files are organized hierarchically in directories and\nidentified by their pathnames. GFS supports the usual file system operations:")]),e._v(" "),n("ul",[n("li",[n("em",[e._v("create")]),e._v(" – To create a new instance of a file.")]),e._v(" "),n("li",[n("em",[e._v("delete")]),e._v(" – To delete an instance of a file.")]),e._v(" "),n("li",[n("em",[e._v("open")]),e._v(" – To open a named file and return a handle.")]),e._v(" "),n("li",[n("em",[e._v("close")]),e._v(" – To close a given file specified by a handle.")]),e._v(" "),n("li",[n("em",[e._v("read")]),e._v(" – To read data from a specified file and offset.")]),e._v(" "),n("li",[n("em",[e._v("write")]),e._v(" – To write data to a specified file and offset.")])]),e._v(" "),n("p",[e._v("In addition, GFS supports two special operations:")]),e._v(" "),n("ul",[n("li",[n("strong",[e._v("Snapshot")]),e._v(": A snapshot is an efficient way of creating a copy of the\ncurrent instance of a file or directory tree.")]),e._v(" "),n("li",[n("strong",[e._v("Append")]),e._v(": An append operation allows multiple clients to append data to\nthe same file concurrently while guaranteeing atomicity. It is useful for\nimplementing multi-way merge results and producer-consumer queues\nthat many clients can simultaneously append to without additional\nlocking.")])]),e._v(" "),n("h2",{attrs:{id:"high-level-architecture"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#high-level-architecture"}},[e._v("#")]),e._v(" High-level Architecture")]),e._v(" "),n("p",[e._v("A GFS cluster consists of a single master and multiple ChunkServers and is\naccessed by multiple clients.")]),e._v(" "),n("h3",{attrs:{id:"chunks"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#chunks"}},[e._v("#")]),e._v(" Chunks")]),e._v(" "),n("p",[e._v("As files stored in GFS tend to be very large, GFS breaks files into multiple\nfixed-size chunks where each chunk is 64 megabytes in size.")]),e._v(" "),n("h3",{attrs:{id:"chunk-handle"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#chunk-handle"}},[e._v("#")]),e._v(" Chunk handle")]),e._v(" "),n("p",[e._v("Each chunk is identified by an immutable and globally unique 64-bit ID\nnumber called chunk handle. This allows for 2"),n("sup",[e._v("64")]),e._v(" unique chunks. If each\nchunk is 64 MB, total storage space would be more than 10"),n("sup",[e._v("9")]),e._v(" exa-bytes.")]),e._v(" "),n("p",[e._v("As files are split into chunks, therefore, the job of GFS is to provide a\nmapping from files to chunks, and then to support standard operations on\nfiles, mapping down to operations on individual chunks.")]),e._v(" "),n("h3",{attrs:{id:"cluster"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#cluster"}},[e._v("#")]),e._v(" Cluster")]),e._v(" "),n("p",[e._v("GFS is organized into a simple network of computers called a cluster. All GFS\nclusters contain three kinds of entities:")]),e._v(" "),n("ol",[n("li",[e._v("A single master server")]),e._v(" "),n("li",[e._v("Multiple ChunkServers")]),e._v(" "),n("li",[e._v("Many clients")])]),e._v(" "),n("p",[e._v("The master stores all metadata about the system, while the ChunkServers\nstore the real file data.")]),e._v(" "),n("p",[n("img",{attrs:{src:a(856),alt:"Figure 1"}}),n("br"),e._v(" "),n("em",[e._v("GFS high-level architecture")])]),e._v(" "),n("h3",{attrs:{id:"chunkserver"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#chunkserver"}},[e._v("#")]),e._v(" ChunkServer")]),e._v(" "),n("p",[e._v("ChunkServers store chunks on local disks as regular Linux files and read or\nwrite chunk data specified by a chunk handle and byte-range.")]),e._v(" "),n("p",[e._v("For reliability, each chunk is replicated to multiple ChunkServers. By default,\nGFS stores three replicas, though different replication factors can be\nspecified on a per-file basis.")]),e._v(" "),n("p",[n("img",{attrs:{src:a(857),alt:"Figure 2"}}),n("br"),e._v(" "),n("em",[e._v("Chunk replication")])]),e._v(" "),n("h3",{attrs:{id:"master"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#master"}},[e._v("#")]),e._v(" Master")]),e._v(" "),n("p",[e._v("Master server is the coordinator of a GFS cluster and is responsible for\nkeeping track of filesystem metadata:")]),e._v(" "),n("ol",[n("li",[e._v("The metadata stored at the master includes:\n"),n("ul",[n("li",[e._v("Name and directory of each file")]),e._v(" "),n("li",[e._v("Mapping of each file to its chunks")]),e._v(" "),n("li",[e._v("Current locations of chunks")]),e._v(" "),n("li",[e._v("Access control information")])])]),e._v(" "),n("li",[e._v("The master also controls system-wide activities such as chunk lease\nmanagement (locks on chunks with expiration), garbage collection of\norphaned chunks, and chunk migration between ChunkServers. Master\nassigns chunk handle to chunks at time of chunk creation.")]),e._v(" "),n("li",[e._v("The master periodically communicates with each ChunkServer in\nHeartBeat messages to give it instructions and collect its state.")]),e._v(" "),n("li",[e._v("For performance and fast random access, all metadata is stored in the\nmaster’s main memory. This includes the entire filesystem namespace\nas well as all the name-to-chunk mappings.")]),e._v(" "),n("li",[e._v("For fault tolerance and to handle a master crash, all metadata changes\nare written to the disk onto an operation log. This operation log is also\nreplicated onto remote machines. The operation log is similar to a\njournal. Every operation to the file system is logged into this file.")]),e._v(" "),n("li",[e._v("The master is a single point of failure, hence, it replicates its data onto\nseveral remote machines so that the master can be readily restored on\nfailure.")]),e._v(" "),n("li",[e._v("The benefit of having a single, centralized master is that it has a global\nview of the file system, and hence, it can make optimum management\ndecisions, for example, related to chunk placement.")])]),e._v(" "),n("h3",{attrs:{id:"client"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#client"}},[e._v("#")]),e._v(" Client")]),e._v(" "),n("p",[e._v("Client is an entity that makes a read or write request to GSF. GFS client\nlibrary is linked into each application that uses GFS. This library\ncommunicates with the master for all metadata-related operations like\ncreating or deleting files, looking up files, etc. To read or write data, the\nclient interacts directly with the ChunkServers that hold the data")]),e._v(" "),n("p",[e._v("Neither the client nor the ChunkServer caches file data. Client caches offer\nlittle benefit because most applications stream through huge files or have\nworking sets too large to be cached. ChunkServers rely on the buffer cache\nin Linux to maintain frequently accessed data in memory.")]),e._v(" "),n("h2",{attrs:{id:"single-master-and-large-chunk-size"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#single-master-and-large-chunk-size"}},[e._v("#")]),e._v(" Single Master and Large Chunk Size")]),e._v(" "),n("h3",{attrs:{id:"single-master"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#single-master"}},[e._v("#")]),e._v(" Single Master")]),e._v(" "),n("p",[e._v("Having a single master vastly simplifies GFS design and enables the master\nto make sophisticated chunk placement and replication decisions using\nglobal knowledge. However, GFS minimizes the master’s involvement in\nreads and writes, so that it does not become a bottleneck. Clients never read\nor write file data through the master. Instead, a client asks the master which\nChunkServers it should contact. The client caches this information for a\nlimited time and interacts with the ChunkServers directly for many\nsubsequent operations.")]),e._v(" "),n("p",[n("img",{attrs:{src:a(858),alt:"Figure 3"}}),n("br"),e._v(" "),n("em",[e._v("GFS's high-level architecture")])]),e._v(" "),n("h3",{attrs:{id:"chunk-size"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#chunk-size"}},[e._v("#")]),e._v(" Chunk size")]),e._v(" "),n("p",[e._v("Chunk size is one of the key design parameters. GFS has chosen 64 MB,\nwhich is much larger than typical filesystem block sizes (which are often\naround 4KB). Here are the advantages of using a large chunk size:")]),e._v(" "),n("ol",[n("li",[e._v("Since GFS was designed to handle huge files, small chunk sizes would\nnot make a lot of sense, as each file would then have a map of a huge\nnumber of chunks.")]),e._v(" "),n("li",[e._v("As the master holds the metadata and manages file distribution, it is\ninvolved whenever chunks are read, modified, or deleted. A small\nchunk size would significantly increase the amount of data a master\nwould need to manage, and also, increase the amount of data that\nwould need to be communicated to a client, resulting in extra network\ntraffic.")]),e._v(" "),n("li",[e._v("A large chunk size reduces the size of the metadata stored on the\nmaster, which enables the master to keep all the metadata in memory,\nthus significantly decreasing the latency for control operations.")]),e._v(" "),n("li",[e._v("By using a large chunk size, GFS reduces the need for frequent\ncommunication with the master to get chunk location information. It\nbecomes feasible for a client to cache all information related to chunk\nlocations of a large file. Client metadata caches have timeouts to reduce\nthe risk of caching stale data.")]),e._v(" "),n("li",[e._v("A large chunk size also makes it possible to keep a TCP connection open\nto a ChunkServer for an extended time, amortizing the time of setting\nup a TCP connection.")]),e._v(" "),n("li",[e._v("A large chunk size simplifies ChunkServer management, i.e., to check\nwhich ChunkServers are near capacity or which are overloaded.")]),e._v(" "),n("li",[e._v("Large chunk size provides highly efficient sequential reads and appends\nof large amounts of data.")])]),e._v(" "),n("h3",{attrs:{id:"lazy-space-allocation"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#lazy-space-allocation"}},[e._v("#")]),e._v(" Lazy space allocation")]),e._v(" "),n("p",[e._v("Each chunk replica is stored as a plain Linux file on a ChunkServer. GFS does\nnot allocate the whole 64MB of disk space when creating a chunk. Instead, as\nthe client appends data, the ChunkServer, lazily extends the chunk. This lazy\nspace allocation avoids wasting space due to internal fragmentation. Internal\nfragmentation refers to having unused portions of the 64 MB chunk. For\nexample, if we allocate a 64 MB chunk and only fill up 20MB, the remaining\nspace is unused")]),e._v(" "),n("p",[e._v("One disadvantage of having a large chunk size is the handling of small files.\nSince a small file will have one or a few chunks, the ChunkServers storing\nthose chunks can become hotspots if a lot of clients access the same file. To\nhandle this scenario, GFS stores such files with a higher replication factor\nand also adds a random delay in the start times of the applications accessing\nthese files.")]),e._v(" "),n("h2",{attrs:{id:"metadata"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#metadata"}},[e._v("#")]),e._v(" Metadata")]),e._v(" "),n("p",[e._v("The master stores three types of metadata:")]),e._v(" "),n("ol",[n("li",[e._v("The file and chunk namespaces (i.e., directory hierarchy).")]),e._v(" "),n("li",[e._v("The mapping from files to chunks.")]),e._v(" "),n("li",[e._v("The locations of each chunk’s replicas.")])]),e._v(" "),n("p",[e._v("There are three aspects of how master manages the metadata:")]),e._v(" "),n("ol",[n("li",[e._v("Master keeps all this metadata in memory.")]),e._v(" "),n("li",[e._v("The first two types (i.e., namespaces and file-to-chunk mapping) are also\npersisted on the master’s local disk.")]),e._v(" "),n("li",[e._v("The third (i.e., chunk replicas’ locations) is not persisted.")])]),e._v(" "),n("h3",{attrs:{id:"storing-metadata-in-memory"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#storing-metadata-in-memory"}},[e._v("#")]),e._v(" Storing metadata in memory")]),e._v(" "),n("p",[e._v("Since metadata is stored in memory, the master operates very quickly.\nAdditionally, it is easy and efficient for the master to periodically scan\nthrough its entire state in the background. This periodic scanning is used to\nimplement three functions:")]),e._v(" "),n("ol",[n("li",[e._v("Chunk garbage collection")]),e._v(" "),n("li",[e._v("Re-replication in the case of ChunkServer failures")]),e._v(" "),n("li",[e._v("Chunk migration to balance load and disk-space usage across ChunkServers")])]),e._v(" "),n("p",[e._v("As discussed above, one potential concern for this memory-only approach is\nthat the number of chunks, and hence the capacity of the whole system, is\nlimited by how much memory the master has. This is not a serious problem\nin practice. The master maintains less than 64 bytes of metadata for each 64\nMB chunk. Most chunks are full because most files contain many chunks,\nonly the last of which may be partially filled. Similarly, the file namespace\ndata typically requires less than 64 bytes per file because the master stores\nfile names compactly using "),n("strong",[e._v("prefix compression.")])]),e._v(" "),n("p",[e._v("If the need for supporting an even larger file system arises, the cost of adding\nextra memory to the master is a small price to pay for the simplicity,\nreliability, performance, and flexibility gained by storing the metadata in\nmemory.")]),e._v(" "),n("h3",{attrs:{id:"chunk-location"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#chunk-location"}},[e._v("#")]),e._v(" Chunk location")]),e._v(" "),n("p",[e._v("The master does not keep a persistent record of which ChunkServers have a\nreplica of a given chunk; instead, the master asks each chunk server about\nits chunks at master startup, and whenever a ChunkServer joins the cluster.\nThe master can keep itself up-to-date after that because it controls all chunk\nplacements and monitors ChunkServer status with regular HeartBeat\nmessages.")]),e._v(" "),n("p",[e._v("By having the ChunkServer as the ultimate source of truth of each chunk’s\nlocation, GFS eliminates the problem of keeping the master and\nChunkServers in sync. It is not beneficial to maintain a consistent view of\nchunk locations on the master, because errors on a ChunkServer may cause\nchunks to vanish spontaneously (e.g., a disk may go bad and be disabled, or\nChunkServer is renamed or failed, etc.) In a cluster with hundreds of servers,\nthese events happen all too often.")]),e._v(" "),n("h3",{attrs:{id:"operation-log"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#operation-log"}},[e._v("#")]),e._v(" Operation log")]),e._v(" "),n("p",[e._v("The master maintains an operation log that contains the namespace and fileto-chunk mappings and stores it on the local disk. Specifically, this log stores\na historical record of all the metadata changes. Operation log is very\nimportant to GFS. It contains the persistent record of metadata and serves as\na logical timeline that defines the order of concurrent operations.")]),e._v(" "),n("p",[e._v("For fault tolerance and reliability, this operation log is replicated on multiple\nremote machines, and changes to the metadata are not made visible to\nclients until they have been persisted on all replicas. The master batches\nseveral log records together before flushing, thereby reducing the impact of\nflushing and replicating on overall system throughput.")]),e._v(" "),n("p",[e._v("Upon restart, the master can restore its file-system state by replaying the\noperation log. This log must be kept small to minimize the startup time, and\nthat is achieved by periodically checkpointing it.")]),e._v(" "),n("h3",{attrs:{id:"checkpointing"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#checkpointing"}},[e._v("#")]),e._v(" Checkpointing")]),e._v(" "),n("p",[e._v("Master’s state is periodically serialized to disk and then replicated, so that on\nrecovery, a master may load the checkpoint into memory, replay any\nsubsequent operations from the operation log, and be available again very\nquickly. To further speed up the recovery and improve availability, GFS\nstores the checkpoint in a compact B-tree like format that can be directly\nmapped into memory and used for namespace lookup without extra parsing.")]),e._v(" "),n("p",[e._v("The checkpoint process can take time, therefore, to avoid delaying incoming\nmutations, the master switches to a new log file and creates the new\ncheckpoint in a separate thread. The new checkpoint includes all mutations\nbefore the switch.")]),e._v(" "),n("h2",{attrs:{id:"master-operations"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#master-operations"}},[e._v("#")]),e._v(" Master Operations")]),e._v(" "),n("p",[e._v("The master executes all namespace operations. Furthermore, it manages\nchunk replicas throughout the system. It is responsible for:")]),e._v(" "),n("ul",[n("li",[e._v("Making replica placement decisions")]),e._v(" "),n("li",[e._v("Creating new chunks and hence replicas")]),e._v(" "),n("li",[e._v("Making sure that chunks are fully replicated according to the replication factor")]),e._v(" "),n("li",[e._v("Balancing the load across all the ChunkServers")]),e._v(" "),n("li",[e._v("Reclaim unused storage")])]),e._v(" "),n("h3",{attrs:{id:"namespace-management-and-locking"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#namespace-management-and-locking"}},[e._v("#")]),e._v(" Namespace management and locking")]),e._v(" "),n("p",[e._v("The master acquires locks over a namespace region to ensure proper\nserialization and to allow multiple operations at the master. GFS does not\nhave an i-node like tree structure for directories and files. Instead, it has a\nhash-map that maps a filename to its metadata, and reader-writer locks are\napplied on each node of the hash table for synchronization.")]),e._v(" "),n("ul",[n("li",[e._v("Each absolute file name or absolute directory name has an associated")]),e._v(" "),n("li",[e._v("read-write lock.")]),e._v(" "),n("li",[e._v("Each master operation acquires a set of locks before it runs.")]),e._v(" "),n("li",[e._v("To make operation on "),n("code",[e._v("/dir1/dir2/leaf")]),e._v(" , it first needs the following locks:\n"),n("ul",[n("li",[e._v("Reader lock on "),n("code",[e._v("/dir1")])]),e._v(" "),n("li",[e._v("Reader lock on "),n("code",[e._v("/dir1/dir2")])]),e._v(" "),n("li",[e._v("Reader or Writer lock on "),n("code",[e._v("/dir21/dir2/leaf")])])])]),e._v(" "),n("li",[e._v("Following this scheme, concurrent writes on the same leaf are prevented right away. However, at the same time, concurrent modifications in the same directory are allowed.")]),e._v(" "),n("li",[e._v("File creation does not require write-lock on the parent directory; a readlock on its name is sufficient to protect the parent directory from deletion, rename, or snapshot.")]),e._v(" "),n("li",[e._v("Write-lock on a file name stops attempts to create multiple files with the same name.")]),e._v(" "),n("li",[e._v("Locks are acquired in a consistent order to prevent "),n("strong",[e._v("deadlock")]),e._v(":\n"),n("ul",[n("li",[e._v("First ordered by level in the namespace tree")]),e._v(" "),n("li",[e._v("Lexicographically ordered within the same level")])])])]),e._v(" "),n("h3",{attrs:{id:"replica-placement"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#replica-placement"}},[e._v("#")]),e._v(" Replica placement")]),e._v(" "),n("p",[e._v("To ensure maximum data availability and integrity, the master distributes\nreplicas on different racks, so that clients can still read or write in case of a\nrack failure. As the in and out bandwidth of a rack may be less than the sum\nof the bandwidths of individual machines, placing the data in various racks\ncan help clients exploit reads from multiple racks. For ‘write’ operations,\nmultiple racks are actually disadvantageous as data has to travel longer\ndistances. It is an intentional tradeoff that GFS made.")]),e._v(" "),n("h4",{attrs:{id:"replica-creation-and-re-replication"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#replica-creation-and-re-replication"}},[e._v("#")]),e._v(" Replica creation and re-replication")]),e._v(" "),n("p",[e._v("The goals of a master are to place replicas on servers with less-than-average\ndisk utilization, spread replicas across racks, and reduce the number of\n‘recent’ creations on each ChunkServer (even though writes are cheap, they\nare followed by heavy write traffic) which might create additional load.")]),e._v(" "),n("p",[e._v("Chunks need to be re-replicated as soon as the number of available replicas\nfalls (due to data corruption on a server or a replica being unavailable)\nbelow the user-specified replication factor. Instead of re-replicating all of\nsuch chunks at once, the master prioritizes re-replication to prevent these\ncloning operations from becoming bottlenecks. Restrictions are placed on the\nbandwidth of each server for re-replication so that client requests are not\ncompromised.")]),e._v(" "),n("p",[n("strong",[e._v("How are chunks prioritized for re-replication?")])]),e._v(" "),n("ul",[n("li",[e._v("A chunk is prioritized based on how far it is from its replication goal.\nFor example, a chunk that has lost two replicas will be given priority on\na chuck that has lost only one replica.")]),e._v(" "),n("li",[e._v("GFS prioritizes chunks of live files as opposed to chunks that belong to\nrecently deleted files. Deleted files are not removed\nimmediately; instead, they are renamed temporarily and garbage collected after a few days. Replicas of deleted files can exist for a few\ndays as well.")])]),e._v(" "),n("h4",{attrs:{id:"replica-rebalancing"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#replica-rebalancing"}},[e._v("#")]),e._v(" Replica rebalancing")]),e._v(" "),n("p",[e._v("Master rebalances replicas regularly to achieve load balancing and better\ndisk space usage. It may move replicas from one ChunkServer to another to\nbring disk usage in a server closer to the average. Any new ChunkServer\nadded to the cluster is filled up gradually by the master rather than flooding\nit with a heavy traffic of write operations.")]),e._v(" "),n("h3",{attrs:{id:"stale-replica-detection"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#stale-replica-detection"}},[e._v("#")]),e._v(" Stale replica detection")]),e._v(" "),n("p",[e._v("Chunk replicas may become stale if a ChunkServer fails and misses\nmutations to the chunk while it is down. For each chunk, the master\nmaintains a chunk Version Number to distinguish between up-to-date and\nstale replicas. The master increments the chunk version every time it grants\na lease (more on this later) and informs all up-to-date replicas. The master\nand these replicas all record the new version number in their persistent\nstate. If the ChunkServer hosting a chunk replica is down during a mutation,\nthe chunk replica will become stale and will have an older version number.\nThe master will detect this when the ChunkServer restarts and reports its set\nof chunks and their associated version numbers. Master removes stale\nreplicas during regular garbage collection.")]),e._v(" "),n("p",[e._v("Stale replicas are not given to clients when they ask the master for a chunk\nlocation, and they are not involved in mutations either. However, because a\nclient caches a chunk’s location, it may read from a stale replica before the\ndata is resynced. The impact of this is low due to the fact that most\noperations to a chunk are append-only. This means that a stale replica\nusually returns a premature end of a chunk rather than outdated data for a\nvalue.")]),e._v(" "),n("h2",{attrs:{id:"anatomy-of-a-read-operation"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#anatomy-of-a-read-operation"}},[e._v("#")]),e._v(" Anatomy of a Read Operation")]),e._v(" "),n("p",[e._v("A typical read interaction with a GFS cluster by a client application goes like\nthis:")]),e._v(" "),n("ol",[n("li",[e._v("First, the client translates the file name and byte offset specified by the\napplication into a chunk index within the file. Given the fixed chunk\nsize, this can be computed easily.")]),e._v(" "),n("li",[e._v("The client then sends the master an RPC request containing the file\nname and chunk index.")]),e._v(" "),n("li",[e._v("The master replies with the chunk handle and the location of replicas\nholding the chunk. The client caches this metadata using the file name\nand chunk-index as the key. This information is subsequently used to\naccess the data.")]),e._v(" "),n("li",[e._v("The  client then sends a request to one of the replicas (the closest one). The request specifies the chunk handle and a byte range within that chunk.\n"),n("ul",[n("li",[e._v("Further reads of the same chunk require no more client-master interaction until the cached information expires or the file is reopened.")]),e._v(" "),n("li",[e._v("In fact, the client typically asks for multiple chunks in the same request, and the master can also include the information for chunks immediately following those requested.")])])]),e._v(" "),n("li",[e._v("The replica ChunkServer replies with the requested data.")]),e._v(" "),n("li",[e._v("As evident from the above workflow, the master is involved at the start\nand is then completely out of the loop, implementing a separation of\ncontrol and data flows – a separation that is crucial for maintaining high\nperformance of file accesses.")])]),e._v(" "),n("p",[n("img",{attrs:{src:a(859),alt:"Figure 4"}}),n("br"),e._v(" "),n("em",[e._v("The anatomy of a read operation")])]),e._v(" "),n("h2",{attrs:{id:"anatomy-of-a-write-operation"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#anatomy-of-a-write-operation"}},[e._v("#")]),e._v(" Anatomy of a Write Operation")]),e._v(" "),n("h3",{attrs:{id:"what-is-a-chunk-lease"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#what-is-a-chunk-lease"}},[e._v("#")]),e._v(" What is a chunk lease?")]),e._v(" "),n("p",[e._v("To safeguard against concurrent writes at two different replicas of a chunk,\nGFS makes use of chunk lease. When a mutation (i.e., a write, append or\ndelete operation) is requested for a chunk, the master finds the\nChunkServers which hold that chunk and grants a chunk lease (for 60\nseconds) to one of them. The server with the lease is called the primary and\nis responsible for providing a serial order for all the currently pending\nconcurrent mutations to that chunk. There is only one lease per chunk at any\ntime, so that if two write requests go to the master, both see the same lease\ndenoting the same primary.")]),e._v(" "),n("p",[e._v("Thus, a global ordering is provided by the ordering of the chunk leases\ncombined with the order determined by that primary. The primary can\nrequest lease extensions if needed. When the master grants the lease, it\nincrements the chunk version number and informs all replicas containing\nthat chunk of the new version number.")]),e._v(" "),n("h3",{attrs:{id:"data-writing"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#data-writing"}},[e._v("#")]),e._v(" Data writing")]),e._v(" "),n("p",[e._v("The actual writing of data is split into two phases:")]),e._v(" "),n("ul",[n("li",[n("strong",[e._v("Sending")]),e._v(": First, the client is given a list of replicas that identifies the\nprimary ChunkServer and secondaries. The client sends the data to the\nclosest replica. Then replicas send the data in chain to all other replicas\nto maximize bandwidth and throughput. Eventually, all the replicas get\nthe data, which is not yet written to a file but sits in a cache.")]),e._v(" "),n("li",[n("strong",[e._v("Writing")]),e._v(": When the client gets an acknowledgment from all replicas that\nthe data has been received, it then sends a write request to the primary,\nidentifying the data that was sent in the previous phase. The primary is\nresponsible for the serialization of writes. It assigns consecutive serial\nnumbers to all write requests that it has received, applies the writes to\nthe file in serial-number order, and forwards the write requests in that\norder to the secondaries. Once the primary gets acknowledgments from\nall the secondaries, the primary responds back to the client, and the\nwrite operation is complete. Any errors at any stage in this process are\nmet with retries and eventual failure. On failure, an error is returned to\nthe client.")])]),e._v(" "),n("p",[e._v("Following is the stepwise breakdown of the data transfer:")]),e._v(" "),n("ol",[n("li",[e._v("Client asks master which chunk server holds the current lease of chunk\nand locations of other replicas.")]),e._v(" "),n("li",[e._v("Master replies with the identity of primary and locations of the\nsecondary replicas.")]),e._v(" "),n("li",[e._v("Client pushes data to the closest replica. Then replicas send the data in\nchain to all other replicas.")]),e._v(" "),n("li",[e._v("Once all replicas have acknowledged receiving the data, the client sends\nthe write request to the primary. The primary assigns consecutive serial\nnumbers to all the mutations it receives, providing serialization. It\napplies mutations in serial number order.")]),e._v(" "),n("li",[e._v("Primary forwards the write request to all secondary replicas. They\napply mutations in the same serial number order.")]),e._v(" "),n("li",[e._v("Secondary replicas reply to primary indicating they have completed\noperation.")]),e._v(" "),n("li",[e._v("Primary replies to the client with success or error message")])]),e._v(" "),n("p",[n("img",{attrs:{src:a(860),alt:"Figure 5"}}),n("br"),e._v(" "),n("em",[e._v("The anatomy of a write operation")])]),e._v(" "),n("p",[e._v("The key point to note is that the data flow is different from the control flow.\nThe data flows from the client to a ChunkServer and then from that\nChunkServer to another ChunkServer, until all ChunkServers that store\nreplicas for that chunk have received the data. The control (the write\nrequest) flow goes from the client to the primary ChunkServer for that\nchunk. The primary then forwards the request to all the secondaries. This\nensures that the primary controls the order of writes even if it receives\nmultiple concurrent write requests. All replicas will have data written in the\nsame sequence. Chunk version numbers are used to detect if any replica has\nstale data which has not been updated because that ChunkServer was down\nduring some update.")]),e._v(" "),n("h2",{attrs:{id:"anatomy-of-an-append-operation"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#anatomy-of-an-append-operation"}},[e._v("#")]),e._v(" Anatomy of an Append Operation")]),e._v(" "),n("p",[e._v("Record append operation is optimized in a unique way that distinguishes\nGFS from other distributed file systems. In a normal write, the client\nspecifies the offset at which data is to be written. Concurrent writes to the\nsame region can experience race conditions, and the region may end up\ncontaining data fragments from multiple clients. In a record append,\nhowever, the client specifies only the data. GFS appends it to the file at least\nonce atomically (i.e., as one continuous sequence of bytes) at an offset of\nGFS’s choosing and returns that offset to the client. This process is similar to\nthe append operation on a file opened with "),n("a",{attrs:{href:"https://man7.org/linux/man-pages/man2/open.2.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("O_APPEND"),n("OutboundLink")],1),e._v(" mode on a POSIXcompliant file system but without the race conditions when multiple writers\ndo so concurrently.")]),e._v(" "),n("p",[e._v("Record Append is a kind of mutation that changes the contents of the\nmetadata of a chunk. When an application tries to append data on a chunk\nby sending a request to the client, the client pushes the data to all replicas of\nthe last chunk of the file just like the write operation. When the client\nforwards the request to the master, the primary checks whether appending\nthe record to the existing chunk will increase the chunk’s size more than its\nlimit (maximum size of a chunk is 64MB). If this happens, it pads the chunk\nto the maximum limit, commands the secondary to do the same, and\nrequests the clients to try to append to the next chunk. If the record fits\nwithin the maximum size, the primary appends the data to its replica, tells\nthe secondary to write the data at the exact offset where it has, and finally\nreplies success to the client")]),e._v(" "),n("p",[e._v("If an append operation fails at any replica, the client retries the operation.\nDue to this reason, replicas of the same chunk may contain different data,\npossibly including duplicates of the same record in whole or in part. GFS\ndoes not guarantee that all replicas are byte-wise identical; instead, it only\nensures that the data is written at-least-once as an atomic unit.")]),e._v(" "),n("h2",{attrs:{id:"gfs-consistency-model-and-snapshotting"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#gfs-consistency-model-and-snapshotting"}},[e._v("#")]),e._v(" GFS Consistency Model and Snapshotting")]),e._v(" "),n("h3",{attrs:{id:"gfs-consistency-model"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#gfs-consistency-model"}},[e._v("#")]),e._v(" GFS consistency model")]),e._v(" "),n("p",[e._v("To keep things simple and efficient, GFS has a relaxed consistency model.")]),e._v(" "),n("p",[e._v("Metadata operations (e.g., file creation) are atomic. They are handled\nexclusively by the master. Namespace locking guarantees atomicity and\ncorrectness, whereas the master’s operation log defines a global total order\nof these operations.")]),e._v(" "),n("p",[e._v("In data mutations, there is an important distinction between "),n("code",[e._v("write")]),e._v(" and\n"),n("code",[e._v("append")]),e._v(" operations. "),n("code",[e._v("Write")]),e._v(" operations specify an offset at which mutations\nshould occur, whereas "),n("code",[e._v("appends")]),e._v(" are always applied at the end of the file. This\nmeans that for the "),n("code",[e._v("write")]),e._v(" operation, the offset in the chunk is\npredetermined, whereas for "),n("code",[e._v("append")]),e._v(" , the system decides. Concurrent "),n("code",[e._v("writes")]),e._v("\nto the same location are not serializable and may result in corrupted regions\nof the file. With "),n("code",[e._v("append")]),e._v(" operations, GFS guarantees the "),n("code",[e._v("append")]),e._v(" will happen\nat-least-once and atomically (that is, as a contiguous sequence of bytes). The\nsystem does not guarantee that all copies of the chunk will be identical (some\nmay have duplicate data).")]),e._v(" "),n("h3",{attrs:{id:"snapshotting"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#snapshotting"}},[e._v("#")]),e._v(" Snapshotting")]),e._v(" "),n("p",[e._v("A snapshot is a copy of some subtree of the global namespace as it exists at a\ngiven point in time. GFS clients use snapshotting to efficiently branch two\nversions of the same data. Snapshots in GFS are initially "),n("strong",[e._v("zero-copy")]),e._v(". This\nmeans that data copies are made only when clients make a request to modify\nthe chunks. This scheme is known as "),n("strong",[e._v("copy-on-write")]),e._v(".")]),e._v(" "),n("p",[e._v("When the master receives a snapshot request, it first revokes any\noutstanding leases on the chunks in the files to snapshot. It waits for leases\nto be revoked or expired and logs the snapshot operation to the operation\nlog. The snapshot is then made by duplicating the metadata for the source\ndirectory tree. Newly created snapshot files still point to the original chunks.")]),e._v(" "),n("p",[e._v("When a client makes a request to write to one of these chunks, the master\ndetects that it is a copy-on-write chunk by examining its reference count\n(which will be more than one). At this point, the master asks each\nChunkServer holding the replica to make a copy of the chunk and store it\nlocally. These local copies are made to avoid copying the chunk over the\nnetwork. Once the copy is complete, the master issues a lease for the new\ncopy, and the write proceeds.")]),e._v(" "),n("h2",{attrs:{id:"fault-tolerance-high-availability-and-data-integrity"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#fault-tolerance-high-availability-and-data-integrity"}},[e._v("#")]),e._v(" Fault Tolerance, High Availability, and Data Integrity")]),e._v(" "),n("h3",{attrs:{id:"fault-tolerance"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#fault-tolerance"}},[e._v("#")]),e._v(" Fault tolerance")]),e._v(" "),n("p",[e._v("To make the system fault-tolerant and available, GFS makes use of two\nsimple strategies:")]),e._v(" "),n("ol",[n("li",[n("strong",[e._v("Fast recovery")]),e._v(" in case of component failures.")]),e._v(" "),n("li",[n("strong",[e._v("Replication")]),e._v(" for high availability.")])]),e._v(" "),n("p",[e._v("Let’s first see how GFS recovers from master or replica failure:")]),e._v(" "),n("ul",[n("li",[n("strong",[e._v("On master failure")]),e._v(": The Master being a single point of failure, can make the entire system unavailable in a short time. To handle this, all operations applied on master are saved in an "),n("strong",[e._v("operation log")]),e._v(". This log is checkpointed and replicated on multiple remote machines, so that on recovery, a master may load the checkpoint into memory, replay any subsequent operations from the operation log, and be available again in a short amount of time. GFS relies on an external monitoring infrastructure to detect the master failure and switch the traffic to the backup master server.\n"),n("ul",[n("li",[n("strong",[e._v("Shadow masters")]),e._v(" are replicas of master and provide read-only\naccess to the file system even when the primary is down. All\nshadow masters keep themselves updated by applying the same\nsequence of updates exactly as the primary master does by reading\nits operation log. Shadow masters may lag the primary slightly, but\nthey enhance read availability for files that are not being actively\nchanged or applications that do not mind getting slightly stale\nmetadata. Since file contents are read from the ChunkServers,\napplications do not observe stale file contents.")])])]),e._v(" "),n("li",[n("strong",[e._v("On primary replica failure")]),e._v(": If an active primary replica fails (or there\nis a network partition), the master detects this failure (as there will be\nno heartbeat), and waits for the current lease to expire (in case the\nprimary replica is still serving traffic from clients directly), and then\nassigns the lease to a new node. When the old primary replica recovers,\nthe master will detect it as ‘stale’ by checking the version number of the\nchunks. The master node will pick new nodes to replace the stale node\nand garbage-collect it before it can join the group again.")]),e._v(" "),n("li",[n("strong",[e._v("On secondary replica failure")]),e._v(": If there is a replica failure, all client\noperations will start failing on it. When this happens, the client retries a\nfew times; if all of the retries fail, it reports failure to the master. This\ncan leave the secondary replica inconsistent because it misses some\nmutations. As described above, stale nodes will be replaced by new\nnodes picked by the master, and eventually garbage-collected.")])]),e._v(" "),n("div",{staticClass:"custom-block tip"},[n("p",{staticClass:"custom-block-title"},[e._v("TIP")]),e._v(" "),n("p",[e._v("Stale replicas might be exposed to clients. It depends on the\napplication programmer to deal with these stale reads. GFS does not\nguarantee strong consistency on chunk reads.")])]),e._v(" "),n("h3",{attrs:{id:"high-availability-through-chunk-replication"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#high-availability-through-chunk-replication"}},[e._v("#")]),e._v(" High availability through Chunk replication")]),e._v(" "),n("p",[e._v("As discussed earlier, each chunk is replicated on multiple ChunkServers on\ndifferent racks. Users can specify different replication levels for different\nparts of the file namespace. The default is three. The master clones the\nexisting replicas to keep each chunk fully replicated as ChunkServers go\noffline or when the master detects corrupted replicas through checksum\nverification.")]),e._v(" "),n("p",[e._v("A chunk is lost irreversibly only if all its replicas are lost before GFS can\nreact. Even in this case, the data becomes unavailable, not corrupted, which\nmeans applications receive clear errors rather than corrupt data.")]),e._v(" "),n("h3",{attrs:{id:"data-integrity-through-checksum"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#data-integrity-through-checksum"}},[e._v("#")]),e._v(" Data integrity through checksum")]),e._v(" "),n("p",[e._v("Checksumming is used by each ChunkServer to detect the corruption of\nstored data. The chunk is broken down into 64 KB blocks. Each has a\ncorresponding 32-bit checksum. Like other metadata, checksums are kept in\nmemory and stored persistently with logging, separate from user data.")]),e._v(" "),n("ol",[n("li",[n("strong",[e._v("For reads")]),e._v(", the ChunkServer verifies the checksum of data blocks that\noverlap the read range before returning any data to the requester,\nwhether a client or another ChunkServer. Therefore, ChunkServers will\nnot propagate corruptions to other machines. If a block does not match\nthe recorded checksum, the ChunkServer returns an error to the\nrequestor and reports the mismatch to the master. In response, the\nrequestor will read from other replicas, and the master will clone the\nchunk from another replica. After a valid new replica is in place, the master instructs the ChunkServer that reported the mismatch to delete\nits replica.")]),e._v(" "),n("li",[n("strong",[e._v("For writes")]),e._v(", ChunkServer verifies the checksum of first and last data\nblocks that overlap the write range before performing the write. Then, it\ncomputes and records the new checksums. For a corrupted block, the\nChunkServer returns an error to the requestor and reports the\nmismatch to the master.")]),e._v(" "),n("li",[n("strong",[e._v("For appends")]),e._v(", checksum computation is optimized as there is no\nchecksum verification on the last block; instead, just incrementally\nupdate the checksum for the last partial block and compute new\nchecksums for any brand-new blocks filed by the append. This way, if\nthe last partial block is already corrupted (and GFS fails to detect it\nnow), the new checksum value will not match the stored data, and the\ncorruption will be detected as usual when the block is next read.")])]),e._v(" "),n("p",[e._v("During idle periods, ChunkServers can scan and verify the contents of\ninactive chunks (prevents an inactive but corrupted chunk replica from\nfooling the master into thinking that it has enough valid replicas of a chunk).")]),e._v(" "),n("p",[e._v("Checksumming has little effect on read performance for the following\nreasons:")]),e._v(" "),n("ul",[n("li",[e._v("Since most of the reads span at least a few blocks, GFS needs to read and\nchecksum only a relatively small amount of extra data for verification.\nGFS client code further reduces this overhead by trying to align reads at\nchecksum block boundaries.")]),e._v(" "),n("li",[e._v("Checksum lookups and comparisons on the ChunkServer are done\nwithout any I/O.")]),e._v(" "),n("li",[e._v("Checksum calculation can often be overlapped with I/Os.")])]),e._v(" "),n("h2",{attrs:{id:"garbage-collection"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#garbage-collection"}},[e._v("#")]),e._v(" Garbage Collection")]),e._v(" "),n("h3",{attrs:{id:"garbage-collection-through-lazy-deletion"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#garbage-collection-through-lazy-deletion"}},[e._v("#")]),e._v(" Garbage collection through lazy deletion")]),e._v(" "),n("p",[e._v("When a file is deleted, GFS does not immediately reclaim the physical space\nused by that file. Instead, it "),n("strong",[e._v("follows a lazy garbage collection strategy.")]),e._v("\nWhen the client issues a delete file operation, GFS does two things:")]),e._v(" "),n("ol",[n("li",[e._v("The master logs the deletion operation just like other changes.")]),e._v(" "),n("li",[e._v("The deleted file is renamed to a hidden name that also includes a\ndeletion timestamp.")])]),e._v(" "),n("p",[e._v("The file can still be read under the new, special name and can also be\nundeleted by renaming it back to normal. To reclaim the physical storage,\nthe master, while performing regular scans of the file system, removes any\nsuch hidden files if they have existed for more than three days (this interval\nis configurable) and also deletes its in-memory metadata. This lazy deletion\nscheme provides a window of opportunity to a user who deleted a file by\nmistake to recover the file.")]),e._v(" "),n("p",[e._v("The master, while performing regular scans of chunk namespace, deletes the\nmetadata of all chunks that are not part of any file. Also, during the\nexchange of regular HeartBeat messages with the master, each ChunkServer\nreports a subset of the chunks it has, and the master replies with a list of\nchunks from that subset that are no longer present in the master’s database;\nsuch chunks are then deleted from the ChunkServer.")]),e._v(" "),n("h3",{attrs:{id:"advantages-of-lazy-deletion"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#advantages-of-lazy-deletion"}},[e._v("#")]),e._v(" Advantages of lazy deletion")]),e._v(" "),n("p",[e._v("Here are the advantages of lazy deletion.")]),e._v(" "),n("ul",[n("li",[e._v("Simple and reliable. If the chunk deletion message is lost, the master\ndoes not have to retry. The ChunkServer can perform the garbage\ncollection with the subsequent heartbeat messages.")]),e._v(" "),n("li",[e._v("GFS merges storage reclamation into regular background activities of\nthe master, such as the regular scans of the filesystem or the exchange\nof HeartBeat messages. Thus, it is done in batches, and the cost is\namortized.")]),e._v(" "),n("li",[e._v("Garbage collection takes place when the master is relatively free.")]),e._v(" "),n("li",[e._v("Lazy deletion provides safety against accidental, irreversible deletions.")])]),e._v(" "),n("h3",{attrs:{id:"disadvantages-of-lazy-deletion"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#disadvantages-of-lazy-deletion"}},[e._v("#")]),e._v(" Disadvantages of lazy deletion")]),e._v(" "),n("p",[e._v("As we know, after deletion, storage space does not become available\nimmediately. Applications that frequently create and delete files may not be\nable to reuse the storage right away. To overcome this, GFS provides\nfollowing options:")]),e._v(" "),n("ul",[n("li",[e._v("If a client deletes a deleted file again, GFS expedites the storage\nreclamation.")]),e._v(" "),n("li",[e._v("Users can specify directories that are to be stored without replication")]),e._v(" "),n("li",[e._v("Users can also specify directories where deletion takes place\nimmediately.")])]),e._v(" "),n("h2",{attrs:{id:"criticism-on-gfs"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#criticism-on-gfs"}},[e._v("#")]),e._v(" Criticism on GFS")]),e._v(" "),n("h3",{attrs:{id:"problems-associated-with-single-master"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#problems-associated-with-single-master"}},[e._v("#")]),e._v(" Problems associated with single master")]),e._v(" "),n("p",[e._v("As GFS has grown in usage, Google has started to see the following problems\nwith the centralized master scheme:")]),e._v(" "),n("ul",[n("li",[e._v("Despite the separation of control flow (i.e., metadata operations) and\ndata flow, the master is emerging as a bottleneck in the design. As the\nnumber of clients grows, a single master could not serve them because\nit does not have enough CPU power.")]),e._v(" "),n("li",[e._v("Despite the reduced amount of metadata (because of the large chunk\nsize), the amount of metadata stored by the master is increasing to a\nlevel where it is getting difficult to keep all the metadata in the main\nmemory.")])]),e._v(" "),n("h3",{attrs:{id:"problems-associated-with-large-chunk-size"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#problems-associated-with-large-chunk-size"}},[e._v("#")]),e._v(" Problems associated with large chunk size")]),e._v(" "),n("p",[e._v("Large chunk size (64MB) in GFS has its disadvantages while reading. Since a\nsmall file will have one or a few chunks, the ChunkServers storing those\nchunks can become hotspots if a lot of clients are accessing the same file. As\na workaround for this problem, GFS stores extra copies of small files for\ndistributing the load to multiple ChunkServers. Furthermore, GFS adds a\nrandom delay in the start times of the applications accessing such files")]),e._v(" "),n("h2",{attrs:{id:"summary"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#summary"}},[e._v("#")]),e._v(" Summary")]),e._v(" "),n("ul",[n("li",[e._v("GFS is a scalable distributed file storage system for large data-intensive applications.")]),e._v(" "),n("li",[e._v("GFS uses commodity hardware to reduce infrastructure costs.")]),e._v(" "),n("li",[e._v("GFS was designed with the understanding that system/hardware failures can and do occur.")]),e._v(" "),n("li",[e._v("Reading workload consists of large streaming reads and small random reads. Writing workloads consists of many large, sequential writes that append data to files.")]),e._v(" "),n("li",[e._v("GFS provides APIs for usual file operations like create, delete, open, close, read, and write. Additionally, GFS supports snapshot and record append operations. Snapshot creates a copy of the file or directory tree. Record append allows multiple clients to append data to the same file concurrently while guaranteeing atomicity.")]),e._v(" "),n("li",[e._v("A GFS cluster consists of a "),n("strong",[e._v("single master")]),e._v(" and "),n("strong",[e._v("multiple ChunkServers")]),e._v(" and is accessed by multiple clients.")]),e._v(" "),n("li",[n("strong",[e._v("Chunk")]),e._v(": Files are broken into fixed-size chunks where each chunk is 64 megabytes in size. Each chunk is identified by an immutable and globally unique "),n("strong",[e._v("64-bit chunk handle")]),e._v(" assigned by the master at the time of chunk creation.")]),e._v(" "),n("li",[e._v("ChunkServers store chunks on the local disk as Linux files.")]),e._v(" "),n("li",[e._v("For reliability, each chunk is replicated on multiple ChunkServers.")]),e._v(" "),n("li",[e._v("Master server is the coordinator of a GFS cluster and is responsible for keeping track of all the filesystem metadata. This includes namespace, authorization, mapping of files to chunks, and the current location of chunks.")]),e._v(" "),n("li",[e._v("Master keeps all metadata in memory for faster operations. For fault tolerance and to handle a master crash, all metadata changes are written to the disk onto an "),n("strong",[e._v("operation log.")]),e._v(" This operation log is also replicated onto remote machines.")]),e._v(" "),n("li",[e._v("The master does not keep a persistent record of which ChunkServers have a replica of a given chunk. Instead, the master asks each\nChunkServer about what chunks it holds at master startup or whenever a ChunkServer joins the cluster.")]),e._v(" "),n("li",[n("strong",[e._v("Checkpointing")]),e._v(": The master's state is periodically serialized to disk and then replicated so that on recovery, a master may load the checkpoint into memory, replay any subsequent operations from the operation log, and be available again very quickly.")]),e._v(" "),n("li",[n("strong",[e._v("HeartBeat")]),e._v(": The master communicates with each ChunkServer through\nHeartbeat messages to pass instructions to it and collects its state.")]),e._v(" "),n("li",[n("strong",[e._v("Client")]),e._v(": GFS client code which is linked into each application, implements filesystem APIs, and communicates with the cluster. Clients interact with the master for metadata, but all data transfers happen directly between the client and ChunkServers.")]),e._v(" "),n("li",[n("strong",[e._v("Data Integrity")]),e._v(": Each ChunkServer uses checksumming to detect the corruption of stored data.")]),e._v(" "),n("li",[n("strong",[e._v("Garbage Collection")]),e._v(": After a file is deleted, GFS does not immediately reclaim the available physical storage. It does so only lazily during regular garbage collection at both the file and chunk levels.")]),e._v(" "),n("li",[n("strong",[e._v("Consistency")]),e._v(": Master guarantees data consistency by ensuring the order of mutations on all replicas and using chunk version numbers. If a replica has an incorrect version, it is garbage collected.")]),e._v(" "),n("li",[e._v("GFS guarantees "),n("strong",[e._v("at-least-once")]),e._v(" writes for writers. This means that records could be written more than once as well (although rarely). It is the responsibility of the readers to deal with these duplicate chunks.\nThis is achieved by having checksums and serial numbers in the chunks, which help readers to filter and discard duplicate data.")]),e._v(" "),n("li",[n("strong",[e._v("Cache")]),e._v(": Neither the client nor the ChunkServer caches file data. Client caches offer little benefit because most applications stream through huge files or have working sets too large to be cached. However, clients do cache metadata.")])]),e._v(" "),n("h3",{attrs:{id:"system-design-patterns"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#system-design-patterns"}},[e._v("#")]),e._v(" System design patterns")]),e._v(" "),n("p",[e._v("Here is a summary of system design patterns used in GFS.")]),e._v(" "),n("ul",[n("li",[n("strong",[e._v("Write-Ahead Log")]),e._v(": For fault-tolerance and in the event of a master crash, all metadata changes are written to the disk onto an operation log which is a write-ahead log.")]),e._v(" "),n("li",[n("strong",[e._v("HeartBeat")]),e._v(": The GFS master periodically communicates with each\nChunkServer in HeartBeat messages to give it instructions and collect its state.")]),e._v(" "),n("li",[n("strong",[e._v("Checksum")]),e._v(": Each ChunkServer uses checksumming to detect the corruption of stored data.")])])])}),[],!1,null,null,null);t.default=i.exports},856:function(e,t,a){e.exports=a.p+"assets/img/f1.5761a2e5.png"},857:function(e,t,a){e.exports=a.p+"assets/img/f2.651ca88e.png"},858:function(e,t,a){e.exports=a.p+"assets/img/f3.20c54b7a.png"},859:function(e,t,a){e.exports=a.p+"assets/img/f4.81407ff3.png"},860:function(e,t,a){e.exports=a.p+"assets/img/f5.e6eefade.png"}}]);