(window.webpackJsonp=window.webpackJsonp||[]).push([[24],{582:function(e,t,a){e.exports=a.p+"assets/img/image--041.29e4a17a.jpg"},583:function(e,t,a){e.exports=a.p+"assets/img/image--042.9fdac72d.png"},584:function(e,t,a){e.exports=a.p+"assets/img/image--043.01457d8c.png"},887:function(e,t,a){"use strict";a.r(t);var n=a(7),o=Object(n.a)({},(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("h1",{attrs:{id:"_20-designing-for-performance"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_20-designing-for-performance"}},[e._v("#")]),e._v(" 20. Designing for Performance")]),e._v(" "),n("p",[e._v("Up until this point, the discussion of software design has focused on complexity; the goal has been to make software as simple and understandable as possible. But what if you are working on a system that needs to be fast? How should performance considerations affect the design process? This chapter discusses how to achieve high performance without sacrificing clean design. The most important idea is still simplicity: not only does simplicity improve a system’s design, but it usually makes systems faster.")]),e._v(" "),n("h2",{attrs:{id:"_20-1-how-to-think-about-performance"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_20-1-how-to-think-about-performance"}},[e._v("#")]),e._v(" 20.1  How to think about performance")]),e._v(" "),n("p",[e._v("The first question to address is “how much should you worry about performance during the normal development process?” If you try to optimize every statement for maximum speed, it will slow down\ndevelopment and create a lot of unnecessary complexity. Furthermore, many of the “optimizations” won’t actually help performance. On the other hand, if you completely ignore performance issues, it’s easy to end up with a large number of significant inefficiencies spread throughout the code; the resulting system can easily be 5–10x slower than it needs to be.")]),e._v(" "),n("p",[e._v("In this “death by a thousand cuts” scenario it’s hard to come back later and\nimprove the performance, because there is no single improvement that will\nhave much impact.")]),e._v(" "),n("p",[e._v("The best approach is something between these extremes, where you\nuse basic knowledge of performance to choose design alternatives that are\n“naturally efficient” yet also clean and simple. The key is to develop an\nawareness of which operations are fundamentally expensive. Here are a\nfew examples of operations that are relatively expensive today:")]),e._v(" "),n("p",[e._v("Network communication: even within a datacenter, a round-trip\nmessage exchange can take 10–50 μs, which is tens of thousands of\ninstruction times. Wide-area round-trips can take 10–100 ms.\nI/O to secondary storage: disk I/O operations typically take 5–10 ms,\nwhich is millions of instruction times. Flash storage takes 10–100 μs.\nNew emerging nonvolatile memories may be as fast as 1 μs, but this\nis still around 2000 instruction times.\nDynamic memory allocation (malloc in C, new in C++ or Java)\ntypically involves significant overhead for allocation, freeing, and\ngarbage collection.\nCache misses: fetching data from DRAM into an on-chip processor\ncache takes a few hundred instruction times; in many programs,\noverall performance is determined as much by cache misses as by\ncomputational costs.\nThe best way to learn which things are expensive is to run micro-\nbenchmarks (small programs that measure the cost of a single operation in\nisolation). In the RAMCloud project, we created a simple program that\nprovides a framework for microbenchmarks. It took a few days to create\nthe framework, but the framework makes it possible to add new micro-\nbenchmarks in five or ten minutes. This has allowed us to accumulate\ndozens of micro-benchmarks. We use these both to understand the\nperformance of existing libraries used in RAMCloud, and also to measure\nthe performance of new classes written for RAMCloud.")]),e._v(" "),n("p",[e._v("Once you have a general sense for what is expensive and what is cheap,\nyou can use that information to choose cheap operations whenever\npossible. In many cases, a more efficient approach will be just as simple\nas a slower approach. For example, when storing a large collection of\nobjects that will be looked up using a key value, you could use either a\nhash table or an ordered map. Both are commonly available in library\npackages, and both are simple and clean to use. However, hash tables can\neasily be 5–10x faster. Thus, you should always use a hash table unless you\nneed the ordering properties provided by the map.")]),e._v(" "),n("p",[e._v("As another example, consider allocating an array of structures in a\nlanguage such as C or C++. There are two ways you can do this. One way is\nfor the array to hold pointers to structures, in which case you must first")]),e._v(" "),n("p",[e._v("allocate space for the array, then allocate space for each individual\nstructure. It is much more efficient to store the structures in the array\nitself, so you only allocate one large block for everything.")]),e._v(" "),n("p",[e._v("If the only way to improve efficiency is by adding complexity, then the\nchoice is more difficult. If the more efficient design adds only a small\namount of complexity, and if the complexity is hidden, so it doesn’t affect\nany interfaces, then it may be worthwhile (but beware: complexity is\nincremental). If the faster design adds a lot of implementation complexity,\nor if it results in more complicated interfaces, then it may be better to start\noff with the simpler approach and optimize later if performance turns out\nto be a problem. However, if you have clear evidence that performance will\nbe important in a particular situation, then you might as well implement\nthe faster approach immediately.")]),e._v(" "),n("p",[e._v("In the RAMCloud project one of our overall goals was to provide the\nlowest possible latency for client machines accessing the storage system\nover a datacenter network. As a result, we decided to use special hardware\nfor networking, which allowed RAMCloud to bypass the kernel and\ncommunicate directly with the network interface controller to send and\nreceive packets. We made this decision even though it added complexity,\nbecause we knew from prior measurements that kernel-based networking\nwould be too slow to meet our needs. In most of the rest of the RAMCloud\nsystem we were able to design for simplicity; getting this one big issue\n“right” made many other things easier.")]),e._v(" "),n("p",[e._v("In general, simpler code tends to run faster than complex code. If you\nhave defined away special cases and exceptions, then no code is needed to\ncheck for those cases and the system runs faster. Deep classes are more\nefficient than shallow ones, because they get more work done for each\nmethod call. Shallow classes result in more layer crossings, and each layer\ncrossing adds overhead.")]),e._v(" "),n("h2",{attrs:{id:"_20-2-measure-before-modifying"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_20-2-measure-before-modifying"}},[e._v("#")]),e._v(" 20.2  Measure before modifying")]),e._v(" "),n("p",[e._v("But suppose that your system is still too slow, even though you have\ndesigned it as described above. It’s tempting to rush off and start making\nperformance tweaks, based on your intuitions about what is slow. Don’t do")]),e._v(" "),n("p",[e._v("this! Programmers’ intuitions about performance are unreliable. This is\ntrue even for experienced developers. If you start making changes based on\nintuition, you’ll waste time on things that don’t actually improve\nperformance, and you’ll probably make the system more complicated in\nthe process.")]),e._v(" "),n("p",[e._v("Before making any changes, measure the system’s existing behavior.\nThis serves two purposes. First, the measurements will identify the places\nwhere performance tuning will have the biggest impact. It isn’t sufficient\njust to measure the top-level system performance. This may tell you that\nthe system is too slow, but it won’t tell you why. You’ll need to measure\ndeeper to identify in detail the factors that contribute to overall\nperformance; the goal is to identify a small number of very specific places\nwhere the system is currently spending a lot of time, and where you have\nideas for improvement. The second purpose of the measurements is to\nprovide a baseline, so that you can re-measure performance after making\nyour changes to ensure that performance actually improved. If the changes\ndidn’t make a measurable difference in performance, then back them out\n(unless they made the system simpler). There’s no point in retaining\ncomplexity unless it provides a significant speedup.")]),e._v(" "),n("h2",{attrs:{id:"_20-3-design-around-the-critical-path"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_20-3-design-around-the-critical-path"}},[e._v("#")]),e._v(" 20.3  Design around the critical path")]),e._v(" "),n("p",[e._v("At this point, let’s assume that you have carefully analyzed performance\nand have identified a piece of code that is slow enough to affect the overall\nsystem performance. The best way to improve its performance is with a\n“fundamental” change, such as introducing a cache, or using a different\nalgorithmic approach (balanced tree vs. list, for instance). Our decision to\nbypass the kernel for network communication in RAMCloud is an example\nof a fundamental fix. If you can identify a fundamental fix, then you can\nimplement it using the design techniques discussed in previous chapters.")]),e._v(" "),n("p",[e._v("Unfortunately, situations will sometimes arise where there isn’t a\nfundamental fix. This brings us to the core issue for this chapter, which is\nhow to redesign an existing piece of code so that it runs faster. This should\nbe your last resort, and it shouldn’t happen often, but there are cases where\nit can make a big difference. The key idea is to design the code around the\ncritical path.")]),e._v(" "),n("p",[e._v("Start off by asking yourself what is the smallest amount of code that\nmust be executed to carry out the desired task in the common case.\nDisregard any existing code structure. Imagine instead that you are writing\na new method that implements just the critical path, which is the\nminimum amount of code that must be executed in the the most common\ncase. The current code is probably cluttered with special cases; ignore\nthem in this exercise. The current code might pass through several method\ncalls on the critical path; imagine instead that you could put all the\nrelevant code in a single method. The current code may also use a variety\nof variables and data structures; consider only the data needed for the\ncritical path, and assume whatever data structure is most convenient for\nthe critical path. For example, it may make sense to combine multiple\nvariables into a single value. Assume that you could completely redesign\nthe system in order to minimize the code that must be executed for the\ncritical path. Let’s call this code “the ideal.”")]),e._v(" "),n("p",[e._v("The ideal code probably clashes with your existing class structure, and\nit may not be practical, but it provides a good target: this represents the\nsimplest and fastest that the code can ever be. The next step is to look for a\nnew design that comes as close as possible to the ideal while still having a\nclean structure. You can apply all of the design ideas from previous\nchapters of this book, but with the additional constraint of keeping the\nideal code (mostly) intact. You may have to add a bit of extra code to the\nideal in order to allow clean abstractions; for example, if the code involves\na hash table lookup, it’s OK to introduce an extra method call to a general-\npurpose hash table class. In my experience it’s almost always possible to\nfind a design that is clean and simple, yet comes very close to the ideal.")]),e._v(" "),n("p",[e._v("One of the most important things that happens in this process is to\nremove special cases from the critical path. When code is slow, it’s often\nbecause it must handle a variety of situations, and the code gets structured\nto simplify the handling of all the different cases. Each special case adds a\nlittle bit of code to the critical path, in the form of extra conditional\nstatements and/or method calls. Each of these additions makes the code a\nbit slower. When redesigning for performance, try to minimize the number\nof special cases you must check. Ideally, there will be a single if")]),e._v(" "),n("p",[e._v("statement at the beginning, which detects all special cases with one test. In")]),e._v(" "),n("p",[e._v("the normal case, only this one test will need to be made, after which the\nthe critical path can be executed with no additional tests for special cases.\nIf the initial test fails (which means a special case has occurred) the code\ncan branch to a separate place off the critical path to handle it.\nPerformance isn’t as important for special cases, so you can structure the\nspecial-case code for simplicity rather than performance.")]),e._v(" "),n("h2",{attrs:{id:"_20-4-an-example-ramcloud-buers"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_20-4-an-example-ramcloud-buers"}},[e._v("#")]),e._v(" 20.4  An example: RAMCloud Buers")]),e._v(" "),n("p",[e._v("Let’s consider an example, in which the Buffer class of the RAMCloud\nstorage system was optimized to achieve a speedup of about 2x for the\nmost common operations.")]),e._v(" "),n("p",[e._v("RAMCloud uses Buffer objects to manage variable-length arrays of\nmemory, such as request and response messages for remote procedure\ncalls. Buffers are designed to reduce overheads from memory copying and\ndynamic storage allocation. A Buffer stores what appears to be a linear\narray of bytes, but for efficiency it allows the underlying storage to be\ndivided into multiple discontiguous chunks of memory, as shown in Figure\n20.1. A Buffer is created by appending chunks of data. Each chunk is either\nexternal or internal. If a chunk is external, its storage is owned by the\ncaller; the Buffer keeps a reference to this storage. External chunks are\ntypically used for large chunks in order to avoid memory copies. If a\nchunk is internal, the Buffer owns the storage for the chunk; data supplied\nby the caller is copied into the Buffer’s internal storage. Each Buffer\ncontains a small built-in allocation, which is a block of memory available\nfor storing internal chunks. If this space is exhausted, then the Buffer\ncreates additional allocations, which must be freed when the Buffer is\ndestroyed. Internal chunks are convenient for small chunks where the\nmemory copying costs are negligible. Figure 20.1 shows a Buffer with 5\nchunks: the first chunk is internal, the next two are external, and the final\ntwo chunks are internal.")]),e._v(" "),n("p",[n("img",{attrs:{src:a(582),alt:"Image"}})]),e._v(" "),n("p",[n("strong",[e._v("Figure 20.1")]),e._v(": "),n("em",[e._v("A Buffer object uses a collection of memory chunks to store what appears to be a linear array of bytes. Internal chunks are owned by the Buffer and freed when the Buffer is destroyed; external chunks are not owned by the Buffer.")])]),e._v(" "),n("p",[e._v("The Buffer class itself represents a “fundamental fix,” in that it\neliminates expensive memory copies that would have been required\nwithout it. For example, when assembling a response message containing a\nshort header and the contents of a large object in the RAMCloud storage\nsystem, RAMCloud uses a Buffer with two chunks. The first chunk is an\ninternal one that contains the header; the second chunk is an external one\nthat refers to the object contents in the RAMCloud storage system. The\nresponse can be collected in the Buffer without copying the large object.")]),e._v(" "),n("p",[e._v("Aside from the fundamental approach of allowing discontiguous\nchunks, we did not attempt to optimize the code of the Buffer class in the\noriginal implementation. Over time, however, we noticed Buffers being\nused in more and more situations; for example, at least four Buffers are\ncreated during the execution of each remote procedure call. Eventually, it\nbecame clear that speeding up the implementation of Buffer could have a\nnoticeable impact on overall system performance. We decided to see if we\ncould improve the performance of the Buffer class.")]),e._v(" "),n("p",[e._v("The most common operation for Buffer is to allocate space for a small\namount of new data using an internal chunk. This happens, for example,\nwhen creating headers for request and response messages. We decided to\nuse this operation as the critical path for optimization. In the simplest\npossible case, the space can be allocated by enlarging the last existing\nchunk in the Buffer. However, this is only possible if the last existing\nchunk is internal, and if there is enough space in its allocation to\naccommodate the new data. The ideal code would perform a single check")]),e._v(" "),n("p",[e._v("to confirm that the simple approach is possible, then it would adjust the\nsize of the existing chunk.")]),e._v(" "),n("p",[e._v("Figure 20.2 shows the original code for the critical path, which starts\nwith the method "),n("code",[e._v("Buffer::alloc")]),e._v(". In the fastest possible case, "),n("code",[e._v("Buffer::alloc")]),e._v(" calls "),n("code",[e._v("Buffer:: allocateAppend")]),e._v(", which calls "),n("code",[e._v("Buffer::Allocation::allocateAppend")]),e._v(". From a performance standpoint, this code has two problems. The first problem is that numerous special cases are checked individually:")]),e._v(" "),n("ul",[n("li",[e._v("Buffer::allocateAppend checks to see if the Buffer currently has any\nallocations.")]),e._v(" "),n("li",[e._v("The code checks twice to see if the current allocation has enough room for the new data: once in "),n("code",[e._v("Buffer::Allocation::allocateAppend")]),e._v(", and again when its return value is tested by "),n("code",[e._v("Buffer::allocateAppend")]),e._v(".")]),e._v(" "),n("li",[n("code",[e._v("Buffer::alloc")]),e._v(" tests the return value from "),n("code",[e._v("Buffer::allocAppend")]),e._v(" to confirm yet again that the allocation succeeded.")])]),e._v(" "),n("p",[e._v("Furthermore, rather than trying to expand the last chunk directly, the code allocates new space without any consideration of the last chunk. Then "),n("code",[e._v("Buffer::alloc")]),e._v(" checks to see if that space happens to be adjacent to the last chunk, in which case it merges the new space with the existing chunk. This results in additional checks. Overall, this code tests 6 distinct conditions in the critical path.")]),e._v(" "),n("p",[e._v("The second problem with the original code is that it has too many\nlayers, all of which are shallow. This is both a performance problem and a\ndesign problem. The critical path makes two additional method calls in\naddition to the original invocation of Buffer::alloc. Each method call")]),e._v(" "),n("p",[e._v("takes additional time, and the result of each call must be checked by its\ncaller, which results in more special cases to consider. Chapter 7 discussed\nhow abstractions should normally change as you pass from one layer to\nanother, but all three of the methods in Figure 20.2 have identical\nsignatures and they provide essentially the same abstraction; this is a red\nflag. Buffer::allocateAppend is nearly a pass-though method; its only\ncontribution is to create a new allocation if needed. The extra layers make\nthe code both slower and more complicated.")]),e._v(" "),n("p",[e._v("To fix these problems, we refactored the Buffer class so that its design\nis centered around the most performance-critical paths. We considered not\njust the allocation code above but several other commonly executed paths,\nsuch as retrieving the total number of bytes of data currently stored in a\nBuffer. For each of these critical paths, we tried to identify the smallest\namount of code that must be executed in the common case. Then we\ndesigned the rest of the class around these critical paths. We also applied\nthe design principles from this book to simplify the class in general. For\nexample, we eliminated shallow layers and created deeper internal\nabstractions. The refactored class is 20% smaller than the original version\n(1476 lines of code, versus 1886 lines in the original).")]),e._v(" "),n("p",[n("img",{attrs:{src:a(583),alt:"Image"}})]),e._v(" "),n("p",[n("strong",[e._v("Figure 20.2")]),e._v(": "),n("em",[e._v("The original code for allocating new space at the end of a Buffer, using an internal chunk.")])]),e._v(" "),n("p",[n("img",{attrs:{src:a(584),alt:"Image"}})]),e._v(" "),n("p",[n("strong",[e._v("Figure 20.3")]),e._v(": "),n("em",[e._v("The new code for allocating new space in an internal chunk of a Buffer.")])]),e._v(" "),n("p",[e._v("Figure 20.3 shows the new critical path for allocating internal space in\na Buffer. The new code is not only faster, but it is also easier to read, since\nit avoids shallow abstractions. The entire path is handled in a single\nmethod, and it uses a single test to rule out all of the special cases. The\nnew code introduces a new instance variable, extraAppendBytes, in order to")]),e._v(" "),n("p",[e._v("simplify the critical path. This variable keeps track of how much unused\nspace is available immediately after the last chunk in the Buffer. If there is\nno space available, or if the last chunk in the Buffer isn’t an internal\nchunk, or if the Buffer contains no chunks at all, then extraAppendBytes is\nzero. The code in Figure 20.3 represents the least possible amount of code\nto handle this common case.")]),e._v(" "),n("p",[e._v("Note: the update to totalLength could have been eliminated by\nrecomputing the total Buffer length from the individual chunks whenever\nit is needed. However, this approach would be expensive for a large Buffer\nwith many chunks, and fetching the total Buffer length is another common\noperation. Thus, we chose to add a small amount of extra overhead to\nalloc in order to ensure that the Buffer length is always immediately\navailable.")]),e._v(" "),n("p",[e._v("The new code is about twice as fast as the old code: the total time to\nappend a 1-byte string to a Buffer using internal storage dropped from 8.8\nns to 4.75 ns. Many other Buffer operations also speeded up because of the\nrevisions. For example, the time to construct a new Buffer, append a small\nchunk in internal storage, and destroy the Buffer dropped from 24 ns to 12\nns.")]),e._v(" "),n("h2",{attrs:{id:"_20-5-conclusion"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_20-5-conclusion"}},[e._v("#")]),e._v(" 20.5  Conclusion")]),e._v(" "),n("p",[e._v("The most important overall lesson from this chapter is that clean design\nand high performance are compatible. The Buffer class rewrite improved\nits performance by a factor of 2 while simplifying its design and reducing\ncode size by 20%. Complicated code tends to be slow because it does\nextraneous or redundant work. On the other hand, if you write clean,\nsimple code, your system will probably be fast enough that you don’t have\nto worry much about performance in the first place. In the few cases where\nyou do need to optimize performance, the key is simplicity again: find the\ncritical paths that are most important for performance and make them as\nsimple as possible.")])])}),[],!1,null,null,null);t.default=o.exports}}]);