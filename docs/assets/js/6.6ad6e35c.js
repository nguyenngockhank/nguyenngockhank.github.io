(window.webpackJsonp=window.webpackJsonp||[]).push([[6],{1069:function(e,t,a){e.exports=a.p+"assets/img/f1.0125b647.png"},1070:function(e,t,a){e.exports=a.p+"assets/img/f2.38a0f459.png"},1071:function(e,t,a){e.exports=a.p+"assets/img/f3.38f21124.png"},1072:function(e,t,a){e.exports=a.p+"assets/img/f4.b8918324.png"},1073:function(e,t,a){e.exports=a.p+"assets/img/f5.30b8e48d.png"},1074:function(e,t,a){e.exports=a.p+"assets/img/f6.e82674de.png"},1075:function(e,t,a){e.exports=a.p+"assets/img/f7.47c82c7c.png"},1076:function(e,t,a){e.exports=a.p+"assets/img/f8.2a56a44a.png"},1077:function(e,t,a){e.exports=a.p+"assets/img/f9.828da5d4.png"},1078:function(e,t,a){e.exports=a.p+"assets/img/f10.30887632.png"},1079:function(e,t,a){e.exports=a.p+"assets/img/f11.013fc037.png"},1080:function(e,t,a){e.exports=a.p+"assets/img/f12.24ad86c6.png"},1081:function(e,t,a){e.exports=a.p+"assets/img/f13.197eb490.png"},1082:function(e,t,a){e.exports=a.p+"assets/img/f14.9d17d969.png"},1083:function(e,t,a){e.exports=a.p+"assets/img/f15.319d02df.png"},1084:function(e,t,a){e.exports=a.p+"assets/img/f16.9bba1d5f.png"},1085:function(e,t,a){e.exports=a.p+"assets/img/f17.35650705.png"},1086:function(e,t,a){e.exports=a.p+"assets/img/f18.d45bee39.png"},1087:function(e,t,a){e.exports=a.p+"assets/img/f19.9f33b331.png"},1088:function(e,t,a){e.exports=a.p+"assets/img/f20.c97d4dda.png"},1089:function(e,t,a){e.exports=a.p+"assets/img/f21.3dc01715.png"},1090:function(e,t,a){e.exports=a.p+"assets/img/f22.81ad6443.png"},1091:function(e,t,a){e.exports=a.p+"assets/img/f23.0e242dc5.png"},1092:function(e,t,a){e.exports=a.p+"assets/img/f24.26f99835.png"},1093:function(e,t,a){e.exports=a.p+"assets/img/f25.84b52e78.png"},1094:function(e,t,a){e.exports=a.p+"assets/img/f26.6f8e0591.png"},1662:function(e,t,a){"use strict";a.r(t);var s=a(7),o=Object(s.a)({},(function(){var e=this,t=e.$createElement,s=e._self._c||t;return s("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[s("h1",{attrs:{id:"s3-like-object-storage"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#s3-like-object-storage"}},[e._v("#")]),e._v(" S3-like object storage")]),e._v(" "),s("p",[e._v("We design an object storage service similar to Amazon Simple Storage Service (S3). S3 is a service offered by Amazon Web Services (AWS) that provides object storage through a RESTful API-based interface. Here are some facts about AWS S3:")]),e._v(" "),s("ul",[s("li",[e._v("Launched in June 2006.")]),e._v(" "),s("li",[e._v("S3 added versioning, bucket policy, and multipart upload support in 2010.")]),e._v(" "),s("li",[e._v("S3 added server-side encryption, multi-object delete, and object expiration in 2011")]),e._v(" "),s("li",[e._v("Amazon reported 2 trillion objects stored in 3 by 2013,")]),e._v(" "),s("li",[e._v("Life cycle policy, event notification, and cross-region replication support were introduced in 2014 and 2015,")]),e._v(" "),s("li",[e._v("Amazon reported over 100 trillion objects stored in S3 by 2021")])]),e._v(" "),s("p",[e._v("Before we dig into object storage, let's first review storage systems in general and define some terminologies.")]),e._v(" "),s("h2",{attrs:{id:"storage-system-101"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#storage-system-101"}},[e._v("#")]),e._v(" Storage System 101")]),e._v(" "),s("p",[e._v("At a high-level, storage systems fall into three broad categories:")]),e._v(" "),s("ul",[s("li",[e._v("Block storage")]),e._v(" "),s("li",[e._v("File storage")]),e._v(" "),s("li",[e._v("Object storage")])]),e._v(" "),s("h3",{attrs:{id:"block-storage"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#block-storage"}},[e._v("#")]),e._v(" Block storage")]),e._v(" "),s("p",[e._v("Block storage came first, in the 1960s. Common storage devices like hard disk drives (HDD) and solid-state drives\n(SSD) that are physically attached to servers are all considered as block storage.")]),e._v(" "),s("p",[e._v("Block storage presents the raw blocks to the server as a volume. This is the most flexible and versatile form of\nstorage. The server can format the raw blocks and use them as a file system, or it can hand control of those blocks\nto an application. Some applications like a database or a virtual machine engine manage these blocks directly in\norder to squeeze every drop of performance out of them.")]),e._v(" "),s("p",[e._v("Block storage is not limited to physically attached storage. Block storage could be connected to a server over a\nhigh-speed network or over industry-standard connectivity protocols like Fibre Channel (FC) [1] and iSCSI [2].\nConceptually, the network-attached block storage still presents raw blocks. To the servers, it works the same as\nphysically attached block storage.")]),e._v(" "),s("h3",{attrs:{id:"file-storage"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#file-storage"}},[e._v("#")]),e._v(" File storage")]),e._v(" "),s("p",[e._v("File storage is built on top of block storage. It provides a higher-level abstraction to make it easier to handle files\nand directories. Data is stored as files under a hierarchical directory structure. File storage is the most common\ngeneral-purpose storage solution. File storage could be made accessible by a large number of servers using\ncommon file-level network protocols like SMB/CIFS [3] and NFS [4]. The servers accessing file storage do not need\nto deal with the complexity of managing the blocks, formatting volume, etc. The simplicity of file storage makes it a\ngreat solution for sharing large number of files and folders within an organization.")]),e._v(" "),s("h3",{attrs:{id:"object-storage"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#object-storage"}},[e._v("#")]),e._v(" Object storage")]),e._v(" "),s("p",[e._v("Object storage is new. It makes a very deliberate tradeoff to sacrifice performance for high durability, vast scale, and\nlow cost. It targets relatively “cold” data and is mainly used for archival and backup. Object storage stores all data\nas objects in a flat structure. There is no hierarchical directory structure. Data access is normally provided via a\nRESTful API. It is relatively slow compared to other storage types. Most public cloud service providers have an\nobject storage offering, such as AWS S3, Google object storage, and Azure blob storage,")]),e._v(" "),s("h3",{attrs:{id:"comparison"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#comparison"}},[e._v("#")]),e._v(" Comparison")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1069),alt:"Figure 1"}}),s("br"),e._v(" "),s("em",[e._v("Figure 1 Three different storage options")])]),e._v(" "),s("p",[e._v("Table 1 compares block storage, file storage, and object storage.")]),e._v(" "),s("table",[s("thead",[s("tr",[s("th",[e._v(".")]),e._v(" "),s("th",[e._v("Block storage")]),e._v(" "),s("th",[e._v("File storage")]),e._v(" "),s("th",[e._v("Object storage")])])]),e._v(" "),s("tbody",[s("tr",[s("td",[e._v("Mutable Content")]),e._v(" "),s("td",[e._v("Y")]),e._v(" "),s("td",[e._v("Y")]),e._v(" "),s("td",[e._v("N (object versioning is supported, in-place update is not)")])]),e._v(" "),s("tr",[s("td",[e._v("Cost")]),e._v(" "),s("td",[e._v("High")]),e._v(" "),s("td",[e._v("Medium to high")]),e._v(" "),s("td",[e._v("Low")])]),e._v(" "),s("tr",[s("td",[e._v("Performance")]),e._v(" "),s("td",[e._v("Medium to high, very high")]),e._v(" "),s("td",[e._v("Medium to high")]),e._v(" "),s("td",[e._v("Low to medium")])]),e._v(" "),s("tr",[s("td",[e._v("Consistency")]),e._v(" "),s("td",[e._v("Strong consistency")]),e._v(" "),s("td",[e._v("Strong consistency")]),e._v(" "),s("td",[e._v("Strong consistency")])]),e._v(" "),s("tr",[s("td",[e._v("Data access")]),e._v(" "),s("td",[e._v("SAS / iSCSI / FC")]),e._v(" "),s("td",[e._v("Standard file access, CIFS/SMB, and NFS")]),e._v(" "),s("td",[e._v("RESTful API")])]),e._v(" "),s("tr",[s("td",[e._v("Scalability")]),e._v(" "),s("td",[e._v("Medium Scalability")]),e._v(" "),s("td",[e._v("High Scalability")]),e._v(" "),s("td",[e._v("Vast Scalability")])]),e._v(" "),s("tr",[s("td",[e._v("Good for")]),e._v(" "),s("td",[e._v("Virtual Machine (VM), high perf apps like database")]),e._v(" "),s("td",[e._v("General-purpose file system access")]),e._v(" "),s("td",[e._v("Binary data, unstructured data")])])])]),e._v(" "),s("p",[s("em",[e._v("Table 1 Storage options")])]),e._v(" "),s("h3",{attrs:{id:"terminology"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#terminology"}},[e._v("#")]),e._v(" Terminology")]),e._v(" "),s("p",[e._v("To design S3-like object storage, we need to understand some core object storage concepts first. This section\nprovides an overview of the terms that apply to object storage.")]),e._v(" "),s("p",[s("strong",[e._v("Bucket")]),e._v(". A logical container for objects. The bucket name is globally unique. To upload data to S3, we must first\ncreate a bucket.")]),e._v(" "),s("p",[s("strong",[e._v("Object")]),e._v(". An object is an individual piece of data we store in a bucket. It contains object data (also called payload)\nand metadata. Object data can be any sequence of bytes we want to store. The metadata is a set of name-value\npairs that describe the object.")]),e._v(" "),s("p",[s("strong",[e._v("Versioning")]),e._v(". A feature that keeps multiple variants of an object in the same bucket. It is enabled at bucket-level. This\nfeature enables users to recover objects that are deleted or overwritten by accident.")]),e._v(" "),s("p",[s("strong",[e._v("Uniform Resource Identifier (URI)")]),e._v(". The object storage provides RESTful APIs to access its resources, namely,\nbuckets and objects. Each resource is uniquely identified by its URI.")]),e._v(" "),s("p",[s("strong",[e._v("Service-level agreement (SLA)")]),e._v(". A service-level agreement is a contract between a service provider and a client. For\nexample, the Amazon S3 Standard-Infrequent Access storage class provides the following SLA [7]")]),e._v(" "),s("ul",[s("li",[e._v("Designed for durability of 99.999999999% of objects across multiple Availability Zones.")]),e._v(" "),s("li",[e._v("Data is resilient in the event of one entire Availability Zone destruction.")]),e._v(" "),s("li",[e._v("Designed for 99.9% availability.")])]),e._v(" "),s("h2",{attrs:{id:"step-1-understand-the-problem-and-establish-design-scope"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#step-1-understand-the-problem-and-establish-design-scope"}},[e._v("#")]),e._v(" Step 1 - Understand the Problem and Establish Design Scope")]),e._v(" "),s("p",[e._v("The following questions help to clarify the requirements and narrow down the scope.")]),e._v(" "),s("p",[s("strong",[e._v("Candidate")]),e._v(": Which features should be included in the design?"),s("br"),e._v(" "),s("strong",[e._v("Interviewer")]),e._v(": We would like you to design an S3-like object storage system with the following functionalities:")]),e._v(" "),s("ul",[s("li",[e._v("Bucket creation.")]),e._v(" "),s("li",[e._v("Object uploading and downloading.")]),e._v(" "),s("li",[e._v("Object versioning.")]),e._v(" "),s("li",[e._v("Listing objects in a bucket. It's similar to the “aws s3 Is” command [8].")])]),e._v(" "),s("p",[s("strong",[e._v("Candidate")]),e._v(": What is the typical data size?"),s("br"),e._v(" "),s("strong",[e._v("Interviewer")]),e._v(": We need to store both massive objects (a few GBs or more) and a large number of small objects (tens of KBs), efficiently.")]),e._v(" "),s("p",[s("strong",[e._v("Candidate")]),e._v(": How much data do we need to store in one year?"),s("br"),e._v(" "),s("strong",[e._v("Interviewer")]),e._v(": 100 petabytes (PB).")]),e._v(" "),s("p",[s("strong",[e._v("Candidate")]),e._v(": Can we assume data durability is 6 nines (99.9999%) and service availability is 4 nines (99.99%)?"),s("br"),e._v(" "),s("strong",[e._v("Interviewer")]),e._v(": Yes, that sounds reasonable.")]),e._v(" "),s("h3",{attrs:{id:"non-functional-requirements"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#non-functional-requirements"}},[e._v("#")]),e._v(" Non-functional requirements")]),e._v(" "),s("ul",[s("li",[e._v("100 PB of data")]),e._v(" "),s("li",[e._v("Data durability is 6 nines")]),e._v(" "),s("li",[e._v("Service availability is 4 nines")]),e._v(" "),s("li",[e._v("Storage efficiency. Reduce storage costs while maintaining a high degree of reliability and performance.")])]),e._v(" "),s("h3",{attrs:{id:"back-of-the-envelope-estimation"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#back-of-the-envelope-estimation"}},[e._v("#")]),e._v(" Back-of-the-envelope estimation")]),e._v(" "),s("p",[e._v("Obiject storage is likely to have bottlenecks in either disk capacity or disk I0 per second (IOPS). Let's take a look.")]),e._v(" "),s("ul",[s("li",[e._v("Disk capacity. Let's assume objects follow the distribution listed below:\n"),s("ul",[s("li",[e._v("20% of all objects are small objects (less than 1MB).")]),e._v(" "),s("li",[e._v("60% of objects are medium-sized objects (1MB ~ 64MB).")]),e._v(" "),s("li",[e._v("20% are large objects (larger than 64MB).")])])]),e._v(" "),s("li",[e._v("IOPS. Let's assume one hard disk (SATA interface, 7200 rpm) is capable of doing 100~150 random seeks per second (100-150 IOPS).")])]),e._v(" "),s("p",[e._v("With those assumptions, we can estimate the total number of objects the system can persist. To simplify the\ncalculation, let’s use the median size for each object type (0.5MB for small objects, 32MB for medium objects, and\n200MB for large objects). A 40% storage usage ratio gives us:")]),e._v(" "),s("ul",[s("li",[e._v("100 PB = 100 % 1000 * 1000 * 1000 MB = 10^11 MB")]),e._v(" "),s("li",[e._v("10 * 11 % 0.4 /( 0.2 * 0.5MB + 0.6 * 32MB + 0.2 * 200MB ) = 0.68 billion objects.")]),e._v(" "),s("li",[e._v("If we assume the metadata of an object is about 1KB in size, we need 0.68 TB space to store all metadata information.")])]),e._v(" "),s("p",[e._v("Even though we may not use those numbers, it's good to have a general idea about the scale and constraint of the system.")]),e._v(" "),s("h2",{attrs:{id:"step-2-propose-high-level-design-and-get-buy-in"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#step-2-propose-high-level-design-and-get-buy-in"}},[e._v("#")]),e._v(" Step 2 - Propose High-Level Design and Get Buy-In")]),e._v(" "),s("p",[e._v("Before diving into the design, let's explore a few interesting properties of object storage, as they may influence it.")]),e._v(" "),s("p",[s("strong",[e._v("Object immutability")]),e._v(". One of the main differences between object storage and the other two types of storage\nsystems is that the objects stored inside of object storage are immutable. We may delete them or replace them\nentirely with a new version, but we cannot make incremental changes.")]),e._v(" "),s("p",[s("strong",[e._v("Key-value store")]),e._v(". We could use object URI to retrieve object data (Listing 1). The object URI is the key and object data is the value.")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("Request:\n\nGET /bucket1/objectl.txt HTTP/1.1\nResponse:\n\nHTTP/1.1 200 OK\n\nContent-Length: 4567\n\n[4567 bytes of object data]\n")])])]),s("p",[s("em",[e._v("Listing 1 Use object URI to ratrieve object data")])]),e._v(" "),s("p",[s("strong",[e._v("Write once, read many times.")]),e._v(" The data access pattem for object data is written once and read many times.\nAccording to the research done by Linkedin, 95% of requests are read operations [9].")]),e._v(" "),s("p",[s("strong",[e._v("Support both small and large objects.")]),e._v(" Object size may vary and we need to support both")]),e._v(" "),s("p",[e._v("The design philosophy of object storage is very similar to that of the UNIX file system. In UNIX, when we save a file\nin the local file system, it does not save the filename and file data together. Instead, the filename is stored in a data\nstructure called “inode” [10], and the file data s stored in different disk locations. The inode contains a list of file\nblock pointers that point to the disk locations of the file data. When we access a local file, we first fetch the\nmetadata in the inode. We then read the file data by following the file block pointers to the actual disk locations.")]),e._v(" "),s("p",[e._v("The object storage works similarly. The inode becomes the metadata store that stores all the object metadata. The\nhard disk becomes the data store that stores the object date. In the UNIX file system, the inode uses the file block\npointer to record the location of data on the hard disk. In object storage, the metadata store uses the ID of the\nobject to find the corresponding object data in the data store, via a network request. Figure 2 shows the UNIX file\nsystem and the object storage.")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1070),alt:"Figure 2"}}),s("br"),e._v(" "),s("em",[e._v("Figure 2 Unix file system and object store")])]),e._v(" "),s("p",[e._v("Separating metadata and object data simplifies the design. The data store contains immutable data while the\nmetadata store contains mutable data. This separation enables us to implement and optimize these two\ncomponents independently. Figure 3 shows what the bucket and object look like.")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1071),alt:"Figure 3"}}),s("br"),e._v(" "),s("em",[e._v("Figure 3 Bucket & object")])]),e._v(" "),s("h3",{attrs:{id:"high-level-design"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#high-level-design"}},[e._v("#")]),e._v(" High-level design")]),e._v(" "),s("p",[e._v("Figure 4 shows the high-level design.")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1072),alt:"Figure 4"}}),s("br"),e._v(" "),s("em",[e._v("Figure 4 High-level design")])]),e._v(" "),s("p",[e._v("Let's go over the components one by one.")]),e._v(" "),s("p",[s("strong",[e._v("Load balancer.")]),e._v(" Distributes RESTful API requests across a number of API servers.")]),e._v(" "),s("p",[s("strong",[e._v("API service.")]),e._v(" Orchestrates remote procedure calls to the identity and access management service, metadata service,\nand storage stores. This service is stateless so it can be horizontally scaled.")]),e._v(" "),s("p",[s("strong",[e._v("Identity and access management (IAM).")]),e._v(" The central place to handle authentication, authorization, and access\ncontrol. Authentication verifies who you are, and authorization validates what operations you could perform based\non who you are.")]),e._v(" "),s("p",[s("strong",[e._v("Data store.")]),e._v(" Stores and retrieves the actual data. All data-related operations are based on object ID (UUID).")]),e._v(" "),s("p",[s("strong",[e._v("Metadata store.")]),e._v(" Stores the metadata of the objects.")]),e._v(" "),s("p",[e._v("Note that the metadata and data stores are just logical components, and there are different ways to implement\nthem. For example, in Ceph’s Rados Gateway [11], there is no standalone metadata store. Everything, including the\nobject bucket, is persisted as one or multiple Rados objects.")]),e._v(" "),s("p",[e._v("Now we have a basic understanding of the high-level design, let's explore some of the most important workflows in\nobject storage.")]),e._v(" "),s("ul",[s("li",[e._v("Uploading an object.")]),e._v(" "),s("li",[e._v("Downloading an object.")]),e._v(" "),s("li",[e._v("Object versioning and listing objects in a bucket. They will be explained in the “Deep Dive” section.")])]),e._v(" "),s("h3",{attrs:{id:"uploading-an-object"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#uploading-an-object"}},[e._v("#")]),e._v(" Uploading an object.")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1073),alt:"Figure 5"}}),s("br"),e._v(" "),s("em",[e._v("Figure 5 Uploading an object")])]),e._v(" "),s("p",[e._v('An object has to reside in a bucket. In this example, we first create a bucket named “bucket-to-share” and then upload a file named "scripttt” to the bucket. Figure 5 explains how this flow works in 7 steps.')]),e._v(" "),s("ol",[s("li",[e._v("The client sends an HTTP PUT request to create a bucket named “bucket-to-share.” The request is forwarded to the API service.")]),e._v(" "),s("li",[e._v("The API service calls the IAM to ensure the user is authorized and has WRITE permission.")]),e._v(" "),s("li",[e._v("The API service calls the metadata store to create an entry with the bucket info in the metadata database. Once the entry is created, a success message is returned to the client.")]),e._v(" "),s("li",[e._v("After the bucket is created, the client sends an HTTP PUT request to create an object named “script.xt”,")]),e._v(" "),s("li",[e._v("The API service verifies the user's identity and ensures the user has WRITE permission on the bucket.")]),e._v(" "),s("li",[e._v("Once validation succeeds, the API service sends the object data in the HTTP PUT payload to the data store. The data store persists the payload as an object and returns the UUID of the object.")]),e._v(" "),s("li",[e._v("The API service calls the metadata store to create a new entry in the metadata database. It contains important metadata such as the object_id (UUID), bucket_id (which bucket the object belongs to), object_name, etc. A sample entry is shown in Table 2.")])]),e._v(" "),s("table",[s("thead",[s("tr",[s("th",[e._v("object_name")]),e._v(" "),s("th",[e._v("object_id")]),e._v(" "),s("th",[e._v("bucket_id")])])]),e._v(" "),s("tbody",[s("tr",[s("td",[e._v("script.txt")]),e._v(" "),s("td",[e._v("239D5866-0052-00F6-014E-CI14E61EDA28")]),e._v(" "),s("td",[e._v("B2AA1B2E-F599-4500-B5E4-1F51AAESETEL")])])])]),e._v(" "),s("p",[s("em",[e._v("Table 2 Sample entry")])]),e._v(" "),s("p",[e._v("The API to upload an object could look like this:")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("PUT /bucket-to-share/script.txt HTTP/1.1\nHost: foo.s3example.org\nDate: Sun, 12 Sept 2021 17:51:00 GHT\nAuthorization: authorization string\nContent-Type: text/plain\nContent-Length: 4567\nx-amz-meta-author: Alex\n\n[4567 bytes of object data]\n")])])]),s("p",[s("em",[e._v("Listing 2 Uploading an object")])]),e._v(" "),s("h3",{attrs:{id:"downloading-an-object"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#downloading-an-object"}},[e._v("#")]),e._v(" Downloading an object")]),e._v(" "),s("p",[e._v("A bucket has no directory hierarchy. However, we can create a logical hierarchy by concatenating the bucket name\nand the object name to simulate a folder structure. For example, we name the object “bucket-to-share/script.txt”\ninstead of “script.txt”. To get an object, we specify the object name in the GET request. The API to download an\nobject looks like this:")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("GET /bucket-to-share/script.txt HTTP/1.1\nHost: foo.s3example.org\nDate: Sun, 12 Sept 2621 18:30:01 GNT\nAuthorization: authorization string\n")])])]),s("p",[s("em",[e._v("Listing 3 Downloading an object")])]),e._v(" "),s("p",[s("img",{attrs:{src:a(1074),alt:"Figure 6"}}),s("br"),e._v(" "),s("em",[e._v("Figure 6 Downloading an object")])]),e._v(" "),s("p",[e._v("As mentioned earlier, the data store does not store the name of the object and it only supports object operations\nvia object_id (UUID). In order to download the object, we first map the object name to the UUID. The workflow of\ndownloading an object is shown below:")]),e._v(" "),s("ol",[s("li",[e._v("The client sends an HTTP GET request to the load balancer: GET /bucket-to-share/script.txt")]),e._v(" "),s("li",[e._v("The API service queries the IAM to verify that the user has READ access to the bucket.")]),e._v(" "),s("li",[e._v("Once validated, the API service fetches the corresponding object's UUID from the metadata store.")]),e._v(" "),s("li",[e._v("Next, the API service fetches the object data from the data store by its UUID.")]),e._v(" "),s("li",[e._v("The API service returns the object data to the client in HTTP GET response.")])]),e._v(" "),s("h2",{attrs:{id:"step-3-design-deep-dive"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#step-3-design-deep-dive"}},[e._v("#")]),e._v(" Step 3 - Design Deep Dive")]),e._v(" "),s("p",[e._v("In this section, we dive deep into a few areas:")]),e._v(" "),s("ul",[s("li",[e._v("Data store")]),e._v(" "),s("li",[e._v("Metadata data model")]),e._v(" "),s("li",[e._v("Listing objects in a bucket")]),e._v(" "),s("li",[e._v("Object versioning")]),e._v(" "),s("li",[e._v("Optimizing uploads of large files")]),e._v(" "),s("li",[e._v("Garbage collection")])]),e._v(" "),s("h3",{attrs:{id:"data-store"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#data-store"}},[e._v("#")]),e._v(" Data store")]),e._v(" "),s("p",[e._v("Let's take a closer look at the design of the data store. As discussed previously, the API service handles external\nrequests from users and calls different internal services to fulfill those requests. To persist or retrieve an object, the\nAPI service calls the data store. Figure 7 shows the interactions between the API service and the data store for\nuploading and downloading an object.")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1075),alt:"Figure 7"}}),s("br"),e._v(" "),s("em",[e._v("Figure 7 Upload and download an object")])]),e._v(" "),s("h4",{attrs:{id:"high-level-design-for-the-data-store"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#high-level-design-for-the-data-store"}},[e._v("#")]),e._v(" High-level design for the data store")]),e._v(" "),s("p",[e._v("The data store has three main components as shown in Figure 8")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1076),alt:"Figure 8"}}),s("br"),e._v(" "),s("em",[e._v("Figure 8 Data store components")])]),e._v(" "),s("p",[s("strong",[e._v("Data routing service")]),s("br"),e._v("\nThe data routing service provides RESTful or gRPC [12] APIs to access the data node cluster. It is a stateless service\nthat can scale by adding more servers. This service has the following responsibilities:")]),e._v(" "),s("ul",[s("li",[e._v("Query the placement service to get the best data node to store data.")]),e._v(" "),s("li",[e._v("Read data from data nodes and return it to the API service.")]),e._v(" "),s("li",[e._v("Write data to data nodes.")])]),e._v(" "),s("p",[s("strong",[e._v("Placement service")]),s("br"),e._v("\nThe placement service determines which data nodes (primary and replicas) should be chosen to store an object. It\nmaintains a virtual cluster map, which provides the physical topology of the cluster. The virtual cluster map contains\nlocation information for each data node which the placement service uses to make sure the replicas are physically\nseparated. This separation is key to high durability. See the “Durability” section below for details. An example of the\nvirtual cluster map is shown in Figure 9.")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1077),alt:"Figure 9"}}),s("br"),e._v(" "),s("em",[e._v("Figure 9 Virtual cluster map")])]),e._v(" "),s("p",[e._v("The placement service continuously monitors all data nodes through heartbeats. If a data node doesn't send a\nheartbeat within a configurable 15-second grace period, the placement service marks the node as “down” in the\nvirtual cluster map.")]),e._v(" "),s("p",[e._v("This is a critical service, so we suggest building a cluster of 5 or 7 placement service nodes with Paxos[13] or Raft[14] consensus protocol. The consensus protocol ensures that as long as more than half of the nodes are healthy, the service s a whole continues to work. For example, if the placement service cluster has 7 nodes, it can tolerate a 3-node failure. To learn more about consensus protocols, refer to the reference materials [13] [14].")]),e._v(" "),s("p",[s("strong",[e._v("Data node")]),s("br"),e._v("\nThe data node stores the actual object data. It ensures reliability and durability by replicating data to multiple data\nnodes, also called a replication group.")]),e._v(" "),s("p",[e._v("Each data node has a data service daemon running on it. The data service daemon continuously sends heartbeats\nto the placement service. The heartbeat message includes the following essential information:")]),e._v(" "),s("ul",[s("li",[e._v("How many disk drives (HDD or SSD) does the data node manage?")]),e._v(" "),s("li",[e._v("How much data is stored on each drive?")])]),e._v(" "),s("p",[e._v("When the placement service receives the heartbeat for the first time, it assigns an ID for this data node, adds it to the virtual cluster map, and returns the following information:")]),e._v(" "),s("ul",[s("li",[e._v("a unique ID of the data node")]),e._v(" "),s("li",[e._v("the virtual cluster map")]),e._v(" "),s("li",[e._v("where to replicate data")])]),e._v(" "),s("h4",{attrs:{id:"data-persistence-flow"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#data-persistence-flow"}},[e._v("#")]),e._v(" Data persistence flow")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1078),alt:"Figure 10"}}),s("br"),e._v(" "),s("em",[e._v("Figure 10 Data persistence flow")])]),e._v(" "),s("p",[e._v("Now let's take a look at how data is persisted in the data node.")]),e._v(" "),s("ol",[s("li",[e._v("The API service forwards the object data to the data store.")]),e._v(" "),s("li",[e._v("The data routing service generates a UUID for this object and queries the placement service for the data node\nto store this object. The placement service checks the virtual cluster map and returns the primary data node.")]),e._v(" "),s("li",[e._v("The data routing service sends data directly to the primary data node, together with its UUID.\n4.The primary data node saves the data locally and replicates it to two secondary data nodes. The primary node\nresponds to the data routing service when data is successfully replicated to all secondary nodes.")]),e._v(" "),s("li",[e._v("The UUID of the object (Objld) is returned to the API service.")])]),e._v(" "),s("p",[e._v("In step 2, given a UUID for the object as an input, the placement service returns the replication group for the object.\nHow does the placement service do this? Keep in mind that this lookup needs to be deterministic, and it must\nsurvive the addition or removal of replication groups. Consistent hashing is a common implementation of such a\nlookup function. Refer to [15] for more information.")]),e._v(" "),s("p",[e._v("In step 4, the primary data node replicates data to all secondary nodes before it returns a response. This makes data\nstrongly consistent among all data nodes. This consistency comes with latency costs because we have to wait until\nthe slowest replica finishes. Figure 11 shows the trade-offs between consistency and latency.")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1079),alt:"Figure 11"}}),s("br"),e._v(" "),s("em",[e._v("Figure 11 Trade-off between consistency and latency")])]),e._v(" "),s("ol",[s("li",[e._v("Data is considered as successfully saved after all three nodes store the data. This approach has the best\nconsistency but the highest latency.")]),e._v(" "),s("li",[e._v("Data is considered as successfully saved after the primary and one of the secondaries store the data. This\napproach has a medium consistency and medium latency.")]),e._v(" "),s("li",[e._v("Data is considered as successfully saved after the primary persists the data. This approach has the worst\nconsistency but the lowest latency.")])]),e._v(" "),s("p",[e._v("Both 2 and 3 are forms of eventual consistency.")]),e._v(" "),s("h4",{attrs:{id:"how-data-is-organized"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#how-data-is-organized"}},[e._v("#")]),e._v(" How data is organized")]),e._v(" "),s("p",[e._v("Now let's take a look at how each data node manages the data. A simple solution is to store each object in 2 stand-\nalone file. This works, but the performance suffers when there are many small files. Two issues arise when having\nt00 many small files on a file system. First, it wastes many data blocks. A file system stores files in discrete disk\nblocks. Disk blocks have the same size, and the size is fixed when the volume is initialized. The typical block size is\naround 4 KB. For a file smaller than 4 KB, it would still consume the entire disk block. If the file system holds a lot of\nsmall files, it wastes a lot of disk blocks, with each one only lightly filled with a small file")]),e._v(" "),s("p",[e._v("Second, it could exceed the system’s inode capacity. The file system stores the location and other information\nabout a file in a special type of block called inode. For most file systems, the number of inodes is fixed when the\ndisk is initialized. With millions of small files, it runs the risk of consuming all inodes. Also, the operating system\ndoes not handle a large number of inodes very well, even with aggressive caching of file system metadata. For\nthese reasons, storing small objects as individual files does not work well in practice.")]),e._v(" "),s("p",[e._v("To address these issues, we can merge many small objects into a larger file. It works conceptually like a write-ahead\nlog (WAL). When we save an object, it is appended to an existing read-write file. When the read-write file reaches its\ncapacity threshold - usually set to a few GBs — the read-write file is marked as read-only, and a new read-write file is\ncreated to receive new objects. Once & file is marked as read-only, it can only serve read requests. Figure 12\nexplains how this process works.")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1080),alt:"Figure 12"}}),s("br"),e._v(" "),s("em",[e._v("Figure 12 Store multiple small objects in one big file")])]),e._v(" "),s("p",[e._v("Note that write access to the read-write file must be serialized. As shown in Figure 12, objects are stored in order,\none after the other, in the read-write file. To maintain this on-disk layout, multiple cores processing incoming write\nrequests in parallel must take their turns to write to the read-write file. For a modern server with a large number of\ncores processing many incoming requests in parallel, this seriously restricts write throughput. To fix this, we could\nprovide dedicated read-write files, one for each core processing incoming requests.")]),e._v(" "),s("h4",{attrs:{id:"object-lookup"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#object-lookup"}},[e._v("#")]),e._v(" Object lookup")]),e._v(" "),s("p",[e._v("With each data file holding many small objects, how does the data node locate an object by UUID? The data node needs the following information:")]),e._v(" "),s("ul",[s("li",[e._v("The data file that contains the object")]),e._v(" "),s("li",[e._v("The starting offset of the object in the data file")]),e._v(" "),s("li",[e._v("The size of the object")])]),e._v(" "),s("p",[e._v("The database schema to support this lookup is shown in Table 3.")]),e._v(" "),s("table",[s("thead",[s("tr",[s("th",[e._v("object_mapping")])])]),e._v(" "),s("tbody")]),e._v(" "),s("p",[s("strong",[e._v("object_id")]),e._v("\nfile_name\nstart_offset\nobject_size")]),e._v(" "),s("table",[s("thead",[s("tr",[s("th",[e._v("Field")]),e._v(" "),s("th",[e._v("Description")])])]),e._v(" "),s("tbody",[s("tr",[s("td",[e._v("object_id")]),e._v(" "),s("td",[e._v("UUID of the object")])]),e._v(" "),s("tr",[s("td",[e._v("file_name")]),e._v(" "),s("td",[e._v("The name of the file that contains the object")])]),e._v(" "),s("tr",[s("td",[e._v("start_offset")]),e._v(" "),s("td",[e._v("Beginning address of the object in the file")])]),e._v(" "),s("tr",[s("td",[e._v("object size")]),e._v(" "),s("td",[e._v("The number of bytes in the object")])])])]),e._v(" "),s("p",[s("em",[e._v("Table 4 Object_mapping fields")])]),e._v(" "),s("p",[e._v("We considered two options for storing this mapping: 2 file-based key-value store such as RocksDB [16] or a\nrelational database. RocksDB is based on SSTable [17], and it is fast for writes but slower for reads. A relational\ndatabase usually uses a B+ tree [18] based storage engine, and it is fast for reads but slower for writes. As\nmentioned earlier, the data access pattem is write once and read multiple times. Since 2 relational database\nprovides better read performance, it is a better choice than RocksDB.")]),e._v(" "),s("p",[e._v("How should we deploy this relational database? At our scale, the data volume for the mapping table is massive.\nDeploying a single large cluster to support all data nodes could work, but is difficult to manage. Note that this\nmapping data s isolated within each data node. There is no need to share this across data nodes. To take\nadvantage of this property, we could simply deploy a simple relational database on each data node. SQlite [19] is a\ngood choice here. It is a file-based relational database with a solid reputation.")]),e._v(" "),s("h4",{attrs:{id:"updated-data-persistence-flow"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#updated-data-persistence-flow"}},[e._v("#")]),e._v(" Updated data persistence flow")]),e._v(" "),s("p",[e._v("Since we have made quite a few changes to the data node, let's revisit how to save a new object in the data node (Figure 13).")]),e._v(" "),s("ol",[s("li",[e._v('The API service sends a request to save a new object named "object 4"')]),e._v(" "),s("li",[e._v('The data node service appends the object named "object 4" at the end of the read-write file named '),s("code",[e._v("/data/c")]),e._v(".")]),e._v(" "),s("li",[e._v('A new record of "object 4" is inserted into the '),s("em",[e._v("object_mapping")]),e._v(" table.")]),e._v(" "),s("li",[e._v("The data node service retums the UUID to the APl service.")])]),e._v(" "),s("p",[s("img",{attrs:{src:a(1081),alt:"Figure 13"}}),s("br"),e._v(" "),s("em",[e._v("Figure 13 Updated data persistence flow")])]),e._v(" "),s("h4",{attrs:{id:"durability"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#durability"}},[e._v("#")]),e._v(" Durability")]),e._v(" "),s("p",[e._v("Data reliability is extremely important for data storage systems. How can we Create a storage system that offers six\nnines of durability? Each failure case has to be carefully considered and data needs to be properly replicated.")]),e._v(" "),s("p",[s("strong",[e._v("Hardware failure and failure domain")])]),e._v(" "),s("p",[e._v("Hard drive failures are inevitable no matter which media we use. Some storage media may have better durability\nthan others, but we cannot rely on a single hard drive to achieve our durability objective. A proven way to increase\ndurability is to replicate data to multiple hard drives, so a single disk failure does not impact the data availability, as\na whole. In our design, we replicate data three times.")]),e._v(" "),s("p",[e._v("Let's assume the spinning hard drive has an annual failure rate of 0.81% [20]. This number highly depends on the\nmodel and make. Making 3 copies of data gives us "),s("code",[e._v("1-(0.0081)*3 = ~0.999999")]),e._v(" reliability. This is a very rough\nestimate. For more sophisticated calculations, please read [20].")]),e._v(" "),s("p",[e._v("For a complete durability evaluation, we also need to consider the impacts of different failure domains. A failure\ndomain is a physical or logical section of the environment that is negatively affected when a critical service\nexperiences problems. In a modern data center, a server s usually put into a rack [21], and the racks are grouped\ninto rows/floors/rooms. Since each rack shares network switches and power, all the servers in a rack are in a rack-\nlevel failure domain. A modern server shares components like the motherboard, processors, power supply, HDD\ndrives, etc. The components in a server are in a node-level failure domain.")]),e._v(" "),s("p",[e._v("Here is a good example of a large-scale failure domain isolation. Typically, data centers divide infrastructure that\nshares nothing into different Availability Zones (AZs). We replicate our data to different AZs to minimize the failure\nimpact (Figure 14). Note that the choice of failure domain level doesn't directly increase the durability of data, but it\nwill result in better reliability in extreme cases, such as large-scale power outages, cooling system failures, natural\ndisasters, etc.")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1082),alt:"Figure 14"}}),s("br"),e._v(" "),s("em",[e._v("Figure 14 Multi-datacenter replication")])]),e._v(" "),s("p",[s("strong",[e._v("Erasure coding")])]),e._v(" "),s("p",[e._v("Making three full copies of data gives us roughly 6 nines of data durability. Are there other options to further\nincrease durability? Yes, erasure coding is one option. Erasure coding [22] deals with data durability differently. It\nchunks data into smaller pieces (placed on different servers) and creates parities for redundancy. In the event of\nfailures, we can use chunk data and parities to reconstruct the data. Let's take a look at a concrete example (4 + 2 erasure coding) as shown in Figure 15.")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1083),alt:"Figure 15"}}),s("br"),e._v(" "),s("em",[e._v("Figure 15 Erasure coding")])]),e._v(" "),s("ol",[s("li",[e._v("Data is broken up into four even-sized data chunks d1, d2, d3, and d4.")]),e._v(" "),s("li",[e._v("The mathematical formula [23] is used to calculate the parities p1 and p2. To give a much simplified example, "),s("code",[e._v("p1 = d1 + 2*d2 - d3 + 4*d4")]),e._v(" and "),s("code",[e._v("p2 = -d1 + 5*d2 + d3 - 3*d4")]),e._v(" [24].")]),e._v(" "),s("li",[e._v("Data d3 and d4 are lost due to node crashes.")]),e._v(" "),s("li",[e._v("The mathematical formula is used to reconstruct lost data d3 and d4, using the known values of d1, d2, p1, and p2.")])]),e._v(" "),s("p",[e._v("Let's take a look at another example as shown in Figure 16 to better understand how erasure coding works with\nfailure domains. An (8+4) erasure coding setup breaks up the original data evenly into 8 chunks and calculates 4\nparities. All 12 pieces of data have the same size. All 12 chunks of data are distributed across 12 different failure\ndomains. The mathematics behind erasure coding ensures that the original data can be reconstructed when at most 4 nodes are down.")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1084),alt:"Figure 16"}}),s("br"),e._v(" "),s("em",[e._v("Figure 16 (8+4) erasure coding")])]),e._v(" "),s("p",[e._v("Compared to replication where the data router only needs to read data for an object from ane healthy node, in\nerasure coding the data router has to read data from at least 8 healthy nodes. This is an architectural design\ntradeoff. We use a more complex solution with a slower access speed, in exchange for higher durability and lower\nstorage cost. For object storage where the main cost is storage, this tradeoff might be worth it.")]),e._v(" "),s("p",[e._v("How much extra space does erasure coding need? For every two chunks of data, we need one parity block, so the\nstorage overhead is 50% (Figure 17). While in 3-copy replication, the storage overhead is 200% (Figure 17).")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1085),alt:"Figure 17"}}),s("br"),e._v(" "),s("em",[e._v("Figure 17 Extra space for replication and erasure coding")])]),e._v(" "),s("p",[e._v("Does erasure coding increase data durability? Let's assume a node has a 0.81% annual failure rate. According to the\ncalculation done by Backblaze [20], erasure coding can achieve 11 nines durabi\nlity. The calculation requires complicated math. If you're interested, refer to [20] for details")]),e._v(" "),s("p",[e._v("Table 5 compares the pros and cons of replication and erasure coding.")]),e._v(" "),s("table",[s("thead",[s("tr",[s("th",[e._v(".")]),e._v(" "),s("th",[e._v("Replication")]),e._v(" "),s("th",[e._v("Erasure coding")])])]),e._v(" "),s("tbody",[s("tr",[s("td",[e._v("Durability")]),e._v(" "),s("td",[e._v("6 nines of durability (data copied 3 times)")]),e._v(" "),s("td",[e._v("11 nines of durability (8+4 erasure coding). "),s("strong",[e._v("Erasure coding wins.")])])]),e._v(" "),s("tr",[s("td",[e._v("Storage efficiency")]),e._v(" "),s("td",[e._v("200% storage overhead")]),e._v(" "),s("td",[e._v("50% storage overhead. "),s("strong",[e._v("Erasure coding wins")])])]),e._v(" "),s("tr",[s("td",[e._v("Compute resource")]),e._v(" "),s("td",[e._v("No computation. "),s("strong",[e._v("Replication wins.")])]),e._v(" "),s("td",[e._v("Higher usage of computation resources to calculate parities.")])]),e._v(" "),s("tr",[s("td",[e._v("Write performance")]),e._v(" "),s("td",[e._v("Replicating data to multiple nodes. No calculation is needed. "),s("strong",[e._v("Replication wins.")])]),e._v(" "),s("td",[e._v("Increased write latency because we need to calculate parities before data is written to disk.")])]),e._v(" "),s("tr",[s("td",[e._v("Read performance")]),e._v(" "),s("td",[e._v("In normal operation, reads are served from the same replica. Reads under a failure mode are not impacted because reads can be served from a non-fault replica. "),s("strong",[e._v("Replication wins.")])]),e._v(" "),s("td",[e._v("In normal operation, every read has to come from multiple nodes in the cluster. Reads under a failure mode are slower because the missing data must be reconstructed first")])])])]),e._v(" "),s("p",[s("em",[e._v("Table 5 Replication vs erasure coding")])]),e._v(" "),s("p",[e._v("In summary, replication is widely adopted in latency-sensitive applications while erasure coding is often used to\nminimize storage cost. Erasure coding s attractive for its cost efficiency and durability, but it greatly complicates the\ndata node design. Therefore, for this design, we mainly focus on replication.")]),e._v(" "),s("h4",{attrs:{id:"correctness-verification"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#correctness-verification"}},[e._v("#")]),e._v(" Correctness verification")]),e._v(" "),s("p",[e._v("Erasure coding increases data durability at comparable storage costs. Now we can move on to solve another hard\nchallenge: data corruption")]),e._v(" "),s("p",[e._v("If a disk fails completely and the failure can be detected, it can be treated as a data node failure. In this case, we can\nreconstruct data using erasure coding. However, in-memory data corruption is a regular occurrence in large-scale\nsystems")]),e._v(" "),s("p",[e._v("This problem can be addressed by verifying checksums [25] between process boundaries. A checksum is a small-sized block of data that is used to detect data errors. Figure 18 illustrates how the checksum is generated")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1086),alt:"Figure 18"}}),s("br"),e._v(" "),s("em",[e._v("Figure 18 Generate checksum")])]),e._v(" "),s("p",[e._v("If we know the checksum of the original data, we can compute the checksum of the data after transmission:")]),e._v(" "),s("ul",[s("li",[e._v("If they are different, data is corrupted.")]),e._v(" "),s("li",[e._v("If they are the same, there is a very high probability the data is not corrupted. The probability is not 100%, but in practice, we could assume they are the same.")])]),e._v(" "),s("p",[s("img",{attrs:{src:a(1087),alt:"Figure 19"}}),s("br"),e._v(" "),s("em",[e._v("Figure 19 Compare checksum")])]),e._v(" "),s("p",[e._v("There are many checksum algorithms, such as MD5 [26], SHA1[27], HMAC [28], etc. A good checksum algorithm\nusually outputs a significantly different value even for a small change made to the input. For this chapter, we\nchoose a simple checksum algorithm such as MD5.")]),e._v(" "),s("p",[e._v("In our design, we append the checksum at the end of each object. Before a file is marked as read-only, we add a\nchecksum of the entire file at the end. Figure 20 shows the layout.")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1088),alt:"Figure 20"}}),s("br"),e._v(" "),s("em",[e._v("Figure 20 Add checksum to data node")])]),e._v(" "),s("p",[e._v("With (8+4) erasure coding and checksum verification, this is what happens when we read data:")]),e._v(" "),s("ol",[s("li",[e._v("Fetch the object data and the checksum.")]),e._v(" "),s("li",[e._v("Compute the checksum against the data received.\n"),s("ul",[s("li",[e._v("(a). If the two checksums match, the data is error-free.")]),e._v(" "),s("li",[e._v("(b). If the checksums are different, the data is corrupted. We will try to recover by reading the data from other failure domains.")])])]),e._v(" "),s("li",[e._v("Repeat steps 1 and 2 until all 8 pieces of data are returned. We then reconstruct the data and send it back to the client.")])]),e._v(" "),s("h3",{attrs:{id:"metadata-data-model"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#metadata-data-model"}},[e._v("#")]),e._v(" Metadata data model")]),e._v(" "),s("p",[e._v("In this section, we first discuss the database schema and then dive into scaling the database.")]),e._v(" "),s("p",[s("strong",[e._v("Schema")]),e._v("\nThe database schema needs to support the following 3 queries:"),s("br"),e._v("\nQuery 1: Find the object ID by object name."),s("br"),e._v("\nQuery 2: Insert and delete an object based on the object name."),s("br"),e._v("\nQuery 3: List objects in a bucket sharing the same prefix."),s("br"),e._v("\nFigure 21 shows the schema design. We need two database tables: bucket and object.")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1089),alt:"Figure 21"}}),s("br"),e._v(" "),s("em",[e._v("Figure 21 Database tables")])]),e._v(" "),s("p",[s("strong",[e._v("Scale the bucket table")]),s("br"),e._v("\nSince there is usually a limit on the number of buckets a user can create, the size of the bucket table is small. Let's\nassume we have 1 million customers, each customer owns 10 buckets and each record takes 1 KB. That means we\nneed 10 GB (1 million * 10 * 1KB) of storage space. The whole table can easily fit in a modern database server.\nHowever, a single database server might not have enough CPU or network bandwidth to handle all read requests. If\nso, we can spread the read load among multiple database replicas.")]),e._v(" "),s("p",[s("strong",[e._v("Scale the object table")]),s("br"),e._v("\nThe object table holds the object metadata. The dataset at our design scale will likely not fit in a single database\ninstance. We can scale the object table by sharding.")]),e._v(" "),s("p",[e._v("One option is to shard by the "),s("em",[e._v("bucket_id")]),e._v(" so all the objects under the same bucket are stored in one shard. This\ndoesn't work because it causes hotspot shards as a bucket might contain billions of objects.")]),e._v(" "),s("p",[e._v("Another option is to shard by "),s("em",[e._v("object_id")]),e._v(". The benefit of this sharding scheme is that it evenly distributes the load. But\nwe will not be able to execute query 1 and query 2 efficiently because those two queries are based on the URI.")]),e._v(" "),s("p",[e._v("We choose to shard by a combination of "),s("em",[e._v("bucket_name")]),e._v(" and "),s("em",[e._v("object_name")]),e._v(". This is because most of the metadata\noperations are based on the object URI, for example, finding the object ID by URI or uploading an object via URI. To\nevenly distribute the data, we can use the hash of the ("),s("em",[e._v("bucket_name")]),e._v(", "),s("em",[e._v("object_name")]),e._v(") as the sharding key.")]),e._v(" "),s("p",[e._v("With this sharding scheme, it is straightforward to support the first two queries, but the last query is less obvious.\nLet's take a look.")]),e._v(" "),s("h3",{attrs:{id:"listing-objects-in-a-bucket"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#listing-objects-in-a-bucket"}},[e._v("#")]),e._v(" Listing objects in a bucket")]),e._v(" "),s("p",[e._v("The object store arranges files in a flat structure instead of a hierarchy, like in a file system. An object can be\naccessed using a path in this format, "),s("em",[e._v("s3://bucket-name/object-name")]),e._v(". For example, "),s("em",[e._v("s3://mybucket/abc/d/e/f/file.txt")]),e._v(" contains:")]),e._v(" "),s("ul",[s("li",[e._v("Bucket name: mybucket")]),e._v(" "),s("li",[e._v("Object name: abc/d/ef/file.txt")])]),e._v(" "),s("p",[e._v("To help users organize their objects in a bucket, 3 introduces a concept called ‘prefixes.” A prefix is a string at the\nbeginning of the object name. S3 uses prefixes to organize the data in a way similar to directories. However,\nprefixes are not directories. Listing a bucket by prefix limits the results to only those object names that begin with\nthe prefix")]),e._v(" "),s("p",[e._v("In the example above with the path "),s("em",[e._v("s3://mybucket/abc/d/e/t/file.txt")]),e._v(", the prefix is "),s("em",[e._v("abc/d/e/t/")]),e._v(".")]),e._v(" "),s("p",[e._v("The AWS S3 listing command has 3 typical uses:")]),e._v(" "),s("ol",[s("li",[e._v("List all buckets owned by a user. The command looks like this:")])]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("aws s3 list-buckets\n")])])]),s("ol",{attrs:{start:"2"}},[s("li",[e._v("List all objects in a bucket that are at the same level as the specified prefix. The command looks like this:")])]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("aws s3 ls s3://mybucket/abc/\n")])])]),s("p",[e._v("In this mode, objects with more slashes in the name after the prefix are rolled up into a common prefix. For\nexample, with these objects in the bucket:")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("CA/cities/losangeles. txt\nCA/cities/sanfranciso.txt\nNY/cities/ny.txt\nfederal.txt\n")])])]),s("p",[e._v("Listing the bucket with the "),s("code",[e._v("/")]),e._v(" prefix would return these results, with everything under CA/ and NY/ rolled up into them: "),s("code",[e._v("CA/ NY/ federal.txt")])]),e._v(" "),s("ol",{attrs:{start:"3"}},[s("li",[e._v("Recursively list all objects in a bucket that shares the same prefix. The command looks like this:")])]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("aws s3 ls s3://mybucket/abc/ --recursive\n")])])]),s("p",[e._v("Using the same example as above, listing the bucket with the CA/ prefix would return these results:")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("CA/cities/losangeles.txt\nCA/cities/sanfranciso.txt\n")])])]),s("h4",{attrs:{id:"single-database"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#single-database"}},[e._v("#")]),e._v(" Single database")]),e._v(" "),s("p",[e._v("Let's first explore how we would support the listing command with a single database. To list all buckets owned by a\nuser, we run the following query:")]),e._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("SELECT")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("*")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("FROM")]),e._v(" bucket "),s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("WHERE")]),e._v(" owner_id"),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v("{id}\n")])])]),s("p",[e._v("To list all objects in a bucket that share the same prefix, we run a query like this.")]),e._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("SELECT")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("*")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("FROM")]),e._v(" object "),s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("WHERE")]),e._v(" bucket_id "),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[e._v('"123"')]),e._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("AND")]),e._v(" object_name "),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("LIKE")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("`")]),e._v("abc"),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("/")]),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("%")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("`")]),e._v("\n")])])]),s("p",[e._v('In this example, we find all objects with bucket_id equals to "123" that share the prefix "abc/". Any objects with\nmore slashes in their names after the specified prefix are rolled up in the application code as stated earlier in use\ncase 2.')]),e._v(" "),s("p",[e._v("The same query would support the recursive listing mode, as stated in use case 3 previously. The application code\nwould list every object sharing the same prefix, without performing any rollups.")]),e._v(" "),s("h4",{attrs:{id:"distributed-databases"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#distributed-databases"}},[e._v("#")]),e._v(" Distributed databases")]),e._v(" "),s("p",[e._v("When the metadata table is sharded, it's difficult to implement the listing function because we don't know which\nshards contain the data. The most obvious solution is to run a search query on all shards and then aggregate the\nresults. To achieve this, we can do the following:")]),e._v(" "),s("ol",[s("li",[e._v("The metadata service queries every shard by running the following query:")])]),e._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("SELECT")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("*")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("FROM")]),e._v(" object "),s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("WHERE")]),e._v(" bucket_id "),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[e._v("123")]),e._v("” "),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("AND")]),e._v(" object_name "),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("LIKE")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("`")]),e._v("a"),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("/")]),e._v("b"),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("/")]),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("%")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("`")]),e._v("\n")])])]),s("ol",{attrs:{start:"2"}},[s("li",[e._v("The metadata service aggregates all objects returned from each shard and returns the result to the caller.")])]),e._v(" "),s("p",[e._v("This solution works, but implementing pagination for this is a bit complicated. Before we explain why, let's review\nhow pagination works for a simple case with a single database. To return pages of listing with 10 objects for each\npage, the SELECT query would start with this:")]),e._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("SELECT")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("*")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("FROM")]),e._v(" object "),s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("WHERE")]),e._v("\nbucket_id "),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[e._v('"123"')]),e._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("AND")]),e._v(" object_name "),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("LIKE")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("`")]),e._v("a"),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("/")]),e._v("b"),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("/")]),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("%")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("`")]),e._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("ORDER")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("BY")]),e._v(" object_name "),s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("OFFSET")]),e._v(" @ "),s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("LIMIT")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[e._v("10")]),e._v("\n")])])]),s("p",[e._v("The OFFSET and LIMIT would restrict the results to the first 10 objects. In the next call, the user sends the request\nwith a hint to the server, 5o it knows to construct the query for the second page with an OFFSET of 10. This hint is\nusually done with a cursor that the server returns with each page to the client. The offset information is encoded in\nthe cursor. The client would include the cursor in the request for the next page. The server decodes the cursor and\nuses the offset information embedded in it to construct the query for the next page. To continue with the example\nabove, the query for the second page looks like this:")]),e._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("SELECT")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("*")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("FROM")]),e._v(" metadata\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("WHERE")]),e._v(" bucket_id "),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[e._v('"123"')]),e._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("AND")]),e._v(" object_name "),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("LIKE")]),e._v(" “a"),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("/")]),e._v("b"),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("/")]),s("span",{pre:!0,attrs:{class:"token operator"}},[e._v("%")]),e._v('"\n'),s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("ORDER")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("BY")]),e._v(" object_name "),s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("OFFSET")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[e._v("10")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("LIMIT")]),e._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[e._v("10")]),e._v("\n")])])]),s("p",[e._v("This client-server request loop continues until the server returns a special cursor that marks the end of the entire listing.")]),e._v(" "),s("p",[e._v("Now, let's explore why it's complicated to support pagination for sharded databases. Since the objects are\ndistributed across shards, the shards would likely return a varying number of results. Some shards would contain a\nfull page of 10 objects, while others would be partial or empty. The application code would receive results from\nevery shard, aggregate and sort them, and return only a page of 10 in our example. The objects that don't get\nincluded in the current round must be considered again for the next round. This means that each shard would likely\nhave a different offset. The server must track the offsets for all the shards and associate those offsets with the\ncursor. If there are hundreds of shards, there will be hundreds of offsets to track.")]),e._v(" "),s("p",[e._v("We have a solution that can solve the problem, but there are some tradeoffs. Since object storage is tuned for vast\nscale and high durability, object listing performance is rarely a priority. In fact, all commercial object storage\nsupports object listing with sub-optimal performance. To take advantage of this, we could denormalize the listing\ndata into a separate table sharded by bucket ID. This table is only used for listing objects. With this setup, even\nbuckets with billions of objects would offer acceptable performance. This isolates the listing query to a single\ndatabase which greatly simplifies the implementation.")]),e._v(" "),s("h3",{attrs:{id:"object-versioning"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#object-versioning"}},[e._v("#")]),e._v(" Object versioning")]),e._v(" "),s("p",[e._v("Versioning is a feature that keeps multiple versions of an object in a bucket. With versioning, we can restore objects\nthat are accidentally deleted or overwritten. For example, we may modify a document and save it under the same\nname, inside the same bucket. Without versioning, the old version of the document metadata is replaced by the\nnew version in the metadata store. The old version of the document is marked as deleted, so its storage space will\nbe reclaimed by the garbage collector. With versioning, the object storage keeps all previous versions of the\ndocument in the metadata store, and the old versions of the document are never marked as deleted in the object\nstore.")]),e._v(" "),s("p",[e._v("Figure 22 explains how to upload a versioned object. For this to work, we first need to enable versioning on the\nbucket.")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1090),alt:"Figure 22"}}),s("br"),e._v(" "),s("em",[e._v("Figure 22 Object versioning")])]),e._v(" "),s("ol",[s("li",[e._v("The client sends an HTTP PUT request to upload an object named “scriptt”.")]),e._v(" "),s("li",[e._v("The AP service verifies the user's identity and ensures that the user has WRITE permission on the bucket.")]),e._v(" "),s("li",[e._v("Once verified, the API service uploads the data to the data store. The data store persists the data as a new object and returns a new UUID to the API service.")]),e._v(" "),s("li",[e._v("The API service calls the metadata store to store the metadata information of this object.")]),e._v(" "),s("li",[e._v("To support versioning, the object table for the metadata store has a column called object_version that is only\nused if versioning is enabled. Instead of overwriting the existing record, a new record is inserted with the same\nbucket_id and object_name as the old record, but with a new object_id and object_version. The object_id is the\nUUID for the new object returned in step 3. The object_version is a TIMEUUID [29] generated when the new row\nis inserted. No matter which database we choose for the metadata store, it should be efficient to look up the\ncurrent version of an abject. The current version has the largest TIMEUUID of all the entries with the same\nobject_name. See Figure 23 for an illustration of how we store versioned metadata.")])]),e._v(" "),s("p",[s("img",{attrs:{src:a(1091),alt:"Figure 23"}}),s("br"),e._v(" "),s("em",[e._v("Figure 23 Versioned metadata")])]),e._v(" "),s("p",[e._v("In addition to uploading a versioned object, it can also be deleted. Let's take a look.")]),e._v(" "),s("p",[e._v("When we delete an object, all versions remain in the bucket and we insert a delete marker, as shown in Figure 24")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1092),alt:"Figure 24"}}),s("br"),e._v(" "),s("em",[e._v("Figure 24 Delete object by inserting a delete marker")])]),e._v(" "),s("p",[e._v("A delete marker is a new version of the object, and it becomes the current version of the object once inserted.\nPerforming a GET request when the current version of the object is a delete marker returns a 404 Object Not Found error")]),e._v(" "),s("h3",{attrs:{id:"optimizing-uploads-of-large-files"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#optimizing-uploads-of-large-files"}},[e._v("#")]),e._v(" Optimizing uploads of large files")]),e._v(" "),s("p",[e._v("In the back-of-the-envelope estimation, we estimated that 20% of the objects are large. Some might be larger than\na few GBs. It is possible to upload such a large object file directly, but it could take a long time. If the network\nconnection fails in the middle of the upload, we have to start over. A better solution is to slice a large object into\nsmaller parts and upload them independently. After all the parts are uploaded, the object store re-assembles the\nobject from the parts. This process is called multipart upload.")]),e._v(" "),s("p",[e._v("Figure 25 illustrates how multipart upload works:")]),e._v(" "),s("p",[s("img",{attrs:{src:a(1093),alt:"Figure 25"}}),s("br"),e._v(" "),s("em",[e._v("Figure 25 Multipart upload")])]),e._v(" "),s("ol",[s("li",[e._v("The client calls the object storage to initiate a multipart upload.")]),e._v(" "),s("li",[e._v("The data store retums an uploadID, which uniquely identifies the upload.")]),e._v(" "),s("li",[e._v("The client splits the large file into small objects and starts uploading. Let's assume the size of the file is 1.6G8\nand the client splits it into 8 parts, so each part is 200 MB in size. The client uploads the first part to the data\nstore together with the uploadiD it received in step 2.")]),e._v(" "),s("li",[e._v("When a part is uploaded, the data store returns an ETag, which is essentially the md5 checksum of that part. It\niis used to verify multipart uploads.")]),e._v(" "),s("li",[e._v("After all parts are uploaded, the client sends a complete multipart upload request, which includes the uploadiD,\npart numbers, and ETags.")]),e._v(" "),s("li",[e._v("The data store reassembles the object from its parts based on the part number. Since the object is really large,\nthis process may take a few minutes. After reassembly is complete, it returns a success message to the client.")])]),e._v(" "),s("p",[e._v("One potential problem with this approach is that old parts are no longer useful after the object has been\nreassembled from them. To solve this problem, we can introduce a garbage collection service responsible for\nfreeing up space from parts that are no longer needed.")]),e._v(" "),s("h3",{attrs:{id:"garbage-collection"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#garbage-collection"}},[e._v("#")]),e._v(" Garbage collection")]),e._v(" "),s("p",[e._v("Garbage collection is the process of automatically reclaiming storage space that is no longer used. There are a few\nways that data might become garbage:")]),e._v(" "),s("ul",[s("li",[e._v("Lazy object deletion. An object is marked as deleted at delete time without actually being deleted.")]),e._v(" "),s("li",[e._v("Orphan data. For example, half uploaded data or abandoned multipart uploads.")]),e._v(" "),s("li",[e._v("Corrupted data. Data that failed the checksum verification.")])]),e._v(" "),s("p",[e._v("The garbage collector does not remove objects from the data store, right away. Deleted objects will be periodically\ncleaned up with a compaction mechanism.")]),e._v(" "),s("p",[e._v("The garbage collector is also responsible for reclaiming unused space in replicas. For replication, we delete the\nobject from both primary and backup nodes. For erasure coding, if we use (8+4) setup, we delete the object from\nall 12 nodes.")]),e._v(" "),s("p",[e._v("Figure 26 shows an example of how compaction works.")]),e._v(" "),s("ol",[s("li",[e._v('The garbage collector copies objects from “/data/b’ to a new file named "/data/d". Note the garbage collector\nskips “Object 2" and “Object 5° because the delete flag is set to true for both of them.')]),e._v(" "),s("li",[e._v("After all objects are copied, the garbage collector updates the object_mapping table. For example, the obj_id\nand object_size fields of “Object 3\" remain the same, but file_name and start_offset are updated to reflect its\nnew location. To ensure data consistency, it's a good idea to wrap the update operations to file_name and\nstart_offset in a database transaction.")])]),e._v(" "),s("p",[s("img",{attrs:{src:a(1094),alt:"Figure 26"}}),s("br"),e._v(" "),s("em",[e._v("Figure 26 Compaction")])]),e._v(" "),s("p",[e._v("As we can see from Figure 26, the size of the new file after compaction is smaller than the old file. To avoid creating\na lot of small files, the garbage collector usually waits until there are a large number of read-only files to compact,\nand the compaction process appends objects from many read-only files into a few large new files.")]),e._v(" "),s("h2",{attrs:{id:"step-4-wrap-up"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#step-4-wrap-up"}},[e._v("#")]),e._v(" Step 4 - Wrap Up")]),e._v(" "),s("p",[e._v("In this chapter, we described the high-level design for S3-like object storage. We compared the differences\nbetween block storage, file storage, and object storage.")]),e._v(" "),s("p",[e._v("The focus of this interview is on the design of object storage, so we listed how the uploading, downloading, listing\nobjects in a bucket, and versioning of objects are typically done in object storage.")]),e._v(" "),s("p",[e._v("Then we dived deeper into the design. Object storage is composed of a data store and a metadata store. We\nexplained how the data is persisted into the data store and discussed two methods for increasing reliability and\ndurability: replication and erasure coding. For the metadata store, we explained how the multipart upload is\nexecuted and how to design the database schema to support typical use cases. Lastly, we explained how to shard\nthe metadata store to support even larger data volume.")])])}),[],!1,null,null,null);t.default=o.exports}}]);