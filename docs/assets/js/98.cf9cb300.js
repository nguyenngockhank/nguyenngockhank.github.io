(window.webpackJsonp=window.webpackJsonp||[]).push([[98],{1649:function(e,t,s){"use strict";s.r(t);var a=s(7),n=Object(a.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"design-fb-newsfeed-2nd"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#design-fb-newsfeed-2nd"}},[e._v("#")]),e._v(" Design Fb Newsfeed (2nd)")]),e._v(" "),a("p",[e._v("Let's design Facebook's Newsfeed, which would contain posts, photos, videos, and status updates from all the people and pages a user follows. Similar Services: Twitter Newsfeed, Instagram Newsfeed, Quora Newsfeed")]),e._v(" "),a("p",[e._v("Difficulty Level: Hard")]),e._v(" "),a("h2",{attrs:{id:"_1-what-is-facebook-s-newsfeed"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-what-is-facebook-s-newsfeed"}},[e._v("#")]),e._v(" 1. What is Facebook’s newsfeed?")]),e._v(" "),a("p",[e._v("A Newsfeed is the constantly updating list of stories in the middle of Facebook’s homepage. It includes\nstatus updates, photos, videos, links, app activity, and ‘likes’ from people, pages, and groups that a user\nfollows on Facebook. In other words, it is a compilation of a complete scrollable version of your\nfriends’ and your life story from photos, videos, locations, status updates, and other activities.")]),e._v(" "),a("p",[e._v("For any social media site you design - Twitter, Instagram, or Facebook - you will need some newsfeed\nsystem to display updates from friends and followers.")]),e._v(" "),a("h2",{attrs:{id:"_2-requirements-and-goals-of-the-system"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-requirements-and-goals-of-the-system"}},[e._v("#")]),e._v(" 2. Requirements and Goals of the System")]),e._v(" "),a("p",[e._v("Let’s design a newsfeed for Facebook with the following requirements:")]),e._v(" "),a("h3",{attrs:{id:"functional-requirements"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#functional-requirements"}},[e._v("#")]),e._v(" Functional requirements:")]),e._v(" "),a("ol",[a("li",[e._v("Newsfeed will be generated based on the posts from the people, pages, and groups that a user\nfollows.")]),e._v(" "),a("li",[e._v("A user may have many friends and follow a large number of pages/groups.")]),e._v(" "),a("li",[e._v("Feeds may contain images, videos, or just text.")]),e._v(" "),a("li",[e._v("Our service should support appending new posts as they arrive to the newsfeed for all active users.")])]),e._v(" "),a("h3",{attrs:{id:"non-functional-requirements"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#non-functional-requirements"}},[e._v("#")]),e._v(" Non-functional requirements:")]),e._v(" "),a("ol",[a("li",[e._v("Our system should be able to generate any user’s newsfeed in real-time - maximum latency seen\nby the end user would be 2s.")]),e._v(" "),a("li",[e._v("A post shouldn’t take more than 5s to make it to a user’s feed assuming a new newsfeed request comes in.")])]),e._v(" "),a("h2",{attrs:{id:"_3-capacity-estimation-and-constraints"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-capacity-estimation-and-constraints"}},[e._v("#")]),e._v(" 3. Capacity Estimation and Constraints")]),e._v(" "),a("p",[e._v("Let’s assume on average a user has 300 friends and follows 200 pages.")]),e._v(" "),a("p",[a("strong",[e._v("Traffic estimates")]),e._v(": Let’s assume 300M daily active users with each user fetching their timeline an\naverage of five times a day. This will result in 1.5B newsfeed requests per day or approximately 17,500\nrequests per second.")]),e._v(" "),a("p",[a("strong",[e._v("Storage estimates")]),e._v(": On average, let’s assume we need to have around 500 posts in every user’s feed\nthat we want to keep in memory for a quick fetch. Let’s also assume that on average each post would\nbe 1KB in size. This would mean that we need to store roughly 500KB of data per user. To store all this\ndata for all the active users we would need 150TB of memory. If a server can hold 100GB we would\nneed around 1500 machines to keep the top 500 posts in memory for all active users.")]),e._v(" "),a("h2",{attrs:{id:"_4-system-apis"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-system-apis"}},[e._v("#")]),e._v(" 4. System APIs")]),e._v(" "),a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[e._v("TIP")]),e._v(" "),a("p",[e._v("Once we have finalized the requirements, it’s always a good idea to define the system APIs.\nThis should explicitly state what is expected from the system.")])]),e._v(" "),a("p",[e._v("We can have SOAP or REST APIs to expose the functionality of our service. The following could be\nthe definition of the API for getting the newsfeed:")]),e._v(" "),a("p",[a("code",[e._v("getUserFeed(api_dev_key, user_id, since_id, count, max_id, exclude_replies)")])]),e._v(" "),a("p",[a("strong",[e._v("Parameters")]),e._v(":")]),e._v(" "),a("ul",[a("li",[e._v("api_dev_key (string): The API developer key of a registered can be used to, among other things, throttle users based on their allocated quota.")]),e._v(" "),a("li",[e._v("user_id (number): The ID of the user for whom the system will generate the newsfeed.")]),e._v(" "),a("li",[e._v("since_id (number): Optional; returns results with an ID higher than (that is, more recent than) the specified ID.")]),e._v(" "),a("li",[e._v("count (number): Optional; specifies the number of feed items to try and retrieve up to a maximum of 200 per distinct request.")]),e._v(" "),a("li",[e._v("max_id (number): Optional; returns results with an ID less than (that is, older than) or equal to the specified ID.")]),e._v(" "),a("li",[e._v("exclude_replies(boolean): Optional; this parameter will prevent replies from appearing in the returned timeline.")])]),e._v(" "),a("p",[e._v("Returns: (JSON) Returns a JSON object containing a list of feed items.")]),e._v(" "),a("h2",{attrs:{id:"_5-database-design"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-database-design"}},[e._v("#")]),e._v(" 5. Database Design")]),e._v(" "),a("p",[e._v("There are three primary objects: User, Entity (e.g. page, group, etc.), and FeedItem (or Post). Here are\nsome observations about the relationships between these entities:")]),e._v(" "),a("ul",[a("li",[e._v("A User can follow other entities and can become friends with other users.")]),e._v(" "),a("li",[e._v("Both users and entities can post FeedItems which can contain text, images, or videos.")]),e._v(" "),a("li",[e._v("Each FeedItem will have a UserID which will point to the User who created it. For simplicity, let’s assume that only users can create feed items, although, on Facebook Pages can post feed item too.")]),e._v(" "),a("li",[e._v("Each FeedItem can optionally have an EntityID pointing to the page or the group where that post was created.")])]),e._v(" "),a("p",[e._v("If we are using a relational database, we would need to model two relations: User-Entity relation and\nFeedItem-Media relation. Since each user can be friends with many people and follow a lot of entities,\nwe can store this relation in a separate table. The “Type” column in “UserFollow” identifies if the\nentity being followed is a User or Entity. Similarly, we can have a table for FeedMedia relation.")]),e._v(" "),a("p",[a("img",{attrs:{src:s(997),alt:"img"}})]),e._v(" "),a("h2",{attrs:{id:"_6-high-level-system-design"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-high-level-system-design"}},[e._v("#")]),e._v(" 6. High Level System Design")]),e._v(" "),a("p",[e._v("At a high level this problem can be divided into two parts:")]),e._v(" "),a("p",[a("strong",[e._v("Feed generation")]),e._v(": Newsfeed is generated from the posts (or feed items) of users and entities (pages and\ngroups) that a user follows. So, whenever our system receives a request to generate the feed for a user\n(say Jane), we will perform the following steps:")]),e._v(" "),a("ol",[a("li",[e._v("Retrieve IDs of all users and entities that Jane follows.")]),e._v(" "),a("li",[e._v("Retrieve latest, most popular and relevant posts for those IDs. These are the potential posts that\nwe can show in Jane’s newsfeed.")]),e._v(" "),a("li",[e._v("Rank these posts based on the relevance to Jane. This represents Jane’s current feed.")]),e._v(" "),a("li",[e._v("Store this feed in the cache and return top posts (say 20) to be rendered on Jane’s feed.")]),e._v(" "),a("li",[e._v("On the front-end, when Jane reaches the end of her current feed, she can fetch the next 20 posts\nfrom the server and so on.")])]),e._v(" "),a("p",[e._v("One thing to notice here is that we generated the feed once and stored it in the cache. What about new\nincoming posts from people that Jane follows? If Jane is online, we should have a mechanism to rank\nand add those new posts to her feed. We can periodically (say every five minutes) perform the above\nsteps to rank and add the newer posts to her feed. Jane can then be notified that there are newer items in\nher feed that she can fetch.")]),e._v(" "),a("p",[a("strong",[e._v("Feed publishing")]),e._v(": Whenever Jane loads her newsfeed page, she has to request and pull feed items from\nthe server. When she reaches the end of her current feed, she can pull more data from the server. For\nnewer items either the server can notify Jane and then she can pull, or the server can push, these new\nposts. We will discuss these options in detail later.")]),e._v(" "),a("p",[e._v("At a high level, we will need following components in our Newsfeed service:")]),e._v(" "),a("ol",[a("li",[e._v("Web servers: To maintain a connection with the user. This connection will be used to transfer\ndata between the user and the server.")]),e._v(" "),a("li",[e._v("Application server: To execute the workflows of storing new posts in the database servers. We\nwill also need some application servers to retrieve and to push the newsfeed to the end user.")]),e._v(" "),a("li",[e._v("Metadata database and cache: To store the metadata about Users, Pages, and Groups.")]),e._v(" "),a("li",[e._v("Posts database and cache: To store metadata about posts and their contents.")]),e._v(" "),a("li",[e._v("Video and photo storage, and cache: Blob storage, to store all the media included in the posts.")]),e._v(" "),a("li",[e._v("Newsfeed generation service: To gather and rank all the relevant posts for a user to generate\nnewsfeed and store in the cache. This service will also receive live updates and will add these\nnewer feed items to any user’s timeline.")]),e._v(" "),a("li",[e._v("Feed notification service: To notify the user that there are newer items available for their\nnewsfeed.")])]),e._v(" "),a("p",[e._v("Following is the high-level architecture diagram of our system. User B and C are following User A.")]),e._v(" "),a("p",[a("img",{attrs:{src:s(998),alt:"img"}}),e._v(" "),a("em",[e._v("Facebook Newsfeed Architecture")])]),e._v(" "),a("h2",{attrs:{id:"_7-detailed-component-design"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_7-detailed-component-design"}},[e._v("#")]),e._v(" 7. Detailed Component Design")]),e._v(" "),a("p",[e._v("Let’s discuss different components of our system in detail.")]),e._v(" "),a("h3",{attrs:{id:"a-feed-generation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#a-feed-generation"}},[e._v("#")]),e._v(" a. Feed generation")]),e._v(" "),a("p",[e._v("Let’s take the simple case of the newsfeed generation service fetching most recent posts from all the\nusers and entities that Jane follows; the query would look like this:")]),e._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("SELECT")]),e._v(" FeedItemID "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("FROM")]),e._v(" FeedItem "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("WHERE")]),e._v(" UserID "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("in")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("SELECT")]),e._v(" EntityOrFriendID "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("FROM")]),e._v(" UserFollow \n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("WHERE")]),e._v(" UserID "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("<")]),e._v("current_user_id"),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v(">")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("and")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("type")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[e._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("user")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("UNION")]),e._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("SELECT")]),e._v(" FeedItemID "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("FROM")]),e._v(" FeedItem "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("WHERE")]),e._v(" EntityID "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("in")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("SELECT")]),e._v(" EntityOrFriendID "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("FROM")]),e._v(" UserFollow \n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("WHERE")]),e._v(" UserID "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("<")]),e._v("current_user_id"),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v(">")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("and")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("type")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[e._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("entity"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("ORDER")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("BY")]),e._v(" CreationDate "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("DESC")]),e._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("LIMIT")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[e._v("100")]),e._v("\n")])])]),a("p",[e._v("Here are issues with this design for the feed generation service:")]),e._v(" "),a("ol",[a("li",[e._v("Crazy slow for users with a lot of friends/follows as we have to perform\nsorting/merging/ranking of a huge number of posts.")]),e._v(" "),a("li",[e._v("We generate the timeline when a user loads their page. This would be quite slow and have a\nhigh latency.")]),e._v(" "),a("li",[e._v("For live updates, each status update will result in feed updates for all followers. This could\nresult in high backlogs in our Newsfeed Generation Service.")]),e._v(" "),a("li",[e._v("For live updates, the server pushing (or notifying about) newer posts to users could lead to very\nheavy loads, especially for people or pages that have a lot of followers. To improve the\nefficiency, we can pre-generate the timeline and store it in a memory.")])]),e._v(" "),a("p",[a("strong",[e._v("Offline generation for newsfeed")]),e._v(": We can have dedicated servers that are continuously generating\nusers’ newsfeed and storing them in memory. So, whenever a user requests for the new posts for their\nfeed, we can simply serve it from the pre-generated, stored location. Using this scheme, user’s\nnewsfeed is not compiled on load, but rather on a regular basis and returned to users whenever they\nrequest for it.")]),e._v(" "),a("p",[e._v("Whenever these servers need to generate the feed for a user, they will first query to see what was the\nlast time the feed was generated for that user. Then, new feed data would be generated from that time\nonwards. We can store this data in a hash table where the “key” would be UserID and “value” would be\na STRUCT like this:")]),e._v(" "),a("div",{staticClass:"language-java extra-class"},[a("pre",{pre:!0,attrs:{class:"language-java"}},[a("code",[a("span",{pre:!0,attrs:{class:"token class-name"}},[e._v("Struct")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("\n "),a("span",{pre:!0,attrs:{class:"token class-name"}},[e._v("LinkedHashMap")]),a("span",{pre:!0,attrs:{class:"token generics"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("<")]),a("span",{pre:!0,attrs:{class:"token class-name"}},[e._v("FeedItemID")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[e._v("FeedItem")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(">")])]),e._v(" feedItems"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(";")]),e._v("\n "),a("span",{pre:!0,attrs:{class:"token class-name"}},[e._v("DateTime")]),e._v(" lastGenerated"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(";")]),e._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v("\n")])])]),a("p",[e._v("We can store FeedItemIDs in a data structure similar to Linked HashMap or TreeMap, which can allow\nus to not only jump to any feed item but also iterate through the map easily. Whenever users want to\nfetch more feed items, they can send the last FeedItemID they currently see in their newsfeed, we can\nthen jump to that FeedItemID in our hash-map and return next batch/page of feed items from there.")]),e._v(" "),a("p",[a("strong",[e._v("How many feed items should we store in memory for a user’s feed?")]),e._v(" Initially, we can decide to store\n500 feed items per user, but this number can be adjusted later based on the usage pattern. For example,\nif we assume that one page of a user’s feed has 20 posts and most of the users never browse more than\nten pages of their feed, we can decide to store only 200 posts per user. For any user who wants to see\nmore posts (more than what is stored in memory), we can always query backend servers.")]),e._v(" "),a("p",[a("strong",[e._v("Should we generate (and keep in memory) newsfeeds for all users?")]),e._v(" There will be a lot of users that\ndon’t login frequently. Here are a few things we can do to handle this;")]),e._v(" "),a("ol",[a("li",[e._v("a more straightforward\napproach could be, to use a LRU based cache that can remove users from memory that haven’t\naccessed their newsfeed for a long time")]),e._v(" "),a("li",[e._v("a smarter solution can figure out the login pattern of users to pre-generate their newsfeed, e.g., at what time of the day a user is active and which days of the week does a user access their newsfeed? etc.")])]),e._v(" "),a("p",[e._v("Let’s now discuss some solutions to our “live updates” problems in the following section.")]),e._v(" "),a("h3",{attrs:{id:"b-feed-publishing"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#b-feed-publishing"}},[e._v("#")]),e._v(" b. Feed publishing")]),e._v(" "),a("p",[e._v("The process of pushing a post to all the followers is called a fanout. By analogy, the push approach is\ncalled fanout-on-write, while the pull approach is called fanout-on-load. Let’s discuss different options\nfor publishing feed data to users.")]),e._v(" "),a("ol",[a("li",[a("strong",[e._v("“Pull” model or Fan-out-on-load")]),e._v(": This method involves keeping all the recent feed data in memory so that users can pull it from the server whenever they need it. Clients can pull the feed data on a regular basis or manually whenever they need it. Possible problems with this approach are\n"),a("ul",[a("li",[e._v("a. New data might not be shown to the users until they issue a pull request")]),e._v(" "),a("li",[e._v("b. It’s hard to find the right pull cadence, as most of the time pull requests will result in an empty  response if there is no new data, causing waste of resources.")])])]),e._v(" "),a("li",[a("strong",[e._v("“Push” model or Fan-out-on-write")]),e._v(": For a push system, once a user has published a post, we\ncan immediately push this post to all the followers. The advantage is that when fetching feed\nyou don’t need to go through your friend’s list and get feeds for each of them. It significantly\nreduces read operations. To efficiently handle this, users have to maintain a Long Poll request\nwith the server for receiving the updates. A possible problem with this approach is that when a\nuser has millions of followers (a celebrity-user) the server has to push updates to a lot of people.")]),e._v(" "),a("li",[a("strong",[e._v("Hybrid")]),e._v(": An alternate method to handle feed data could be to use a hybrid approach, i.e., to do a\ncombination of fan-out-on-write and fan-out-on-load. Specifically, we can stop pushing posts\nfrom users with a high number of followers (a celebrity user) and only push data for those users\nwho have a few hundred (or thousand) followers. For celebrity users, we can let the followers\npull the updates. Since the push operation can be extremely costly for users who have a lot of\nfriends or followers, by disabling fanout for them, we can save a huge number of resources.\nAnother alternate approach could be that, once a user publishes a post, we can limit the fanout\nto only her online friends. Also, to get benefits from both the approaches, a combination of\n‘push to notify’ and ‘pull for serving’ end users is a great way to go. Purely a push or pull model\nis less versatile.")])]),e._v(" "),a("p",[a("strong",[e._v("How many feed items can we return to the client in each request?")]),e._v(" We should have a maximum limit\nfor the number of items a user can fetch in one request (say 20). But, we should let the client specify\nhow many feed items they want with each request as the user may like to fetch a different number of\nposts depending on the device (mobile vs. desktop).")]),e._v(" "),a("p",[a("strong",[e._v("Should we always notify users if there are new posts available for their newsfeed?")]),e._v(" It could be\nuseful for users to get notified whenever new data is available. However, on mobile devices, where data\nusage is relatively expensive, it can consume unnecessary bandwidth. Hence, at least for mobile\ndevices, we can choose not to push data, instead, let users “Pull to Refresh” to get new posts.")]),e._v(" "),a("h2",{attrs:{id:"_8-feed-ranking"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_8-feed-ranking"}},[e._v("#")]),e._v(" 8. Feed Ranking")]),e._v(" "),a("p",[e._v("The most straightforward way to rank posts in a newsfeed is by the creation time of the posts, but\ntoday’s ranking algorithms are doing a lot more than that to ensure “important” posts are ranked higher.\nThe high-level idea of ranking is first to select key “signals” that make a post important and then to\nfind out how to combine them to calculate a final ranking score.")]),e._v(" "),a("p",[e._v("More specifically, we can select features that are relevant to the importance of any feed item, e.g.,\nnumber of likes, comments, shares, time of the update, whether the post has images/videos, etc., and\nthen, a score can be calculated using these features. This is generally enough for a simple ranking\nsystem. A better ranking system can significantly improve itself by constantly evaluating if we are\nmaking progress in user stickiness, retention, ads revenue, etc.")]),e._v(" "),a("h2",{attrs:{id:"_9-data-partitioning"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_9-data-partitioning"}},[e._v("#")]),e._v(" 9. Data Partitioning")]),e._v(" "),a("h3",{attrs:{id:"a-sharding-posts-and-metadata"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#a-sharding-posts-and-metadata"}},[e._v("#")]),e._v(" a. Sharding posts and metadata")]),e._v(" "),a("p",[e._v("Since we have a huge number of new posts every day and our read load is extremely high too, we need\nto distribute our data onto multiple machines such that we can read/write it efficiently. For sharding our\ndatabases that are storing posts and their metadata, we can have a similar design as discussed under\nDesigning Twitter.")]),e._v(" "),a("h3",{attrs:{id:"b-sharding-feed-data"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#b-sharding-feed-data"}},[e._v("#")]),e._v(" b. Sharding feed data")]),e._v(" "),a("p",[e._v("For feed data, which is being stored in memory, we can partition it based on UserID. We can try storing\nall the data of a user on one server. When storing, we can pass the UserID to our hash function that will\nmap the user to a cache server where we will store the user’s feed objects. Also, for any given user,\nsince we don’t expect to store more than 500 FeedItmeIDs, we will not run into a scenario where feed\ndata for a user doesn’t fit on a single server. To get the feed of a user, we would always have to query\nonly one server. For future growth and replication, we must use Consistent Hashing.")])])}),[],!1,null,null,null);t.default=n.exports},997:function(e,t,s){e.exports=s.p+"assets/img/24.5dbc32ef.png"},998:function(e,t,s){e.exports=s.p+"assets/img/25.870b7b10.png"}}]);