(window.webpackJsonp=window.webpackJsonp||[]).push([[61],{1629:function(e,t,a){"use strict";a.r(t);var s=a(7),o=Object(s.a)({},(function(){var e=this,t=e.$createElement,s=e._self._c||t;return s("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[s("h1",{attrs:{id:"design-instagram-3rd"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#design-instagram-3rd"}},[e._v("#")]),e._v(" Design Instagram (3rd)")]),e._v(" "),s("p",[e._v("Let's design a photo-sharing service like Instagram, where users can upload photos to share them with\nother users. Similar Services: Flickr, Picasa")]),e._v(" "),s("p",[e._v("Difficulty Level: Medium")]),e._v(" "),s("h2",{attrs:{id:"_1-what-is-instagram"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-what-is-instagram"}},[e._v("#")]),e._v(" 1. What is Instagram?")]),e._v(" "),s("p",[e._v("Instagram is a social networking service which enables its users to upload and share their photos and\nvideos with other users. Instagram users can choose to share information either publicly or privately.\nAnything shared publicly can be seen by any other user, whereas privately shared content can only be\naccessed by a specified set of people. Instagram also enables its users to share through many other\nsocial networking platforms, such as Facebook, Twitter, Flickr, and Tumblr.")]),e._v(" "),s("p",[e._v("For the sake of this exercise, we plan to design a simpler version of Instagram, where a user can share\nphotos and can also follow other users. The ‘News Feed’ for each user will consist of top photos of all\nthe people the user follows.")]),e._v(" "),s("h2",{attrs:{id:"_2-requirements-and-goals-of-the-system"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-requirements-and-goals-of-the-system"}},[e._v("#")]),e._v(" 2. Requirements and Goals of the System")]),e._v(" "),s("p",[e._v("We’ll focus on the following set of requirements while designing the Instagram:")]),e._v(" "),s("h3",{attrs:{id:"functional-requirements"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#functional-requirements"}},[e._v("#")]),e._v(" Functional Requirements")]),e._v(" "),s("ol",[s("li",[e._v("Users should be able to upload/download/view photos.")]),e._v(" "),s("li",[e._v("Users can perform searches based on photo/video titles.")]),e._v(" "),s("li",[e._v("Users can follow other users.")]),e._v(" "),s("li",[e._v("The system should be able to generate and display a user’s News Feed consisting of top photos\nfrom all the people the user follows.")])]),e._v(" "),s("h3",{attrs:{id:"non-functional-requirements"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#non-functional-requirements"}},[e._v("#")]),e._v(" Non-functional Requirements")]),e._v(" "),s("ol",[s("li",[e._v("Our service needs to be highly available.")]),e._v(" "),s("li",[e._v("The acceptable latency of the system is 200ms for News Feed generation.")]),e._v(" "),s("li",[e._v("Consistency can take a hit (in the interest of availability), if a user doesn’t see a photo for a\nwhile; it should be fine.")]),e._v(" "),s("li",[e._v("The system should be highly reliable; any uploaded photo or video should never be lost.\nNot in scope: Adding tags to photos, searching photos on tags, commenting on photos, tagging users to\nphotos, who to follow, etc.")])]),e._v(" "),s("h2",{attrs:{id:"_3-some-design-considerations"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-some-design-considerations"}},[e._v("#")]),e._v(" 3. Some Design Considerations")]),e._v(" "),s("p",[e._v("The system would be read-heavy, so we will focus on building a system that can retrieve photos\nquickly.")]),e._v(" "),s("ol",[s("li",[e._v("Practically, users can upload as many photos as they like. Efficient management of storage\nshould be a crucial factor while designing this system.")]),e._v(" "),s("li",[e._v("Low latency is expected while viewing photos.")]),e._v(" "),s("li",[e._v("Data should be 100% reliable. If a user uploads a photo, the system will guarantee that it will\nnever be lost.")])]),e._v(" "),s("h2",{attrs:{id:"_4-capacity-estimation-and-constraints"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-capacity-estimation-and-constraints"}},[e._v("#")]),e._v(" 4. Capacity Estimation and Constraints")]),e._v(" "),s("ul",[s("li",[e._v("Let’s assume we have 500M total users, with 1M daily active users.")]),e._v(" "),s("li",[e._v("2M new photos every day, 23 new photos every second.")]),e._v(" "),s("li",[e._v("Average photo file size => 200KB")]),e._v(" "),s("li",[e._v("Total space required for 1 day of photos")])]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("2M * 200KB => 400 GB\n")])])]),s("ul",[s("li",[e._v("Total space required for 10 years:")])]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("400GB * 365 (days a year) * 10 (years) ~= 1425TB \n")])])]),s("h2",{attrs:{id:"_5-high-level-system-design"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_5-high-level-system-design"}},[e._v("#")]),e._v(" 5. High Level System Design")]),e._v(" "),s("p",[e._v("At a high-level, we need to support two scenarios, one to upload photos and the other to view/search\nphotos. Our service would need some object storage servers to store photos and also some database\nservers to store metadata information about the photos.")]),e._v(" "),s("p",[s("img",{attrs:{src:a(936),alt:"Figure 1"}})]),e._v(" "),s("h2",{attrs:{id:"_6-database-schema"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_6-database-schema"}},[e._v("#")]),e._v(" 6. Database Schema")]),e._v(" "),s("div",{staticClass:"custom-block tip"},[s("p",{staticClass:"custom-block-title"},[e._v("TIP")]),e._v(" "),s("p",[e._v("Defining the DB schema in the early stages of the interview would help to understand the data\nflow among various components and later would guide towards data partitioning.")]),e._v(" "),s("p",[e._v("We need to store data about users, their uploaded photos, and people they follow. Photo table will store\nall data related to a photo; we need to have an index on (PhotoID, CreationDate) since we need to fetch\nrecent photos first.")])]),e._v(" "),s("p",[s("img",{attrs:{src:a(937),alt:"Figure 2"}})]),e._v(" "),s("p",[e._v("A straightforward approach for storing the above schema would be to use an RDBMS like MySQL\nsince we require joins. But relational databases come with their challenges, especially when we need to\nscale them.")]),e._v(" "),s("p",[e._v("We can store photos in a distributed file storage like "),s("a",{attrs:{href:"https://en.wikipedia.org/wiki/Apache_Hadoop",target:"_blank",rel:"noopener noreferrer"}},[e._v("HDFS"),s("OutboundLink")],1),e._v(" or "),s("a",{attrs:{href:"https://en.wikipedia.org/wiki/Amazon_S3",target:"_blank",rel:"noopener noreferrer"}},[e._v("S3"),s("OutboundLink")],1),e._v(".")]),e._v(" "),s("p",[e._v("We can store the above schema in a distributed key-value store to enjoy the benefits offered by NoSQL.\nAll the metadata related to photos can go to a table where the ‘key’ would be the ‘PhotoID’ and the\n‘value’ would be an object containing PhotoLocation, UserLocation, CreationTimestamp, etc.")]),e._v(" "),s("p",[e._v("We need to store relationships between users and photos, to know who owns which photo. We also\nneed to store the list of people a user follows. For both of these tables, we can use a wide-column\ndatastore like Cassandra. For the ‘UserPhoto’ table, the ‘key’ would be ‘UserID’ and the ‘value’ would\nbe the list of ‘PhotoIDs’ the user owns, stored in different columns. We will have a similar scheme for\nthe ‘UserFollow’ table.")]),e._v(" "),s("p",[e._v("Cassandra or key-value stores in general, always maintain a certain number of replicas to offer\nreliability. Also, in such data stores, deletes don’t get applied instantly, data is retained for certain days (to support undeleting) before getting removed from the system permanently.")]),e._v(" "),s("h2",{attrs:{id:"_7-data-size-estimation"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_7-data-size-estimation"}},[e._v("#")]),e._v(" 7. Data Size Estimation")]),e._v(" "),s("p",[e._v("Let’s estimate how much data will be going into each table and how much total storage we will need\nfor 10 years.")]),e._v(" "),s("p",[e._v("User: Assuming each “int” and “dateTime” is four bytes, each row in the User’s table will be of 68\nbytes:")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("UserID (4 bytes) + Name (20 bytes) + Email (32 bytes) \n    + DateOfBirth (4 bytes) + CreationDate (4bytes) \n    + LastLogin (4 bytes) = 68 bytes\n")])])]),s("p",[e._v("If we have 500 million users, we will need 32GB of total storage.")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("500 million * 68 ~= 32GB \n")])])]),s("p",[s("strong",[e._v("Photo")]),e._v(": Each row in Photo’s table will be of 284 bytes:")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("PhotoID (4 bytes) + UserID (4 bytes) + PhotoPath (256 bytes) \n    + PhotoLatitude (4 bytes) +  PhotLongitude(4 bytes) + UserLatitude (4 bytes) \n    + UserLongitude (4 bytes) + CreationDate (4 bytes) = 284 bytes\n")])])]),s("p",[e._v("If 2M new photos get uploaded every day, we will need 0.5GB of storage for one day:")]),e._v(" "),s("p",[e._v("2M * 284 bytes ~= 0.5GB per day")]),e._v(" "),s("p",[e._v("For 10 years we will need 1.88TB of storage.")]),e._v(" "),s("p",[s("strong",[e._v("UserFollow")]),e._v(": Each row in the UserFollow table will consist of 8 bytes. If we have 500 million users and on average each user follows 500 users. We would need 1.82TB of storage for the UserFollow table:")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("500 million users * 500 followers * 8 bytes ~= 1.82TB\n")])])]),s("p",[e._v("Total space required for all tables for 10 years will be 3.7TB:")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("32GB + 1.88TB + 1.82TB ~= 3.7TB\n")])])]),s("h2",{attrs:{id:"_8-component-design"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_8-component-design"}},[e._v("#")]),e._v(" 8. Component Design")]),e._v(" "),s("p",[e._v("Photo uploads (or writes) can be slow as they have to go to the disk, whereas reads will be faster,\nespecially if they are being served from cache.")]),e._v(" "),s("p",[e._v("Uploading users can consume all the available connections, as uploading is a slow process. This means\nthat ‘reads’ cannot be served if the system gets busy with all the write requests. We should keep in mind\nthat web servers have a connection limit before designing our system. If we assume that a web server\ncan have a maximum of 500 connections at any time, then it can’t have more than 500 concurrent\nuploads or reads. To handle this bottleneck we can split reads and writes into separate services. We will\nhave dedicated servers for reads and different servers for writes to ensure that uploads don’t hog the\nsystem.")]),e._v(" "),s("p",[e._v("Separating photos’ read and write requests will also allow us to scale and optimize each of these\noperations independently")]),e._v(" "),s("p",[s("img",{attrs:{src:a(938),alt:"Figure 3"}})]),e._v(" "),s("h2",{attrs:{id:"_9-reliability-and-redundancy"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_9-reliability-and-redundancy"}},[e._v("#")]),e._v(" 9. Reliability and Redundancy")]),e._v(" "),s("p",[e._v("Losing files is not an option for our service. Therefore, we will store multiple copies of each file so that\nif one storage server dies we can retrieve the photo from the other copy present on a different storage\nserver.")]),e._v(" "),s("p",[e._v("This same principle also applies to other components of the system. If we want to have high availability\nof the system, we need to have multiple replicas of services running in the system, so that if a few\nservices die down the system still remains available and running. Redundancy removes the single point\nof failure in the system.")]),e._v(" "),s("p",[e._v("If only one instance of a service is required to run at any point, we can run a redundant secondary copy\nof the service that is not serving any traffic, but it can take control after the failover when primary has a\nproblem.")]),e._v(" "),s("p",[e._v("Creating redundancy in a system can remove single points of failure and provide a backup or spare\nfunctionality if needed in a crisis. For example, if there are two instances of the same service running in\nproduction and one fails or degrades, the system can failover to the healthy copy. Failover can happen\nautomatically or require manual intervention.")]),e._v(" "),s("p",[s("img",{attrs:{src:a(939),alt:"Figure 4"}})]),e._v(" "),s("h2",{attrs:{id:"_10-data-sharding"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_10-data-sharding"}},[e._v("#")]),e._v(" 10. Data Sharding")]),e._v(" "),s("p",[e._v("Let’s discuss different schemes for metadata sharding:")]),e._v(" "),s("p",[s("strong",[e._v("a. Partitioning based on UserID")]),e._v(" Let’s assume we shard based on the ‘UserID’ so that we can keep all\nphotos of a user on the same shard. If one DB shard is 1TB, we will need four shards to store 3.7TB of\ndata. Let’s assume for better performance and scalability we keep 10 shards.")]),e._v(" "),s("p",[e._v("So we’ll find the shard number by UserID % 10 and then store the data there. To uniquely identify any\nphoto in our system, we can append shard number with each PhotoID.")]),e._v(" "),s("p",[s("strong",[e._v("How can we generate PhotoIDs?")]),e._v(" Each DB shard can have its own auto-increment sequence for PhotoIDs and since we will append ShardID with each PhotoID, it will make it unique throughout our\nsystem.")]),e._v(" "),s("p",[s("strong",[e._v("What are the different issues with this partitioning scheme?")])]),e._v(" "),s("ol",[s("li",[e._v("How would we handle hot users? Several people follow such hot users and a lot of other people\nsee any photo they upload.")]),e._v(" "),s("li",[e._v("Some users will have a lot of photos compared to others, thus making a non-uniform\ndistribution of storage.")]),e._v(" "),s("li",[e._v("What if we cannot store all pictures of a user on one shard? If we distribute photos of a user\nonto multiple shards will it cause higher latencies?")]),e._v(" "),s("li",[e._v("Storing all photos of a user on one shard can cause issues like unavailability of all of the user’s\ndata if that shard is down or higher latency if it is serving high load etc.")])]),e._v(" "),s("p",[s("strong",[e._v("b. Partitioning based on PhotoID")]),e._v(" If we can generate unique PhotoIDs first and then find a shard\nnumber through “"),s("code",[e._v("PhotoID % 10")]),e._v("”, the above problems will have been solved. We would not need to\nappend ShardID with PhotoID in this case as PhotoID will itself be unique throughout the system.")]),e._v(" "),s("p",[s("strong",[e._v("How can we generate PhotoIDs?")]),e._v(" Here we cannot have an auto-incrementing sequence in each shard\nto define PhotoID because we need to know PhotoID first to find the shard where it will be stored. One\nsolution could be that we dedicate a separate database instance to generate auto-incrementing IDs. If\nour PhotoID can fit into 64 bits, we can define a table containing only a 64 bit ID field. So whenever\nwe would like to add a photo in our system, we can insert a new row in this table and take that ID to be\nour PhotoID of the new photo.")]),e._v(" "),s("p",[s("strong",[e._v("Wouldn’t this key generating DB be a single point of failure?")]),e._v(" Yes, it would be. A workaround for\nthat could be defining two such databases with one generating even numbered IDs and the other odd\nnumbered. For the MySQL, the following script can define such sequences:")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("KeyGeneratingServer1:\nauto-increment-increment = 2\nauto-increment-offset = 1\n\nKeyGeneratingServer2:\nauto-increment-increment = 2\nauto-increment-offset = 2\n")])])]),s("p",[e._v("We can put a load balancer in front of both of these databases to round robin between them and to deal\nwith downtime. Both these servers could be out of sync with one generating more keys than the other,\nbut this will not cause any issue in our system. We can extend this design by defining separate ID tables\nfor Users, Photo-Comments, or other objects present in our system.")]),e._v(" "),s("p",[s("strong",[e._v("Alternately")]),e._v(", we can implement a ‘key’ generation scheme similar to what we have discussed in\nDesigning a URL Shortening service like TinyURL.")]),e._v(" "),s("p",[s("strong",[e._v("How can we plan for the future growth of our system?")]),e._v(" We can have a large number of logical\npartitions to accommodate future data growth, such that in the beginning, multiple logical partitions\nreside on a single physical database server. Since each database server can have multiple database\ninstances on it, we can have separate databases for each logical partition on any server. So whenever\nwe feel that a particular database server has a lot of data, we can migrate some logical partitions from it\nto another server. We can maintain a config file (or a separate database) that can map our logical\npartitions to database servers; this will enable us to move partitions around easily. Whenever we want\nto move a partition, we only have to update the config file to announce the change.")]),e._v(" "),s("h2",{attrs:{id:"_11-ranking-and-news-feed-generation"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_11-ranking-and-news-feed-generation"}},[e._v("#")]),e._v(" 11. Ranking and News Feed Generation")]),e._v(" "),s("p",[e._v("To create the News Feed for any given user, we need to fetch the latest, most popular and relevant\nphotos of the people the user follows.")]),e._v(" "),s("p",[e._v("For simplicity, let’s assume we need to fetch top 100 photos for a user’s News Feed. Our application\nserver will first get a list of people the user follows and then fetch metadata info of latest 100 photos\nfrom each user. In the final step, the server will submit all these photos to our ranking algorithm which\nwill determine the top 100 photos (based on recency, likeness, etc.) and return them to the user. A\npossible problem with this approach would be higher latency as we have to query multiple tables and\nperform sorting/merging/ranking on the results. To improve the efficiency, we can pre-generate the\nNews Feed and store it in a separate table.")]),e._v(" "),s("p",[e._v("Pre-generating the News Feed: We can have dedicated servers that are continuously generating users’\nNews Feeds and storing them in a ‘UserNewsFeed’ table. So whenever any user needs the latest photos\nfor their News Feed, we will simply query this table and return the results to the user.\nWhenever these servers need to generate the News Feed of a user, they will first query the\nUserNewsFeed table to find the last time the News Feed was generated for that user. Then, new News\nFeed data will be generated from that time onwards (following the steps mentioned above).\nWhat are the different approaches for sending News Feed contents to the users?")]),e._v(" "),s("ol",[s("li",[s("strong",[e._v("Pull")]),e._v(": Clients can pull the News Feed contents from the server on a regular basis or manually whenever they need it. Possible problems with this approach are\n"),s("ul",[s("li",[e._v("a) New data might not be shown to the users until clients issue a pull request")]),e._v(" "),s("li",[e._v("b) Most of the time pull requests will result in an empty response if there is no new data.")])])]),e._v(" "),s("li",[s("strong",[e._v("Push")]),e._v(": Servers can push new data to the users as soon as it is available. To efficiently manage this,\nusers have to maintain a Long Poll request with the server for receiving the updates. A possible\nproblem with this approach is, a user who follows a lot of people or a celebrity user who has millions\nof followers; in this case, the server has to push updates quite frequently.")]),e._v(" "),s("li",[s("strong",[e._v("Hybrid")]),e._v(": We can adopt a hybrid approach. We can move all the users who have a high number of\nfollows to a pull-based model and only push data to those users who have a few hundred (or thousand)\nfollows. Another approach could be that the server pushes updates to all the users not more than a\ncertain frequency, letting users with a lot of follows/updates to regularly pull data.\nFor a detailed discussion about News Feed generation, take a look at Designing Facebook’s Newsfeed.")])]),e._v(" "),s("h2",{attrs:{id:"_12-news-feed-creation-with-sharded-data"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_12-news-feed-creation-with-sharded-data"}},[e._v("#")]),e._v(" 12. News Feed Creation with Sharded Data")]),e._v(" "),s("p",[e._v("One of the most important requirement to create the News Feed for any given user is to fetch the latest\nphotos from all people the user follows. For this, we need to have a mechanism to sort photos on their\ntime of creation. To efficiently do this, we can make photo creation time part of the PhotoID. As we\nwill have a primary index on PhotoID, it will be quite quick to find the latest PhotoIDs.")]),e._v(" "),s("p",[e._v("We can use epoch time for this. Let’s say our PhotoID will have two parts; the first part will be\nrepresenting epoch time and the second part will be an auto-incrementing sequence. So to make a new\nPhotoID, we can take the current epoch time and append an auto-incrementing ID from our keygenerating DB. We can figure out shard number from this PhotoID ( "),s("code",[e._v("PhotoID % 10")]),e._v(") and store the photo there.")]),e._v(" "),s("p",[s("strong",[e._v("What could be the size of our PhotoID?")]),e._v(" Let’s say our epoch time starts today, how many bits we\nwould need to store the number of seconds for next 50 years?")]),e._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[e._v("86400 sec/day * 365 (days a year) * 50 (years) => 1.6 billion seconds \n")])])]),s("p",[e._v("We would need 31 bits to store this number. Since on the average, we are expecting 23 new photos per second; we can allocate 9 bits to store auto incremented sequence. So every second we can store ("),s("code",[e._v("2^9 => 512")]),e._v(") new photos. We can reset our auto incrementing sequence every second.")]),e._v(" "),s("h2",{attrs:{id:"_13-cache-and-load-balancing"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_13-cache-and-load-balancing"}},[e._v("#")]),e._v(" 13. Cache and Load balancing")]),e._v(" "),s("p",[e._v("Our service would need a massive-scale photo delivery system to serve the globally distributed users.\nOur service should push its content closer to the user using a large number of geographically\ndistributed photo cache servers and use CDNs")]),e._v(" "),s("p",[e._v("We can introduce a cache for metadata servers to cache hot database rows. We can use Memcache to\ncache the data and Application servers before hitting database can quickly check if the cache has\ndesired rows. Least Recently Used (LRU) can be a reasonable cache eviction policy for our system.\nUnder this policy, we discard the least recently viewed row first.")]),e._v(" "),s("p",[s("strong",[e._v("How can we build more intelligent cache?")]),e._v(" If we go with 80-20 rule, i.e., 20% of daily read volume\nfor photos is generating 80% of traffic which means that certain photos are so popular that the majority\nof people read them. This dictates that we can try caching 20% of daily read volume of photos and\nmetadata.")])])}),[],!1,null,null,null);t.default=o.exports},936:function(e,t,a){e.exports=a.p+"assets/img/f1.375b48c4.png"},937:function(e,t,a){e.exports=a.p+"assets/img/f2.85c3518b.png"},938:function(e,t,a){e.exports=a.p+"assets/img/f3.a1ef0cfa.png"},939:function(e,t,a){e.exports=a.p+"assets/img/f4.3bc3184e.png"}}]);