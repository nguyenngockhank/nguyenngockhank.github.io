(window.webpackJsonp=window.webpackJsonp||[]).push([[29],{592:function(e,t,n){e.exports=n.p+"assets/img/image--019.d18fd686.jpg"},593:function(e,t,n){e.exports=n.p+"assets/img/image--020.6f6d6ce9.jpg"},594:function(e,t,n){e.exports=n.p+"assets/img/image--021.e99ad9d0.jpg"},978:function(e,t,n){"use strict";n.r(t);var a=n(7),o=Object(a.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"chapter-5-consistency"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#chapter-5-consistency"}},[e._v("#")]),e._v(" Chapter 5. Consistency")]),e._v(" "),a("p",[e._v("One of the biggest changes from a centralized relational database to a cluster-oriented NoSQL\ndatabase is in how you think about consistency. Relational databases try to exhibit "),a("strong",[e._v("strong\nconsistency")]),e._v(" by avoiding all the various inconsistencies that we’ll shortly be discussing. Once you\nstart looking at the NoSQL world, phrases such as “CAP theorem” and “eventual consistency”\nappear, and as soon as you start building something you have to think about what sort of consistency\nyou need for your system.")]),e._v(" "),a("p",[e._v("Consistency comes in various forms, and that one word covers a myriad of ways errors can creep\ninto your life. So we’re going to begin by talking about the various shapes consistency can take. After\nthat we’ll discuss why you may want to relax consistency (and its big sister, durability).")]),e._v(" "),a("h3",{attrs:{id:"_5-1-update-consistency"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-1-update-consistency"}},[e._v("#")]),e._v(" 5.1. Update Consistency")]),e._v(" "),a("p",[e._v("We’ll begin by considering updating a telephone number. Coincidentally, Martin and Pramod are\nlooking at the company website and notice that the phone number is out of date. Implausibly, they both\nhave update access, so they both go in at the same time to update the number. To make the example\ninteresting, we’ll assume they update it slightly differently, because each uses a slightly different\nformat. This issue is called a "),a("strong",[e._v("write-write conflict")]),e._v(" : two people updating the same data item at the\nsame time.")]),e._v(" "),a("p",[e._v("When the writes reach the server, the server will "),a("strong",[e._v("serialize")]),e._v(" them—decide to apply one, then the\nother. Let’s assume it uses alphabetical order and picks Martin’s update first, then Pramod’s. Without\nany concurrency control, Martin’s update would be applied and immediately overwritten by\nPramod’s. In this case Martin’s is a "),a("strong",[e._v("lost update")]),e._v(". Here the lost update is not a big problem, but often it is. We see this as a failure of consistency because Pramod’s update was based on the state before Martin’s update, yet was applied after it.")]),e._v(" "),a("p",[e._v("Approaches for maintaining consistency in the face of concurrency are often described as\npessimistic or optimistic. A "),a("strong",[e._v("pessimistic")]),e._v(" approach works by preventing conflicts from occurring; an\n"),a("strong",[e._v("optimistic")]),e._v(" approach lets conflicts occur, but detects them and takes action to sort them out. For update conflicts, the most common pessimistic approach is to have write locks, so that in order to change a value you need to acquire a lock, and the system ensures that only one client can get a lock at a time.")]),e._v(" "),a("p",[e._v("So Martin and Pramod would both attempt to acquire the write lock, but only Martin (the first one)\nwould succeed. Pramod would then see the result of Martin’s write before deciding whether to make\nhis own update.")]),e._v(" "),a("p",[e._v("A common optimistic approach is a "),a("strong",[e._v("conditional update")]),e._v(" where any client that does an update tests\nthe value just before updating it to see if it’s changed since his last read. In this case, Martin’s update would succeed but Pramod’s would fail. The error would let Pramod know that he should look at the\nvalue again and decide whether to attempt a further update.")]),e._v(" "),a("p",[e._v("Both the pessimistic and optimistic approaches that we’ve just described rely on a consistent\nserialization of the updates. With a single server, this is obvious—it has to choose one, then the other.")]),e._v(" "),a("p",[e._v("But if there’s more than one server, such as with peer-to-peer replication, then two nodes might apply\nthe updates in a different order, resulting in a different value for the telephone number on each peer.\nOften, when people talk about concurrency in distributed systems, they talk about sequential\nconsistency—ensuring that all nodes apply operations in the same order.")]),e._v(" "),a("p",[e._v("There is another optimistic way to handle a write-write conflict—save both updates and record\nthat they are in conflict. This approach is familiar to many programmers from version control systems,\nparticularly distributed version control systems that by their nature will often have conflicting\ncommits. The next step again follows from version control: You have to merge the two updates\nsomehow. Maybe you show both values to the user and ask them to sort it out—this is what happens if\nyou update the same contact on your phone and your computer. Alternatively, the computer may be\nable to perform the merge itself; if it was a phone formatting issue, it may be able to realize that and\napply the new number with the standard format. Any automated merge of write-write conflicts is\nhighly domain-specific and needs to be programmed for each particular case.")]),e._v(" "),a("p",[e._v("Often, when people first encounter these issues, their reaction is to prefer pessimistic concurrency\nbecause they are determined to avoid conflicts. While in some cases this is the right answer, there is\nalways a tradeoff. Concurrent programming involves a fundamental tradeoff between safety (avoiding\nerrors such as update conflicts) and liveness (responding quickly to clients). Pessimistic approaches\noften severely degrade the responsiveness of a system to the degree that it becomes unfit for its\npurpose. This problem is made worse by the danger of errors—pessimistic concurrency often leads\nto deadlocks, which are hard to prevent and debug.")]),e._v(" "),a("p",[e._v("Replication makes it much more likely to run into write-write conflicts. If different nodes have\ndifferent copies of some data which can be independently updated, then you’ll get conflicts unless you\ntake specific measures to avoid them. Using a single node as the target for all writes for some data\nmakes it much easier to maintain update consistency. Of the distribution models we discussed earlier,\nall but peer-to-peer replication do this.")]),e._v(" "),a("h3",{attrs:{id:"_5-2-read-consistency"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-2-read-consistency"}},[e._v("#")]),e._v(" 5.2. Read Consistency")]),e._v(" "),a("p",[e._v("Having a data store that maintains update consistency is one thing, but it doesn’t guarantee that\nreaders of that data store will always get consistent responses to their requests. Let’s imagine we\nhave an order with line items and a shipping charge. The shipping charge is calculated based on the\nline items in the order. If we add a line item, we thus also need to recalculate and update the shipping\ncharge. In a relational database, the shipping charge and line items will be in separate tables. The\ndanger of inconsistency is that Martin adds a line item to his order, Pramod then reads the line items\nand shipping charge, and then Martin updates the shipping charge. This is an "),a("strong",[e._v("inconsistent read")]),e._v(" or\n"),a("strong",[e._v("read-write conflict")]),e._v(" : In Figure 5.1 Pramod has done a read in the middle of Martin’s write.")]),e._v(" "),a("p",[a("img",{attrs:{src:n(592),alt:"img"}})]),e._v(" "),a("p",[a("strong",[e._v("Figure 5.1. A read-write conflict in logical consistency")])]),e._v(" "),a("p",[e._v("We refer to this type of consistency as "),a("strong",[e._v("logical consistency")]),e._v(" : ensuring that different data items make sense together. To avoid a logically inconsistent read-write conflict, relational databases support the notion of transactions. Providing Martin wraps his two writes in a transaction, the system guarantees that Pramod will either read both data items before the update or both after the update.")]),e._v(" "),a("p",[e._v("A common claim we hear is that NoSQL databases don’t support transactions and thus can’t be\nconsistent. Such claim is mostly wrong because it glosses over lots of important details. Our first\nclarification is that any statement about lack of transactions usually only applies to some NoSQL\ndatabases, in particular the aggregate-oriented ones. In contrast, graph databases tend to support\nACID transactions just the same as relational databases.")]),e._v(" "),a("p",[e._v("Secondly, aggregate-oriented databases do support atomic updates, but only within a single\naggregate. This means that you will have logical consistency within an aggregate but not between\naggregates. So in the example, you could avoid running into that inconsistency if the order, the\ndelivery charge, and the line items are all part of a single order aggregate.")]),e._v(" "),a("p",[e._v("Of course not all data can be put in the same aggregate, so any update that affects multiple\naggregates leaves open a time when clients could perform an inconsistent read. The length of time an\ninconsistency is present is called the "),a("strong",[e._v("inconsistency window")]),e._v(". A NoSQL system may have a quite short\ninconsistency window: As one data point, Amazon’s documentation says that the inconsistency\nwindow for its SimpleDB service is usually less than a second.")]),e._v(" "),a("p",[e._v("This example of a logically inconsistent read is the classic example that you’ll see in any book that\ntouches database programming. Once you introduce replication, however, you get a whole new kind\nof inconsistency. Let’s imagine there’s one last hotel room for a desirable event. The hotel\nreservation system runs on many nodes. Martin and Cindy are a couple considering this room, but they\nare discussing this on the phone because Martin is in London and Cindy is in Boston. Meanwhile\nPramod, who is in Mumbai, goes and books that last room. That updates the replicated room\navailability, but the update gets to Boston quicker than it gets to London. When Martin and Cindy fire\nup their browsers to see if the room is available, Cindy sees it booked and Martin sees it free. This is\nanother inconsistent read—but it’s a breach of a different form of consistency we call "),a("strong",[e._v("replication\nconsistency")]),e._v(" : ensuring that the same data item has the same value when read from different replicas\n(see Figure 5.2).")]),e._v(" "),a("p",[a("img",{attrs:{src:n(593),alt:"img"}})]),e._v(" "),a("p",[a("strong",[e._v("Figure 5.2. An example of replication inconsistency")])]),e._v(" "),a("p",[e._v("Eventually, of course, the updates will propagate fully, and Martin will see the room is fully\nbooked. Therefore this situation is generally referred to as "),a("strong",[e._v("eventually consistent")]),e._v(" , meaning that at any time nodes may have replication inconsistencies but, if there are no further updates, eventually all nodes will be updated to the same value. Data that is out of date is generally referred to as "),a("strong",[e._v("stale")]),e._v(" , which reminds us that a cache is another form of replication—essentially following the master-slave distribution model.")]),e._v(" "),a("p",[e._v("Although replication consistency is independent from logical consistency, replication can\nexacerbate a logical inconsistency by lengthening its inconsistency window. Two different updates on\nthe master may be performed in rapid succession, leaving an inconsistency window of milliseconds.\nBut delays in networking could mean that the same inconsistency window lasts for much longer on a\nslave.")]),e._v(" "),a("p",[e._v("Consistency guarantees aren’t something that’s global to an application. You can usually specify the level of consistency you want with individual requests. This allows you to use weak consistency\nmost of the time when it isn’t an issue, but request strong consistency when it is.")]),e._v(" "),a("p",[e._v("The presence of an inconsistency window means that different people will see different things at\nthe same time. If Martin and Cindy are looking at rooms while on a transatlantic call, it can cause\nconfusion. It’s more common for users to act independently, and then this is not a problem. But\ninconsistency windows can be particularly problematic when you get inconsistencies with yourself.\nConsider the example of posting comments on a blog entry. Few people are going to worry about\ninconsistency windows of even a few minutes while people are typing in their latest thoughts. Often,\nsystems handle the load of such sites by running on a cluster and load-balancing incoming requests to\ndifferent nodes. Therein lies a danger: You may post a message using one node, then refresh your\nbrowser, but the refresh goes to a different node which hasn’t received your post yet—and it looks\nlike your post was lost.")]),e._v(" "),a("p",[e._v("In situations like this, you can tolerate reasonably long inconsistency windows, but you need "),a("strong",[e._v("read-\nyour-writes consistency")]),e._v(" which means that, once you’ve made an update, you’re guaranteed to\ncontinue seeing that update. One way to get this in an otherwise eventually consistent system is to\nprovide "),a("strong",[e._v("session consistency")]),e._v(" : Within a user’s session there is read-your-writes consistency. This\ndoes mean that the user may lose that consistency should their session end for some reason or should\nthe user access the same system simultaneously from different computers, but these cases are\nrelatively rare.")]),e._v(" "),a("p",[e._v("There are a couple of techniques to provide session consistency. A common way, and often the\neasiest way, is to have a "),a("strong",[e._v("sticky session")]),e._v(" : a session that’s tied to one node (this is also called "),a("strong",[e._v("session affinity")]),e._v(" ). A sticky session allows you to ensure that as long as you keep read-your-writes consistency on a node, you’ll get it for sessions too. The downside is that sticky sessions reduce the ability of the load balancer to do its job.")]),e._v(" "),a("p",[e._v("Another approach for session consistency is to use version stamps (“Version Stamps,” p. 61 ) and\nensure every interaction with the data store includes the latest version stamp seen by a session. The\nserver node must then ensure that it has the updates that include that version stamp before responding\nto a request.")]),e._v(" "),a("p",[e._v("Maintaining session consistency with sticky sessions and master-slave replication can be awkward\nif you want to read from the slaves to improve read performance but still need to write to the master.\nOne way of handling this is for writes to be sent the slave, who then takes responsibility for\nforwarding them to the master while maintaining session consistency for its client. Another approach\nis to switch the session to the master temporarily when doing a write, just long enough that reads are\ndone from the master until the slaves have caught up with the update.")]),e._v(" "),a("p",[e._v("We’re talking about replication consistency in the context of a data store, but it’s also an important\nfactor in overall application design. Even a simple database system will have lots of occasions\nwhere data is presented to a user, the user cogitates, and then updates that data. It’s usually a bad idea\nto keep a transaction open during user interaction because there’s a real danger of conflicts when the\nuser tries to make her update, which leads to such approaches as offline locks [Fowler PoEAA].")]),e._v(" "),a("h3",{attrs:{id:"_5-3-relaxing-consistency"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-3-relaxing-consistency"}},[e._v("#")]),e._v(" 5.3. Relaxing Consistency")]),e._v(" "),a("p",[e._v("Consistency is a Good Thing—but, sadly, sometimes we have to sacrifice it. It is always possible to\ndesign a system to avoid inconsistencies, but often impossible to do so without making unbearable sacrifices in other characteristics of the system. As a result, we often have to tradeoff consistency for something else. While some architects see this as a disaster, we see it as part of the inevitable\ntradeoffs involved in system design. Furthermore, different domains have different tolerances for\ninconsistency, and we need to take this tolerance into account as we make our decisions.")]),e._v(" "),a("p",[e._v("Trading off consistency is a familiar concept even in single-server relational database systems.\nHere, our principal tool to enforce consistency is the transaction, and transactions can provide strong\nconsistency guarantees. However, transaction systems usually come with the ability to relax isolation\nlevels, allowing queries to read data that hasn’t been committed yet, and in practice we see most\napplications relax consistency down from the highest isolation level (serialized) in order to get\neffective performance. We most commonly see people using the read-committed transaction level,\nwhich eliminates some read-write conflicts but allows others.")]),e._v(" "),a("p",[e._v("Many systems forgo transactions entirely because the performance impact of transactions is too\nhigh. We’ve seen this in a couple different ways. On a small scale, we saw the popularity of MySQL\nduring the days when it didn’t support transactions. Many websites liked the high speed of MySQL\nand were prepared to live without transactions. At the other end of the scale, some very large\nwebsites, such as eBay [Pritchett], have had to forgo transactions in order to perform acceptably—\nthis is particularly true when you need to introduce sharding. Even without these constraints, many\napplication builders need to interact with remote systems that can’t be properly included within a\ntransaction boundary, so updating outside of transactions is a quite common occurrence for enterprise\napplications.")]),e._v(" "),a("h3",{attrs:{id:"_5-3-1-the-cap-theorem"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-3-1-the-cap-theorem"}},[e._v("#")]),e._v(" "),a("strong",[e._v("5.3.1. The CAP Theorem")])]),e._v(" "),a("p",[e._v("In the NoSQL world it’s common to refer to the CAP theorem as the reason why you may need to\nrelax consistency. It was originally proposed by Eric Brewer in 2000 [Brewer] and given a formal\nproof by Seth Gilbert and Nancy Lynch [Lynch and Gilbert] a couple of years later. (You may also\nhear this referred to as Brewer’s Conjecture.)")]),e._v(" "),a("p",[e._v("The basic statement of the CAP theorem is that, given the three properties of Consistency,\nAvailability, and Partition tolerance, you can only get two. Obviously this depends very much on how\nyou define these three properties, and differing opinions have led to several debates on what the real\nconsequences of the CAP theorem are.")]),e._v(" "),a("p",[e._v("Consistency is pretty much as we’ve defined it so far. "),a("strong",[e._v("Availability")]),e._v(" has a particular meaning in the\ncontext of CAP—it means that if you can talk to a node in the cluster, it can read and write data.\nThat’s subtly different from the usual meaning, which we’ll explore later. "),a("strong",[e._v("Partition tolerance")]),e._v(" means\nthat the cluster can survive communication breakages in the cluster that separate the cluster into\nmultiple partitions unable to communicate with each other (situation known as a "),a("strong",[e._v("split brain")]),e._v(" , see\nFigure 5.3).")]),e._v(" "),a("p",[a("img",{attrs:{src:n(594),alt:"img"}})]),e._v(" "),a("p",[a("strong",[e._v("Figure 5.3. With two breaks in the communication lines, the network partitions into two groups.")])]),e._v(" "),a("p",[e._v("A single-server system is the obvious example of a CA system—a system that has Consistency and Availability but not Partition tolerance. A single machine can’t partition, so it does not have to worry\nabout partition tolerance. There’s only one node—so if it’s up, it’s available. Being up and keeping\nconsistency is reasonable. This is the world that most relational database systems live in.")]),e._v(" "),a("p",[e._v("It is theoretically possible to have a CA cluster. However, this would mean that if a partition ever\noccurs in the cluster, all the nodes in the cluster would go down so that no client can talk to a node.\nBy the usual definition of “available,” this would mean a lack of availability, but this is where CAP’s\nspecial usage of “availability” gets confusing. CAP defines “availability” to mean “every request\nreceived by a nonfailing node in the system must result in a response” [Lynch and Gilbert]. So a\nfailed, unresponsive node doesn’t infer a lack of CAP availability.")]),e._v(" "),a("p",[e._v("This does imply that you can build a CA cluster, but you have to ensure it will only partition rarely\nand completely. This can be done, at least within a data center, but it’s usually prohibitively\nexpensive. Remember that in order to bring down all the nodes in a cluster on a partition, you also\nhave to detect the partition in a timely manner—which itself is no small feat.")]),e._v(" "),a("p",[e._v("So clusters have to be tolerant of network partitions. And here is the real point of the CAP theorem.\nAlthough the CAP theorem is often stated as “you can only get two out of three,” in practice what it’s\nsaying is that in a system that may suffer partitions, as distributed system do, you have to trade off\nconsistency versus availability. This isn’t a binary decision; often, you can trade off a little\nconsistency to get some availability. The resulting system would be neither perfectly consistent nor\nperfectly available—but would have a combination that is reasonable for your particular needs.")]),e._v(" "),a("p",[e._v("An example should help illustrate this. Martin and Pramod are both trying to book the last hotel\nroom on a system that uses peer-to-peer distribution with two nodes (London for Martin and Mumbai\nfor Pramod). If we want to ensure consistency, then when Martin tries to book his room on the London\nnode, that node must communicate with the Mumbai node before confirming the booking. Essentially,\nboth nodes must agree on the serialization of their requests. This gives us consistency—but should the\nnetwork link break, then neither system can book any hotel room, sacrificing availability.")]),e._v(" "),a("p",[e._v("One way to improve availability is to designate one node as the master for a particular hotel and\nensure all bookings are processed by that master. Should that master be Mumbai, then Mumbai can\nstill process hotel bookings for that hotel and Pramod will get the last room. If we use master-slave\nreplication, London users can see the inconsistent room information but cannot make a booking and\nthus cause an update inconsistency. However, users expect that it could happen in this situation—so,\nagain, the compromise works for this particular use case.")]),e._v(" "),a("p",[e._v("This improves the situation, but we still can’t book a room on the London node for the hotel whose\nmaster is in Mumbai if the connection goes down. In CAP terminology, this is a failure of availability\nin that Martin can talk to the London node but the London node cannot update the data. To gain more\navailability, we might allow both systems to keep accepting hotel reservations even if the network\nlink breaks down. The danger here is that Martin and Pramod book the last hotel room. However,\ndepending on how this hotel operates, that may be fine. Often, travel companies tolerate a certain\namount of overbooking in order to cope with no-shows. Conversely, some hotels always keep a few\nrooms clear even when they are fully booked, in order to be able to swap a guest out of a room with\nproblems or to accommodate a high-status late booking. Some might even cancel the booking with an\napology once they detected the conflict—reasoning that the cost of that is less than the cost of losing\nbookings on network failures.")]),e._v(" "),a("p",[e._v("The classic example of allowing inconsistent writes is the shopping cart, as discussed in Dynamo\n[Amazon’s Dynamo]. In this case you are always allowed to write to your shopping cart, even if\nnetwork failures mean you end up with multiple shopping carts. The checkout process can merge the\ntwo shopping carts by putting the union of the items from the carts into a single cart and returning that.\nAlmost always that’s the correct answer—but if not, the user gets the opportunity to look at the cart\nbefore completing the order.")]),e._v(" "),a("p",[e._v("The lesson here is that although most software developers treat update consistency as The Way\nThings Must Be, there are cases where you can deal gracefully with inconsistent answers to requests.\nThese situations are closely tied to the domain and require domain knowledge to know how to\nresolve. Thus you can’t usually look to solve them purely within the development team—you have to\ntalk to domain experts. If you can find a way to handle inconsistent updates, this gives you more\noptions to increase availability and performance. For a shopping cart, it means that shoppers can\nalways shop, and do so quickly. And as Patriotic Americans, we know how vital it is to support Our\nRetail Destiny.")]),e._v(" "),a("p",[e._v("A similar logic applies to read consistency. If you are trading financial instruments over a\ncomputerized exchange, you may not be able to tolerate any data that isn’t right up to date. However,\nif you are posting a news item to a media website, you may be able to tolerate old pages for minutes.")]),e._v(" "),a("p",[e._v("In these cases you need to know how tolerant you are of stale reads, and how long the\ninconsistency window can be—often in terms of the average length, worst case, and some measure of\nthe distribution for the lengths. Different data items may have different tolerances for staleness, and\nthus may need different settings in your replication configuration.")]),e._v(" "),a("p",[e._v("Advocates of NoSQL often say that instead of following the ACID properties of relational\ntransactions, NoSQL systems follow the BASE properties (Basically Available, Soft state, Eventual\nconsistency) [Brewer]. Although we feel we ought to mention the BASE acronym here, we don’t think\nit’s very useful. The acronym is even more contrived than ACID, and neither “basically available”\nnor “soft state” have been well defined. We should also stress that when Brewer introduced the\nnotion of BASE, he saw the tradeoff between ACID and BASE as a spectrum, not a binary choice.")]),e._v(" "),a("p",[e._v("We’ve included this discussion of the CAP theorem because it’s often used (and abused) when\ntalking about the tradeoffs involving consistency in distributed databases. However, it’s usually better\nto think not about the tradeoff between consistency and availability but rather between consistency\nand "),a("em",[e._v("latency")]),e._v(". We can summarize much of the discussion about consistency in distribution by saying\nthat we can improve consistency by getting more nodes involved in the interaction, but each node we\nadd increases the response time of that interaction. We can then think of availability as the limit of\nlatency that we’re prepared to tolerate; once latency gets too high, we give up and treat the data as\nunavailable—which neatly fits its definition in the context of CAP.")]),e._v(" "),a("h3",{attrs:{id:"_5-4-relaxing-durability"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-4-relaxing-durability"}},[e._v("#")]),e._v(" 5.4. Relaxing Durability")]),e._v(" "),a("p",[e._v("So far we’ve talked about consistency, which is most of what people mean when they talk about the\nACID properties of database transactions. The key to Consistency is serializing requests by forming\nAtomic, Isolated work units. But most people would scoff at relaxing durability—after all, what is the\npoint of a data store if it can lose updates?")]),e._v(" "),a("p",[e._v("As it turns out, there are cases where you may want to trade off some durability for higher\nperformance. If a database can run mostly in memory, apply updates to its in-memory representation,\nand periodically flush changes to disk, then it may be able to provide substantially higher\nresponsiveness to requests. The cost is that, should the server crash, any updates since the last flush\nwill be lost.")]),e._v(" "),a("p",[e._v("One example of where this tradeoff may be worthwhile is storing user-session state. A big website\nmay have many users and keep temporary information about what each user is doing in some kind of\nsession state. There’s a lot of activity on this state, creating lots of demand, which affects the\nresponsiveness of the website. The vital point is that losing the session data isn’t too much of a\ntragedy—it will create some annoyance, but maybe less than a slower website would cause. This\nmakes it a good candidate for nondurable writes. Often, you can specify the durability needs on a\ncall-by-call basis, so that more important updates can force a flush to disk.")]),e._v(" "),a("p",[e._v("Another example of relaxing durability is capturing telemetric data from physical devices. It may\nbe that you’d rather capture data at a faster rate, at the cost of missing the last updates should the\nserver go down.")]),e._v(" "),a("p",[e._v("Another class of durability tradeoffs comes up with replicated data. A failure of "),a("strong",[e._v("replication durability")]),e._v(" occurs when a node processes an update but fails before that update is replicated to the\nother nodes. A simple case of this may happen if you have a master-slave distribution model where\nthe slaves appoint a new master automatically should the existing master fail. If a master does fail,\nany writes not passed onto the replicas will effectively become lost. Should the master come back\nonline, those updates will conflict with updates that have happened since. We think of this as a\ndurability problem because you think your update has succeeded since the master acknowledged it,\nbut a master node failure caused it to be lost.")]),e._v(" "),a("p",[e._v("If you’re sufficiently confident in bringing the master back online rapidly, this is a reason not to\nauto-failover to a slave. Otherwise, you can improve replication durability by ensuring that the master\nwaits for some replicas to acknowledge the update before the master acknowledges it to the client.\nObviously, however, that will slow down updates and make the cluster unavailable if slaves fail—\nso, again, we have a tradeoff, depending upon how vital durability is. As with basic durability, it’s\nuseful for individual calls to indicate what level of durability they need.")]),e._v(" "),a("h3",{attrs:{id:"_5-5-quorums"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-5-quorums"}},[e._v("#")]),e._v(" 5.5. Quorums")]),e._v(" "),a("p",[e._v("When you’re trading off consistency or durability, it’s not an all or nothing proposition. The more\nnodes you involve in a request, the higher is the chance of avoiding an inconsistency. This naturally\nleads to the question: How many nodes need to be involved to get strong consistency?")]),e._v(" "),a("p",[e._v("Imagine some data replicated over three nodes. You don’t need all nodes to acknowledge a write\nto ensure strong consistency; all you need is two of them—a majority. If you have conflicting writes,\nonly one can get a majority. This is referred to as a "),a("strong",[e._v("write quorum")]),e._v(" and expressed in a slightly\npretentious inequality of W > N/2, meaning the number of nodes participating in the write (W) must be\nmore than the half the number of nodes involved in replication (N). The number of replicas is often\ncalled the "),a("strong",[e._v("replication factor")]),e._v(".")]),e._v(" "),a("p",[e._v("Similarly to the write quorum, there is the notion of read quorum: How many nodes you need to\ncontact to be sure you have the most up-to-date change. The read quorum is a bit more complicated\nbecause it depends on how many nodes need to confirm a write.")]),e._v(" "),a("p",[e._v("Let’s consider a replication factor of 3. If all writes need two nodes to confirm (W = 2) then we\nneed to contact at least two nodes to be sure we’ll get the latest data. If, however, writes are only\nconfirmed by a single node (W = 1) we need to talk to all three nodes to be sure we have the latest\nupdates. In this case, since we don’t have a write quorum, we may have an update conflict, but by\ncontacting enough readers we can be sure to detect it. Thus we can get strongly consistent reads even\nif we don’t have strong consistency on our writes.")]),e._v(" "),a("p",[e._v("This relationship between the number of nodes you need to contact for a read ("),a("code",[e._v("R")]),e._v("), those confirming\na write ("),a("code",[e._v("W")]),e._v("), and the replication factor ("),a("code",[e._v("N")]),e._v(") can be captured in an inequality: You can have a strongly\nconsistent read if "),a("code",[e._v("R + W > N.")])]),e._v(" "),a("p",[e._v("These inequalities are written with a peer-to-peer distribution model in mind. If you have a master-\nslave distribution, you only have to write to the master to avoid write-write conflicts, and similarly\nonly read from the master to avoid read-write conflicts. With this notation, it is common to confuse\nthe number of nodes in the cluster with the replication factor, but these are often different. I may have\n100 nodes in my cluster, but only have a replication factor of 3, with most of the distribution\noccurring due to sharding.")]),e._v(" "),a("p",[e._v("Indeed most authorities suggest that a replication factor of 3 is enough to have good resilience. This\nallows a single node to fail while still maintaining quora for reads and writes. If you have automatic\nrebalancing, it won’t take too long for the cluster to create a third replica, so the chances of losing a second replica before a replacement comes up are slight.")]),e._v(" "),a("p",[e._v("The number of nodes participating in an operation can vary with the operation. When writing, we\nmight require quorum for some types of updates but not others, depending on how much we value\nconsistency and availability. Similarly, a read that needs speed but can tolerate staleness should\ncontact less nodes.")]),e._v(" "),a("p",[e._v("Often you may need to take both into account. If you need fast, strongly consistent reads, you could\nrequire writes to be acknowledged by all the nodes, thus allowing reads to contact only one (N = 3, W\n= 3, R = 1). That would mean that your writes are slow, since they have to contact all three nodes,\nand you would not be able to tolerate losing a node. But in some circumstances that may be the\ntradeoff to make.")]),e._v(" "),a("p",[e._v("The point to all of this is that you have a range of options to work with and can choose which\ncombination of problems and advantages to prefer. Some writers on NoSQL talk about a simple\ntradeoff between consistency and availability; we hope you now realize that it’s more flexible—and\nmore complicated—than that.")]),e._v(" "),a("h3",{attrs:{id:"_5-6-further-reading"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-6-further-reading"}},[e._v("#")]),e._v(" 5.6. Further Reading")]),e._v(" "),a("p",[e._v("There are all sorts of interesting blog posts and papers on the Internet about consistency in distributed\nsystems, but the most helpful source for us was [Tanenbaum and Van Steen]. It does an excellent job\nof organizing much of the fundamentals of distributed systems and is the best place to go if you’d like\nto delve deeper than we have in this chapter.")]),e._v(" "),a("p",[e._v("As we were finishing this book, "),a("em",[e._v("IEEE Computer")]),e._v(" had a special issue [IEEE Computer Feb 2012]\non the growing influence of the CAP theorem, which is a helpful source of further clarification for this\ntopic.")]),e._v(" "),a("h3",{attrs:{id:"_5-7-key-points"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-7-key-points"}},[e._v("#")]),e._v(" 5.7. Key Points")]),e._v(" "),a("ul",[a("li",[e._v("Write-write conflicts occur when two clients try to write the same data at the same time. Read-    write conflicts occur when one client reads inconsistent data in the middle of another client’s     write.")]),e._v(" "),a("li",[e._v("Pessimistic approaches lock data records to prevent conflicts. Optimistic approaches detect     conflicts and fix them.")]),e._v(" "),a("li",[e._v("Distributed systems see read-write conflicts due to some nodes having received updates while     other nodes have not. Eventual consistency means that at some point the system will become    consistent once all the writes have propagated to all the nodes.")]),e._v(" "),a("li",[e._v("Clients usually want read-your-writes consistency, which means a client can write and then     immediately read the new value. This can be difficult if the read and the write happen on different nodes.")]),e._v(" "),a("li",[e._v("To get good consistency, you need to involve many nodes in data operations, but this increases     latency. So you often have to trade off consistency versus latency.")]),e._v(" "),a("li",[e._v("The CAP theorem states that if you get a network partition, you have to trade off availability of     data versus consistency.")]),e._v(" "),a("li",[e._v("Durability can also be traded off against latency, particularly if you want to survive failures with replicated data.")]),e._v(" "),a("li",[e._v("You do not need to contact all replicants to preserve strong consistency with replication; you just need a large enough quorum.")])])])}),[],!1,null,null,null);t.default=o.exports}}]);