<!DOCTYPE html>
<html lang="vi">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Design Google File Storage | Nguyễn Khánk&#39;s Wiki</title>
    <meta name="generator" content="VuePress 1.7.1">
    <link rel="apple-touch-icon" sizes="180x180" href="../public/favicons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../public/favicons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../public/favicons/favicon-16x16.png">
    <link rel="manifest" href="../public/favicons/manifest.json">
    <link rel="mask-icon" href="../public/favicons/safari-pinned-tab.svg" color="#3a0839">
    <link rel="shortcut icon" href="../public/favicons/favicon.ico">
    <meta name="description" content="Wikipedia của tui">
    <meta name="msapplication-TileColor" content="#3a0839">
    <meta name="msapplication-config" content="../public/favicons/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">
    
    <link rel="preload" href="/docs/assets/css/0.styles.6906ca04.css" as="style"><link rel="preload" href="/docs/assets/js/app.8b37fff2.js" as="script"><link rel="preload" href="/docs/assets/js/2.d4412a2d.js" as="script"><link rel="preload" href="/docs/assets/js/54.9280baf7.js" as="script"><link rel="preload" href="/docs/assets/js/78.542f7f90.js" as="script"><link rel="prefetch" href="/docs/assets/js/10.4d183542.js"><link rel="prefetch" href="/docs/assets/js/100.0110052e.js"><link rel="prefetch" href="/docs/assets/js/101.b1a6b1ab.js"><link rel="prefetch" href="/docs/assets/js/102.f30cd881.js"><link rel="prefetch" href="/docs/assets/js/103.1362a24f.js"><link rel="prefetch" href="/docs/assets/js/104.c48b0d06.js"><link rel="prefetch" href="/docs/assets/js/105.bfc40953.js"><link rel="prefetch" href="/docs/assets/js/106.dc8405a6.js"><link rel="prefetch" href="/docs/assets/js/107.802ed6a8.js"><link rel="prefetch" href="/docs/assets/js/108.094de7e6.js"><link rel="prefetch" href="/docs/assets/js/109.8675aed9.js"><link rel="prefetch" href="/docs/assets/js/11.91e9c0b7.js"><link rel="prefetch" href="/docs/assets/js/110.82be6cb3.js"><link rel="prefetch" href="/docs/assets/js/111.554734f9.js"><link rel="prefetch" href="/docs/assets/js/112.67b73248.js"><link rel="prefetch" href="/docs/assets/js/113.a29343c9.js"><link rel="prefetch" href="/docs/assets/js/114.ba0a1ea9.js"><link rel="prefetch" href="/docs/assets/js/115.a9207708.js"><link rel="prefetch" href="/docs/assets/js/116.8203d4b2.js"><link rel="prefetch" href="/docs/assets/js/117.1abdda74.js"><link rel="prefetch" href="/docs/assets/js/118.60254762.js"><link rel="prefetch" href="/docs/assets/js/119.ab9c9d05.js"><link rel="prefetch" href="/docs/assets/js/12.b6232d32.js"><link rel="prefetch" href="/docs/assets/js/120.0a508574.js"><link rel="prefetch" href="/docs/assets/js/121.c89f90d0.js"><link rel="prefetch" href="/docs/assets/js/122.1b8cb6e8.js"><link rel="prefetch" href="/docs/assets/js/123.b7520280.js"><link rel="prefetch" href="/docs/assets/js/124.89354f6a.js"><link rel="prefetch" href="/docs/assets/js/125.e06c7296.js"><link rel="prefetch" href="/docs/assets/js/126.63184c1c.js"><link rel="prefetch" href="/docs/assets/js/127.10ac87c5.js"><link rel="prefetch" href="/docs/assets/js/128.6b5532e1.js"><link rel="prefetch" href="/docs/assets/js/129.2893c57d.js"><link rel="prefetch" href="/docs/assets/js/13.6c555615.js"><link rel="prefetch" href="/docs/assets/js/130.27831408.js"><link rel="prefetch" href="/docs/assets/js/131.421b117c.js"><link rel="prefetch" href="/docs/assets/js/132.b5fa1378.js"><link rel="prefetch" href="/docs/assets/js/133.97b9cf3c.js"><link rel="prefetch" href="/docs/assets/js/134.5780283b.js"><link rel="prefetch" href="/docs/assets/js/135.dabbfaeb.js"><link rel="prefetch" href="/docs/assets/js/136.f73b5f6e.js"><link rel="prefetch" href="/docs/assets/js/137.db4bcf6a.js"><link rel="prefetch" href="/docs/assets/js/138.1e0f18dc.js"><link rel="prefetch" href="/docs/assets/js/139.6b7d6915.js"><link rel="prefetch" href="/docs/assets/js/14.edbacbc5.js"><link rel="prefetch" href="/docs/assets/js/140.bd84388e.js"><link rel="prefetch" href="/docs/assets/js/141.dd9fca5a.js"><link rel="prefetch" href="/docs/assets/js/142.e6281dee.js"><link rel="prefetch" href="/docs/assets/js/143.fb5efc77.js"><link rel="prefetch" href="/docs/assets/js/144.3d74a0ca.js"><link rel="prefetch" href="/docs/assets/js/145.b5b3bc0e.js"><link rel="prefetch" href="/docs/assets/js/146.71a5e952.js"><link rel="prefetch" href="/docs/assets/js/147.ea8e0784.js"><link rel="prefetch" href="/docs/assets/js/148.531e2535.js"><link rel="prefetch" href="/docs/assets/js/149.8fdf50e4.js"><link rel="prefetch" href="/docs/assets/js/15.ee32eba9.js"><link rel="prefetch" href="/docs/assets/js/150.ab4101eb.js"><link rel="prefetch" href="/docs/assets/js/151.78ce1a7a.js"><link rel="prefetch" href="/docs/assets/js/152.62f31d99.js"><link rel="prefetch" href="/docs/assets/js/153.2788376a.js"><link rel="prefetch" href="/docs/assets/js/154.cdbb4537.js"><link rel="prefetch" href="/docs/assets/js/155.aea3e499.js"><link rel="prefetch" href="/docs/assets/js/156.b321c0d5.js"><link rel="prefetch" href="/docs/assets/js/157.4a590501.js"><link rel="prefetch" href="/docs/assets/js/158.4b5d0484.js"><link rel="prefetch" href="/docs/assets/js/159.d360524f.js"><link rel="prefetch" href="/docs/assets/js/16.641ea76e.js"><link rel="prefetch" href="/docs/assets/js/160.c69966c2.js"><link rel="prefetch" href="/docs/assets/js/161.87771612.js"><link rel="prefetch" href="/docs/assets/js/162.26ee95b5.js"><link rel="prefetch" href="/docs/assets/js/163.efaed824.js"><link rel="prefetch" href="/docs/assets/js/164.4103622c.js"><link rel="prefetch" href="/docs/assets/js/165.12656159.js"><link rel="prefetch" href="/docs/assets/js/166.cf86d1b4.js"><link rel="prefetch" href="/docs/assets/js/167.600a92ec.js"><link rel="prefetch" href="/docs/assets/js/168.2d0b2b5c.js"><link rel="prefetch" href="/docs/assets/js/169.30b08f88.js"><link rel="prefetch" href="/docs/assets/js/17.2deb9ed1.js"><link rel="prefetch" href="/docs/assets/js/170.6672fa36.js"><link rel="prefetch" href="/docs/assets/js/171.a7187d8c.js"><link rel="prefetch" href="/docs/assets/js/172.4cd0c679.js"><link rel="prefetch" href="/docs/assets/js/173.a657c159.js"><link rel="prefetch" href="/docs/assets/js/174.a01f7705.js"><link rel="prefetch" href="/docs/assets/js/175.bb015d2a.js"><link rel="prefetch" href="/docs/assets/js/176.280ab919.js"><link rel="prefetch" href="/docs/assets/js/177.b9c29e27.js"><link rel="prefetch" href="/docs/assets/js/178.ffb8b38d.js"><link rel="prefetch" href="/docs/assets/js/179.74261e61.js"><link rel="prefetch" href="/docs/assets/js/18.852fa2f3.js"><link rel="prefetch" href="/docs/assets/js/180.a6f5b899.js"><link rel="prefetch" href="/docs/assets/js/181.69b841be.js"><link rel="prefetch" href="/docs/assets/js/182.ef2afaf6.js"><link rel="prefetch" href="/docs/assets/js/183.66d1a07c.js"><link rel="prefetch" href="/docs/assets/js/184.131d23b3.js"><link rel="prefetch" href="/docs/assets/js/185.b96f968a.js"><link rel="prefetch" href="/docs/assets/js/186.6f5417ad.js"><link rel="prefetch" href="/docs/assets/js/187.5751f4b6.js"><link rel="prefetch" href="/docs/assets/js/188.0a48168a.js"><link rel="prefetch" href="/docs/assets/js/189.521b5a3f.js"><link rel="prefetch" href="/docs/assets/js/19.07b39eed.js"><link rel="prefetch" href="/docs/assets/js/190.0e82fcce.js"><link rel="prefetch" href="/docs/assets/js/191.8370b01d.js"><link rel="prefetch" href="/docs/assets/js/192.ca8cc11d.js"><link rel="prefetch" href="/docs/assets/js/193.fa7841dc.js"><link rel="prefetch" href="/docs/assets/js/194.3d7b2485.js"><link rel="prefetch" href="/docs/assets/js/195.27ef3123.js"><link rel="prefetch" href="/docs/assets/js/196.d504e577.js"><link rel="prefetch" href="/docs/assets/js/197.5cb06c0d.js"><link rel="prefetch" href="/docs/assets/js/198.dd1503c5.js"><link rel="prefetch" href="/docs/assets/js/199.ddbbef37.js"><link rel="prefetch" href="/docs/assets/js/20.771d8f4a.js"><link rel="prefetch" href="/docs/assets/js/200.8b34fb56.js"><link rel="prefetch" href="/docs/assets/js/201.fb4f45b3.js"><link rel="prefetch" href="/docs/assets/js/202.c5d395af.js"><link rel="prefetch" href="/docs/assets/js/203.5811a165.js"><link rel="prefetch" href="/docs/assets/js/204.307ac3fc.js"><link rel="prefetch" href="/docs/assets/js/205.56fcf979.js"><link rel="prefetch" href="/docs/assets/js/206.17aad8c7.js"><link rel="prefetch" href="/docs/assets/js/207.a1c18e37.js"><link rel="prefetch" href="/docs/assets/js/208.26c36a56.js"><link rel="prefetch" href="/docs/assets/js/209.cf442c71.js"><link rel="prefetch" href="/docs/assets/js/21.e422cc33.js"><link rel="prefetch" href="/docs/assets/js/210.78495eee.js"><link rel="prefetch" href="/docs/assets/js/211.69eb6872.js"><link rel="prefetch" href="/docs/assets/js/212.484c227c.js"><link rel="prefetch" href="/docs/assets/js/213.5482b491.js"><link rel="prefetch" href="/docs/assets/js/214.b99ba601.js"><link rel="prefetch" href="/docs/assets/js/215.2b606580.js"><link rel="prefetch" href="/docs/assets/js/216.aac18d8c.js"><link rel="prefetch" href="/docs/assets/js/217.324d1c96.js"><link rel="prefetch" href="/docs/assets/js/218.5256d814.js"><link rel="prefetch" href="/docs/assets/js/219.20b8bc42.js"><link rel="prefetch" href="/docs/assets/js/22.43a7b01f.js"><link rel="prefetch" href="/docs/assets/js/220.7044127b.js"><link rel="prefetch" href="/docs/assets/js/221.6ef43d12.js"><link rel="prefetch" href="/docs/assets/js/222.da392b9b.js"><link rel="prefetch" href="/docs/assets/js/223.b3253b8a.js"><link rel="prefetch" href="/docs/assets/js/224.9095bd20.js"><link rel="prefetch" href="/docs/assets/js/225.a6d16189.js"><link rel="prefetch" href="/docs/assets/js/226.d28374e9.js"><link rel="prefetch" href="/docs/assets/js/227.2dbbaecc.js"><link rel="prefetch" href="/docs/assets/js/228.72b78474.js"><link rel="prefetch" href="/docs/assets/js/229.6f4c91d0.js"><link rel="prefetch" href="/docs/assets/js/23.d8ec463a.js"><link rel="prefetch" href="/docs/assets/js/230.adfdb599.js"><link rel="prefetch" href="/docs/assets/js/231.25ea524c.js"><link rel="prefetch" href="/docs/assets/js/232.5e97fe3b.js"><link rel="prefetch" href="/docs/assets/js/233.76a766c2.js"><link rel="prefetch" href="/docs/assets/js/234.31389d90.js"><link rel="prefetch" href="/docs/assets/js/235.def39886.js"><link rel="prefetch" href="/docs/assets/js/236.a7eddaa5.js"><link rel="prefetch" href="/docs/assets/js/237.5a2d2822.js"><link rel="prefetch" href="/docs/assets/js/238.30477af1.js"><link rel="prefetch" href="/docs/assets/js/239.954e0b37.js"><link rel="prefetch" href="/docs/assets/js/24.fd896037.js"><link rel="prefetch" href="/docs/assets/js/240.596d23cb.js"><link rel="prefetch" href="/docs/assets/js/241.67c60edb.js"><link rel="prefetch" href="/docs/assets/js/242.0e9ae9a1.js"><link rel="prefetch" href="/docs/assets/js/243.f846b3b5.js"><link rel="prefetch" href="/docs/assets/js/244.821c42e9.js"><link rel="prefetch" href="/docs/assets/js/245.8eeb54ed.js"><link rel="prefetch" href="/docs/assets/js/246.1a991b50.js"><link rel="prefetch" href="/docs/assets/js/247.859a0bb2.js"><link rel="prefetch" href="/docs/assets/js/248.aee77a6c.js"><link rel="prefetch" href="/docs/assets/js/249.f4e2e7d8.js"><link rel="prefetch" href="/docs/assets/js/25.61b948cf.js"><link rel="prefetch" href="/docs/assets/js/250.9869f12b.js"><link rel="prefetch" href="/docs/assets/js/251.242f9848.js"><link rel="prefetch" href="/docs/assets/js/252.0929b9f6.js"><link rel="prefetch" href="/docs/assets/js/253.20281dd9.js"><link rel="prefetch" href="/docs/assets/js/254.a68f3f45.js"><link rel="prefetch" href="/docs/assets/js/255.ba50f565.js"><link rel="prefetch" href="/docs/assets/js/256.f6a54e76.js"><link rel="prefetch" href="/docs/assets/js/257.0a506ada.js"><link rel="prefetch" href="/docs/assets/js/258.f3b1d71a.js"><link rel="prefetch" href="/docs/assets/js/259.f87a2fb0.js"><link rel="prefetch" href="/docs/assets/js/26.a49ffea9.js"><link rel="prefetch" href="/docs/assets/js/260.44742b88.js"><link rel="prefetch" href="/docs/assets/js/261.aa0c91c6.js"><link rel="prefetch" href="/docs/assets/js/262.ab94fb05.js"><link rel="prefetch" href="/docs/assets/js/263.38c4b2e8.js"><link rel="prefetch" href="/docs/assets/js/264.e5d4a94f.js"><link rel="prefetch" href="/docs/assets/js/265.10909648.js"><link rel="prefetch" href="/docs/assets/js/266.779335a0.js"><link rel="prefetch" href="/docs/assets/js/267.efb3a4eb.js"><link rel="prefetch" href="/docs/assets/js/268.5f724152.js"><link rel="prefetch" href="/docs/assets/js/269.00f2acd0.js"><link rel="prefetch" href="/docs/assets/js/27.61988419.js"><link rel="prefetch" href="/docs/assets/js/270.c0fe818d.js"><link rel="prefetch" href="/docs/assets/js/271.a7bf9987.js"><link rel="prefetch" href="/docs/assets/js/272.b7a0ea5e.js"><link rel="prefetch" href="/docs/assets/js/273.cf862889.js"><link rel="prefetch" href="/docs/assets/js/274.8fa548e4.js"><link rel="prefetch" href="/docs/assets/js/275.0718df7a.js"><link rel="prefetch" href="/docs/assets/js/276.7c797ae9.js"><link rel="prefetch" href="/docs/assets/js/277.5b31b574.js"><link rel="prefetch" href="/docs/assets/js/278.2fc07320.js"><link rel="prefetch" href="/docs/assets/js/279.12f03252.js"><link rel="prefetch" href="/docs/assets/js/28.98a3ae7a.js"><link rel="prefetch" href="/docs/assets/js/280.76a2d991.js"><link rel="prefetch" href="/docs/assets/js/281.b5a60498.js"><link rel="prefetch" href="/docs/assets/js/282.9d634081.js"><link rel="prefetch" href="/docs/assets/js/283.aef1eee0.js"><link rel="prefetch" href="/docs/assets/js/284.8dd39af8.js"><link rel="prefetch" href="/docs/assets/js/285.74fdbf1f.js"><link rel="prefetch" href="/docs/assets/js/286.764c5e11.js"><link rel="prefetch" href="/docs/assets/js/287.a9cc792f.js"><link rel="prefetch" href="/docs/assets/js/288.bf4cd87e.js"><link rel="prefetch" href="/docs/assets/js/289.f11f00a3.js"><link rel="prefetch" href="/docs/assets/js/29.5ade7e16.js"><link rel="prefetch" href="/docs/assets/js/290.d060a3b4.js"><link rel="prefetch" href="/docs/assets/js/291.e8b4b8b4.js"><link rel="prefetch" href="/docs/assets/js/292.32feeeb9.js"><link rel="prefetch" href="/docs/assets/js/293.e6ab8984.js"><link rel="prefetch" href="/docs/assets/js/294.7bade475.js"><link rel="prefetch" href="/docs/assets/js/295.68f2b7a4.js"><link rel="prefetch" href="/docs/assets/js/296.bf3ef227.js"><link rel="prefetch" href="/docs/assets/js/297.619604b4.js"><link rel="prefetch" href="/docs/assets/js/298.37e3b095.js"><link rel="prefetch" href="/docs/assets/js/299.019349d1.js"><link rel="prefetch" href="/docs/assets/js/3.36c38af0.js"><link rel="prefetch" href="/docs/assets/js/30.bfd973b7.js"><link rel="prefetch" href="/docs/assets/js/300.6e5e6062.js"><link rel="prefetch" href="/docs/assets/js/301.73a9bcb8.js"><link rel="prefetch" href="/docs/assets/js/302.81afcd46.js"><link rel="prefetch" href="/docs/assets/js/303.61851e0f.js"><link rel="prefetch" href="/docs/assets/js/304.53eb1249.js"><link rel="prefetch" href="/docs/assets/js/305.e3edf110.js"><link rel="prefetch" href="/docs/assets/js/306.df0cabbb.js"><link rel="prefetch" href="/docs/assets/js/307.09e24b2d.js"><link rel="prefetch" href="/docs/assets/js/308.64d723a4.js"><link rel="prefetch" href="/docs/assets/js/309.cff8533d.js"><link rel="prefetch" href="/docs/assets/js/31.7fdbbe7c.js"><link rel="prefetch" href="/docs/assets/js/310.8c79f60f.js"><link rel="prefetch" href="/docs/assets/js/311.f334b488.js"><link rel="prefetch" href="/docs/assets/js/312.1bc7565c.js"><link rel="prefetch" href="/docs/assets/js/313.1e981392.js"><link rel="prefetch" href="/docs/assets/js/314.bffe5123.js"><link rel="prefetch" href="/docs/assets/js/315.eb3bb611.js"><link rel="prefetch" href="/docs/assets/js/316.5d6a1da6.js"><link rel="prefetch" href="/docs/assets/js/317.9fc0f693.js"><link rel="prefetch" href="/docs/assets/js/318.2bc57f12.js"><link rel="prefetch" href="/docs/assets/js/319.cdb1c280.js"><link rel="prefetch" href="/docs/assets/js/32.588c4613.js"><link rel="prefetch" href="/docs/assets/js/320.ca9baf28.js"><link rel="prefetch" href="/docs/assets/js/321.f189d62b.js"><link rel="prefetch" href="/docs/assets/js/322.11248ca3.js"><link rel="prefetch" href="/docs/assets/js/323.6e0fd789.js"><link rel="prefetch" href="/docs/assets/js/324.030c98e3.js"><link rel="prefetch" href="/docs/assets/js/325.302ab8f2.js"><link rel="prefetch" href="/docs/assets/js/326.32647c57.js"><link rel="prefetch" href="/docs/assets/js/327.4b26dbb7.js"><link rel="prefetch" href="/docs/assets/js/328.bc431a59.js"><link rel="prefetch" href="/docs/assets/js/329.375ffbc8.js"><link rel="prefetch" href="/docs/assets/js/33.baad5d0b.js"><link rel="prefetch" href="/docs/assets/js/330.feeadd23.js"><link rel="prefetch" href="/docs/assets/js/331.5d357b70.js"><link rel="prefetch" href="/docs/assets/js/332.f084e171.js"><link rel="prefetch" href="/docs/assets/js/333.24c84c54.js"><link rel="prefetch" href="/docs/assets/js/334.817cd07e.js"><link rel="prefetch" href="/docs/assets/js/335.74b4dee9.js"><link rel="prefetch" href="/docs/assets/js/336.19e8e41e.js"><link rel="prefetch" href="/docs/assets/js/337.39567060.js"><link rel="prefetch" href="/docs/assets/js/338.a3f09726.js"><link rel="prefetch" href="/docs/assets/js/339.0d33562c.js"><link rel="prefetch" href="/docs/assets/js/34.eb39292c.js"><link rel="prefetch" href="/docs/assets/js/340.9e6f8748.js"><link rel="prefetch" href="/docs/assets/js/341.a34fa2e2.js"><link rel="prefetch" href="/docs/assets/js/342.279b9fdc.js"><link rel="prefetch" href="/docs/assets/js/343.73441cfb.js"><link rel="prefetch" href="/docs/assets/js/344.821a90ed.js"><link rel="prefetch" href="/docs/assets/js/345.3d751c82.js"><link rel="prefetch" href="/docs/assets/js/346.ae127724.js"><link rel="prefetch" href="/docs/assets/js/347.4d39bd6f.js"><link rel="prefetch" href="/docs/assets/js/348.a95a1ffa.js"><link rel="prefetch" href="/docs/assets/js/349.1e1624bd.js"><link rel="prefetch" href="/docs/assets/js/35.7d21eac6.js"><link rel="prefetch" href="/docs/assets/js/350.71367b0f.js"><link rel="prefetch" href="/docs/assets/js/351.293e165b.js"><link rel="prefetch" href="/docs/assets/js/352.99975689.js"><link rel="prefetch" href="/docs/assets/js/353.65a34f67.js"><link rel="prefetch" href="/docs/assets/js/354.c1d3f452.js"><link rel="prefetch" href="/docs/assets/js/355.ed31c658.js"><link rel="prefetch" href="/docs/assets/js/356.c8f79715.js"><link rel="prefetch" href="/docs/assets/js/357.abfe4335.js"><link rel="prefetch" href="/docs/assets/js/358.4a200fbc.js"><link rel="prefetch" href="/docs/assets/js/359.7542b756.js"><link rel="prefetch" href="/docs/assets/js/36.67c15a1f.js"><link rel="prefetch" href="/docs/assets/js/360.e980bde0.js"><link rel="prefetch" href="/docs/assets/js/361.8deeb2a3.js"><link rel="prefetch" href="/docs/assets/js/362.391151f5.js"><link rel="prefetch" href="/docs/assets/js/363.943b1871.js"><link rel="prefetch" href="/docs/assets/js/364.b4cab09e.js"><link rel="prefetch" href="/docs/assets/js/365.9247cf79.js"><link rel="prefetch" href="/docs/assets/js/366.d821ebc5.js"><link rel="prefetch" href="/docs/assets/js/367.f528a295.js"><link rel="prefetch" href="/docs/assets/js/368.858d38f6.js"><link rel="prefetch" href="/docs/assets/js/369.9cb0ff53.js"><link rel="prefetch" href="/docs/assets/js/37.54da6b52.js"><link rel="prefetch" href="/docs/assets/js/370.4eec32c2.js"><link rel="prefetch" href="/docs/assets/js/371.dc015969.js"><link rel="prefetch" href="/docs/assets/js/372.7d07b409.js"><link rel="prefetch" href="/docs/assets/js/373.a0b85cdc.js"><link rel="prefetch" href="/docs/assets/js/374.15952e23.js"><link rel="prefetch" href="/docs/assets/js/375.cb60be95.js"><link rel="prefetch" href="/docs/assets/js/376.8b34693c.js"><link rel="prefetch" href="/docs/assets/js/377.fd23311b.js"><link rel="prefetch" href="/docs/assets/js/378.f299d4a6.js"><link rel="prefetch" href="/docs/assets/js/379.3d85263c.js"><link rel="prefetch" href="/docs/assets/js/38.75056fa7.js"><link rel="prefetch" href="/docs/assets/js/380.5c09189b.js"><link rel="prefetch" href="/docs/assets/js/381.11f89ae3.js"><link rel="prefetch" href="/docs/assets/js/382.31590b63.js"><link rel="prefetch" href="/docs/assets/js/383.9ef6813a.js"><link rel="prefetch" href="/docs/assets/js/384.0dd5d78d.js"><link rel="prefetch" href="/docs/assets/js/385.63fa7625.js"><link rel="prefetch" href="/docs/assets/js/386.c1c742b7.js"><link rel="prefetch" href="/docs/assets/js/387.f68c63ff.js"><link rel="prefetch" href="/docs/assets/js/388.8117fd27.js"><link rel="prefetch" href="/docs/assets/js/389.11026756.js"><link rel="prefetch" href="/docs/assets/js/39.fda45605.js"><link rel="prefetch" href="/docs/assets/js/390.6f11af62.js"><link rel="prefetch" href="/docs/assets/js/391.29f729b9.js"><link rel="prefetch" href="/docs/assets/js/392.701e8f74.js"><link rel="prefetch" href="/docs/assets/js/393.9ff46998.js"><link rel="prefetch" href="/docs/assets/js/394.9675ed5a.js"><link rel="prefetch" href="/docs/assets/js/395.9da7a4e9.js"><link rel="prefetch" href="/docs/assets/js/396.464b85d8.js"><link rel="prefetch" href="/docs/assets/js/397.abc292eb.js"><link rel="prefetch" href="/docs/assets/js/398.f7b4c1c2.js"><link rel="prefetch" href="/docs/assets/js/399.e838c72a.js"><link rel="prefetch" href="/docs/assets/js/4.a9cf95b2.js"><link rel="prefetch" href="/docs/assets/js/40.5bb360a7.js"><link rel="prefetch" href="/docs/assets/js/400.a1c7c3e6.js"><link rel="prefetch" href="/docs/assets/js/401.cb2bc9f0.js"><link rel="prefetch" href="/docs/assets/js/402.faf3bb42.js"><link rel="prefetch" href="/docs/assets/js/403.eea434c5.js"><link rel="prefetch" href="/docs/assets/js/404.fb52b229.js"><link rel="prefetch" href="/docs/assets/js/405.6ade75eb.js"><link rel="prefetch" href="/docs/assets/js/406.3690c68a.js"><link rel="prefetch" href="/docs/assets/js/407.8d48d927.js"><link rel="prefetch" href="/docs/assets/js/408.b85129c2.js"><link rel="prefetch" href="/docs/assets/js/409.98b2f46d.js"><link rel="prefetch" href="/docs/assets/js/41.2685bab2.js"><link rel="prefetch" href="/docs/assets/js/410.9feebfe2.js"><link rel="prefetch" href="/docs/assets/js/411.201746f2.js"><link rel="prefetch" href="/docs/assets/js/412.90090b5b.js"><link rel="prefetch" href="/docs/assets/js/413.557ec882.js"><link rel="prefetch" href="/docs/assets/js/414.753cf8b7.js"><link rel="prefetch" href="/docs/assets/js/415.65c79d8e.js"><link rel="prefetch" href="/docs/assets/js/416.33f567d0.js"><link rel="prefetch" href="/docs/assets/js/417.4bc44c4e.js"><link rel="prefetch" href="/docs/assets/js/418.e133815e.js"><link rel="prefetch" href="/docs/assets/js/419.f46fd217.js"><link rel="prefetch" href="/docs/assets/js/42.433f3d02.js"><link rel="prefetch" href="/docs/assets/js/420.7fc8e783.js"><link rel="prefetch" href="/docs/assets/js/421.0224d1ab.js"><link rel="prefetch" href="/docs/assets/js/422.b2b5a409.js"><link rel="prefetch" href="/docs/assets/js/423.3537ec38.js"><link rel="prefetch" href="/docs/assets/js/424.f481fe66.js"><link rel="prefetch" href="/docs/assets/js/425.0d51bc0f.js"><link rel="prefetch" href="/docs/assets/js/426.509e0cce.js"><link rel="prefetch" href="/docs/assets/js/427.a9c7170a.js"><link rel="prefetch" href="/docs/assets/js/428.a943b65b.js"><link rel="prefetch" href="/docs/assets/js/429.2fd28de6.js"><link rel="prefetch" href="/docs/assets/js/43.239d08c8.js"><link rel="prefetch" href="/docs/assets/js/430.1ae58eb9.js"><link rel="prefetch" href="/docs/assets/js/431.639e9b27.js"><link rel="prefetch" href="/docs/assets/js/432.64ba697f.js"><link rel="prefetch" href="/docs/assets/js/433.6b16d19d.js"><link rel="prefetch" href="/docs/assets/js/434.73c60b96.js"><link rel="prefetch" href="/docs/assets/js/435.58113e20.js"><link rel="prefetch" href="/docs/assets/js/436.648b42e6.js"><link rel="prefetch" href="/docs/assets/js/437.a167b929.js"><link rel="prefetch" href="/docs/assets/js/438.472814ce.js"><link rel="prefetch" href="/docs/assets/js/439.3875a13e.js"><link rel="prefetch" href="/docs/assets/js/44.221d05d2.js"><link rel="prefetch" href="/docs/assets/js/440.1652b850.js"><link rel="prefetch" href="/docs/assets/js/441.44a5cf43.js"><link rel="prefetch" href="/docs/assets/js/442.b66d3613.js"><link rel="prefetch" href="/docs/assets/js/443.d87244b9.js"><link rel="prefetch" href="/docs/assets/js/444.01b6250e.js"><link rel="prefetch" href="/docs/assets/js/445.41f9d053.js"><link rel="prefetch" href="/docs/assets/js/446.dadbd952.js"><link rel="prefetch" href="/docs/assets/js/447.9d3dba2f.js"><link rel="prefetch" href="/docs/assets/js/448.9862a673.js"><link rel="prefetch" href="/docs/assets/js/449.dce0e0f7.js"><link rel="prefetch" href="/docs/assets/js/45.180de453.js"><link rel="prefetch" href="/docs/assets/js/450.0bd0b9be.js"><link rel="prefetch" href="/docs/assets/js/451.ac57a6c7.js"><link rel="prefetch" href="/docs/assets/js/452.dd40becc.js"><link rel="prefetch" href="/docs/assets/js/453.1e8072bb.js"><link rel="prefetch" href="/docs/assets/js/454.5f688ea3.js"><link rel="prefetch" href="/docs/assets/js/455.f7e10c0e.js"><link rel="prefetch" href="/docs/assets/js/456.f1a9991e.js"><link rel="prefetch" href="/docs/assets/js/457.80c3d7cc.js"><link rel="prefetch" href="/docs/assets/js/458.d9c2d7a5.js"><link rel="prefetch" href="/docs/assets/js/459.569f84db.js"><link rel="prefetch" href="/docs/assets/js/46.c97dd31a.js"><link rel="prefetch" href="/docs/assets/js/460.6c2d9c1e.js"><link rel="prefetch" href="/docs/assets/js/461.425a5752.js"><link rel="prefetch" href="/docs/assets/js/462.f83a3250.js"><link rel="prefetch" href="/docs/assets/js/463.82baa8e4.js"><link rel="prefetch" href="/docs/assets/js/464.0366b057.js"><link rel="prefetch" href="/docs/assets/js/465.08534052.js"><link rel="prefetch" href="/docs/assets/js/466.5f118340.js"><link rel="prefetch" href="/docs/assets/js/467.a0df9b7b.js"><link rel="prefetch" href="/docs/assets/js/468.7151aa68.js"><link rel="prefetch" href="/docs/assets/js/469.66c75474.js"><link rel="prefetch" href="/docs/assets/js/47.734a2f3d.js"><link rel="prefetch" href="/docs/assets/js/470.5edc6211.js"><link rel="prefetch" href="/docs/assets/js/471.61a17bd6.js"><link rel="prefetch" href="/docs/assets/js/472.fd39c1ec.js"><link rel="prefetch" href="/docs/assets/js/473.fd8ab7c5.js"><link rel="prefetch" href="/docs/assets/js/474.8b48ce0d.js"><link rel="prefetch" href="/docs/assets/js/475.21cc9d30.js"><link rel="prefetch" href="/docs/assets/js/476.1fecbdb5.js"><link rel="prefetch" href="/docs/assets/js/477.d51ca82a.js"><link rel="prefetch" href="/docs/assets/js/478.b111b617.js"><link rel="prefetch" href="/docs/assets/js/479.fdc921fc.js"><link rel="prefetch" href="/docs/assets/js/48.24c46da8.js"><link rel="prefetch" href="/docs/assets/js/480.6dd0512b.js"><link rel="prefetch" href="/docs/assets/js/481.fb966228.js"><link rel="prefetch" href="/docs/assets/js/482.1fe9c33a.js"><link rel="prefetch" href="/docs/assets/js/483.1b4f9e50.js"><link rel="prefetch" href="/docs/assets/js/484.d2aa5578.js"><link rel="prefetch" href="/docs/assets/js/485.7931bdb8.js"><link rel="prefetch" href="/docs/assets/js/486.6bead944.js"><link rel="prefetch" href="/docs/assets/js/487.3e1009dc.js"><link rel="prefetch" href="/docs/assets/js/488.e60d9e21.js"><link rel="prefetch" href="/docs/assets/js/489.51de1ba8.js"><link rel="prefetch" href="/docs/assets/js/49.c32010f7.js"><link rel="prefetch" href="/docs/assets/js/490.8f052334.js"><link rel="prefetch" href="/docs/assets/js/491.6698360d.js"><link rel="prefetch" href="/docs/assets/js/492.0281f6c3.js"><link rel="prefetch" href="/docs/assets/js/493.107c310a.js"><link rel="prefetch" href="/docs/assets/js/494.bb6c109e.js"><link rel="prefetch" href="/docs/assets/js/495.00dd0a1d.js"><link rel="prefetch" href="/docs/assets/js/496.8eab9a5f.js"><link rel="prefetch" href="/docs/assets/js/497.65c700a5.js"><link rel="prefetch" href="/docs/assets/js/498.afeaccf8.js"><link rel="prefetch" href="/docs/assets/js/499.d06d8f5f.js"><link rel="prefetch" href="/docs/assets/js/5.6be623ca.js"><link rel="prefetch" href="/docs/assets/js/50.427f1e71.js"><link rel="prefetch" href="/docs/assets/js/500.fcec3ae8.js"><link rel="prefetch" href="/docs/assets/js/501.48f06a25.js"><link rel="prefetch" href="/docs/assets/js/502.4d6c4a16.js"><link rel="prefetch" href="/docs/assets/js/503.23c5d510.js"><link rel="prefetch" href="/docs/assets/js/504.15637d49.js"><link rel="prefetch" href="/docs/assets/js/505.ee29cc3d.js"><link rel="prefetch" href="/docs/assets/js/506.e53109ef.js"><link rel="prefetch" href="/docs/assets/js/507.8f4d0b7e.js"><link rel="prefetch" href="/docs/assets/js/508.e666a58e.js"><link rel="prefetch" href="/docs/assets/js/509.ddb38f4d.js"><link rel="prefetch" href="/docs/assets/js/51.e672177c.js"><link rel="prefetch" href="/docs/assets/js/510.9187eeeb.js"><link rel="prefetch" href="/docs/assets/js/511.daa5daa4.js"><link rel="prefetch" href="/docs/assets/js/512.0904731e.js"><link rel="prefetch" href="/docs/assets/js/513.52b02494.js"><link rel="prefetch" href="/docs/assets/js/514.dd22c2b9.js"><link rel="prefetch" href="/docs/assets/js/515.4f4acfaa.js"><link rel="prefetch" href="/docs/assets/js/52.ed3fb72d.js"><link rel="prefetch" href="/docs/assets/js/53.99ec48d3.js"><link rel="prefetch" href="/docs/assets/js/55.56c79497.js"><link rel="prefetch" href="/docs/assets/js/56.ff6b4405.js"><link rel="prefetch" href="/docs/assets/js/57.3ea7e627.js"><link rel="prefetch" href="/docs/assets/js/58.b117b74e.js"><link rel="prefetch" href="/docs/assets/js/59.7ab02f07.js"><link rel="prefetch" href="/docs/assets/js/6.3871f457.js"><link rel="prefetch" href="/docs/assets/js/60.d95b33de.js"><link rel="prefetch" href="/docs/assets/js/61.f2b9b5bc.js"><link rel="prefetch" href="/docs/assets/js/62.c2d965f7.js"><link rel="prefetch" href="/docs/assets/js/63.895fc836.js"><link rel="prefetch" href="/docs/assets/js/64.f513c142.js"><link rel="prefetch" href="/docs/assets/js/65.c6c0a395.js"><link rel="prefetch" href="/docs/assets/js/66.7d082b2e.js"><link rel="prefetch" href="/docs/assets/js/67.adbd255d.js"><link rel="prefetch" href="/docs/assets/js/68.bee59031.js"><link rel="prefetch" href="/docs/assets/js/69.54fd9e04.js"><link rel="prefetch" href="/docs/assets/js/7.adb1cc8d.js"><link rel="prefetch" href="/docs/assets/js/70.01fa7ffa.js"><link rel="prefetch" href="/docs/assets/js/71.b3f28afb.js"><link rel="prefetch" href="/docs/assets/js/72.7ef980b4.js"><link rel="prefetch" href="/docs/assets/js/73.8bd1d412.js"><link rel="prefetch" href="/docs/assets/js/74.f74c8bd0.js"><link rel="prefetch" href="/docs/assets/js/75.ee2d792a.js"><link rel="prefetch" href="/docs/assets/js/76.56ba520e.js"><link rel="prefetch" href="/docs/assets/js/77.192260cf.js"><link rel="prefetch" href="/docs/assets/js/79.e091da28.js"><link rel="prefetch" href="/docs/assets/js/8.0c3dd2d9.js"><link rel="prefetch" href="/docs/assets/js/80.3be244af.js"><link rel="prefetch" href="/docs/assets/js/81.3b53e455.js"><link rel="prefetch" href="/docs/assets/js/82.59872882.js"><link rel="prefetch" href="/docs/assets/js/83.6ce62d88.js"><link rel="prefetch" href="/docs/assets/js/84.51612d59.js"><link rel="prefetch" href="/docs/assets/js/85.e8a415cc.js"><link rel="prefetch" href="/docs/assets/js/86.a0b57c63.js"><link rel="prefetch" href="/docs/assets/js/87.3f0f61e3.js"><link rel="prefetch" href="/docs/assets/js/88.5481b182.js"><link rel="prefetch" href="/docs/assets/js/89.176a8d3e.js"><link rel="prefetch" href="/docs/assets/js/9.04d9ce93.js"><link rel="prefetch" href="/docs/assets/js/90.e7dd4516.js"><link rel="prefetch" href="/docs/assets/js/91.5b3f0ff4.js"><link rel="prefetch" href="/docs/assets/js/92.01768c97.js"><link rel="prefetch" href="/docs/assets/js/93.92f2f011.js"><link rel="prefetch" href="/docs/assets/js/94.d1f4fa66.js"><link rel="prefetch" href="/docs/assets/js/95.49a29444.js"><link rel="prefetch" href="/docs/assets/js/96.253dded0.js"><link rel="prefetch" href="/docs/assets/js/97.734c0000.js"><link rel="prefetch" href="/docs/assets/js/98.2e3c642e.js"><link rel="prefetch" href="/docs/assets/js/99.96343fd6.js">
    <link rel="stylesheet" href="/docs/assets/css/0.styles.6906ca04.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/docs/" class="home-link router-link-active"><!----> <span class="site-name">Nguyễn Khánk's Wiki</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/docs/architect/" class="nav-link">
  Architecture
</a></div><div class="nav-item"><a href="https://nguyenngockhank.github.io" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Home
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/docs/architect/" class="nav-link">
  Architecture
</a></div><div class="nav-item"><a href="https://nguyenngockhank.github.io" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Home
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>System Design Case Studies</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/docs/kungfu/case-study/notification/" class="sidebar-link">Design a notification system</a></li><li><a href="/docs/kungfu/case-study/metrics/" class="sidebar-link">Metrics Monitoring &amp; Alerting System</a></li><li><a href="/docs/kungfu/case-study/ad-click/" class="sidebar-link">Ad Click Event Aggregation</a></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading open"><span>Data storage</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/docs/kungfu/case-study/dynamo/" class="sidebar-link">Design Dynamo</a></li><li><a href="/docs/kungfu/case-study/bigtable/" class="sidebar-link">Design BigTable</a></li><li><a href="/docs/kungfu/case-study/s3/" class="sidebar-link">S3-like object storage</a></li><li><a href="/docs/kungfu/case-study/gfs/" aria-current="page" class="active sidebar-link">Design Google File Storage</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/docs/kungfu/case-study/gfs/#overview" class="sidebar-link">Overview</a></li><li class="sidebar-sub-header"><a href="/docs/kungfu/case-study/gfs/#high-level-architecture" class="sidebar-link">High-level Architecture</a></li><li class="sidebar-sub-header"><a href="/docs/kungfu/case-study/gfs/#single-master-and-large-chunk-size" class="sidebar-link">Single Master and Large Chunk Size</a></li><li class="sidebar-sub-header"><a href="/docs/kungfu/case-study/gfs/#metadata" class="sidebar-link">Metadata</a></li><li class="sidebar-sub-header"><a href="/docs/kungfu/case-study/gfs/#master-operations" class="sidebar-link">Master Operations</a></li><li class="sidebar-sub-header"><a href="/docs/kungfu/case-study/gfs/#anatomy-of-a-read-operation" class="sidebar-link">Anatomy of a Read Operation</a></li><li class="sidebar-sub-header"><a href="/docs/kungfu/case-study/gfs/#anatomy-of-a-write-operation" class="sidebar-link">Anatomy of a Write Operation</a></li><li class="sidebar-sub-header"><a href="/docs/kungfu/case-study/gfs/#anatomy-of-an-append-operation" class="sidebar-link">Anatomy of an Append Operation</a></li><li class="sidebar-sub-header"><a href="/docs/kungfu/case-study/gfs/#gfs-consistency-model-and-snapshotting" class="sidebar-link">GFS Consistency Model and Snapshotting</a></li><li class="sidebar-sub-header"><a href="/docs/kungfu/case-study/gfs/#fault-tolerance-high-availability-and-data-integrity" class="sidebar-link">Fault Tolerance, High Availability, and Data Integrity</a></li><li class="sidebar-sub-header"><a href="/docs/kungfu/case-study/gfs/#garbage-collection" class="sidebar-link">Garbage Collection</a></li><li class="sidebar-sub-header"><a href="/docs/kungfu/case-study/gfs/#criticism-on-gfs" class="sidebar-link">Criticism on GFS</a></li><li class="sidebar-sub-header"><a href="/docs/kungfu/case-study/gfs/#summary" class="sidebar-link">Summary</a></li></ul></li><li><a href="/docs/kungfu/case-study/kafka/" class="sidebar-link">Design Kafka</a></li><li><a href="/docs/kungfu/case-study/dropbox/" class="sidebar-link">Design Dropbox</a></li><li><a href="/docs/kungfu/case-study/google-drive/" class="sidebar-link">Design Google Drive</a></li></ul></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>Online tools</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>Booking System</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>Social Network</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>Location-based</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>Video</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>Chat system</span> <span class="arrow right"></span></p> <!----></section></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>The Art of Readable Code</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>97 Things Every Programmer Should Know</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Go</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="design-google-file-storage"><a href="#design-google-file-storage" class="header-anchor">#</a> Design Google File Storage</h1> <h2 id="overview"><a href="#overview" class="header-anchor">#</a> Overview</h2> <h3 id="goal"><a href="#goal" class="header-anchor">#</a> Goal</h3> <p>Design a distributed file system to store huge files (terabyte and larger). The
system should be scalable, reliable, and highly available.</p> <h3 id="what-is-google-file-system-gfs"><a href="#what-is-google-file-system-gfs" class="header-anchor">#</a> What is Google File System (GFS)?</h3> <p>GFS is a scalable distributed file system developed by Google for its large
data-intensive applications.</p> <h3 id="background"><a href="#background" class="header-anchor">#</a> Background</h3> <p>GFS was built for handling batch processing on large data sets and is
designed for system-to-system interaction, not user-to-system interaction.</p> <p>Google built GFS keeping the following goals in mind:</p> <ul><li><strong>Scalable</strong>: GFS should run reliably on a very large system built from
commodity hardware.</li> <li><strong>Fault-tolerant</strong>: The design must be sufficiently tolerant of hardware
and software failures to enable application-level services to continue
their operation in the face of any likely combination of failure
conditions.</li> <li><strong>Large files</strong>: Files stored in GFS will be huge. Multi-GB files are common.
Large sequential and small random reads: The workloads primarily
consist of two kinds of reads: large, streaming reads and small, random
reads.</li> <li><strong>Sequential writes</strong>: The workloads also have many large, sequential
writes that append data to files. Typical operation sizes are similar to
those for reads. Once written, files are seldom modified again.</li> <li><strong>Not optimized for small data:</strong> Small, random reads and writes do
occur and are supported, but the system is not optimized for such cases.</li> <li><strong>Concurrent access</strong>: The level of concurrent access will also be high,
with large numbers of concurrent appends being particularly prevalent,
often accompanied by concurrent reads.</li> <li><strong>High throughput</strong>: GFS should be optimized for high and sustained
throughput in reading the data, and this is prioritized over latency. This
is not to say that latency is unimportant; rather, GFS needs to be
optimized for high-performance reading and appending large volumes
of data for the correct operation of the system.</li></ul> <h3 id="gfs-use-cases"><a href="#gfs-use-cases" class="header-anchor">#</a> GFS use cases</h3> <p>-GFS is a distributed file system built for large, distributed data-intensive
applications like <strong>Gmail</strong> or <strong>YouTube</strong>.</p> <ul><li>Originally, it was built to store data generated by Google’s large
<strong>crawling and indexing system</strong></li> <li>Google’s <strong>BigTable</strong> uses the distributed Google File System to store log
and data files.</li></ul> <h3 id="apis"><a href="#apis" class="header-anchor">#</a> APIs</h3> <p>GFS does not provide standard POSIX-like APIs; instead, user-level APIs are
provided. In GFS, files are organized hierarchically in directories and
identified by their pathnames. GFS supports the usual file system operations:</p> <ul><li><em>create</em> – To create a new instance of a file.</li> <li><em>delete</em> – To delete an instance of a file.</li> <li><em>open</em> – To open a named file and return a handle.</li> <li><em>close</em> – To close a given file specified by a handle.</li> <li><em>read</em> – To read data from a specified file and offset.</li> <li><em>write</em> – To write data to a specified file and offset.</li></ul> <p>In addition, GFS supports two special operations:</p> <ul><li><strong>Snapshot</strong>: A snapshot is an efficient way of creating a copy of the
current instance of a file or directory tree.</li> <li><strong>Append</strong>: An append operation allows multiple clients to append data to
the same file concurrently while guaranteeing atomicity. It is useful for
implementing multi-way merge results and producer-consumer queues
that many clients can simultaneously append to without additional
locking.</li></ul> <h2 id="high-level-architecture"><a href="#high-level-architecture" class="header-anchor">#</a> High-level Architecture</h2> <p>A GFS cluster consists of a single master and multiple ChunkServers and is
accessed by multiple clients.</p> <h3 id="chunks"><a href="#chunks" class="header-anchor">#</a> Chunks</h3> <p>As files stored in GFS tend to be very large, GFS breaks files into multiple
fixed-size chunks where each chunk is 64 megabytes in size.</p> <h3 id="chunk-handle"><a href="#chunk-handle" class="header-anchor">#</a> Chunk handle</h3> <p>Each chunk is identified by an immutable and globally unique 64-bit ID
number called chunk handle. This allows for 2<sup>64</sup> unique chunks. If each
chunk is 64 MB, total storage space would be more than 10<sup>9</sup> exa-bytes.</p> <p>As files are split into chunks, therefore, the job of GFS is to provide a
mapping from files to chunks, and then to support standard operations on
files, mapping down to operations on individual chunks.</p> <h3 id="cluster"><a href="#cluster" class="header-anchor">#</a> Cluster</h3> <p>GFS is organized into a simple network of computers called a cluster. All GFS
clusters contain three kinds of entities:</p> <ol><li>A single master server</li> <li>Multiple ChunkServers</li> <li>Many clients</li></ol> <p>The master stores all metadata about the system, while the ChunkServers
store the real file data.</p> <p><img src="/docs/assets/img/f1.5761a2e5.png" alt="Figure 1"><br> <em>GFS high-level architecture</em></p> <h3 id="chunkserver"><a href="#chunkserver" class="header-anchor">#</a> ChunkServer</h3> <p>ChunkServers store chunks on local disks as regular Linux files and read or
write chunk data specified by a chunk handle and byte-range.</p> <p>For reliability, each chunk is replicated to multiple ChunkServers. By default,
GFS stores three replicas, though different replication factors can be
specified on a per-file basis.</p> <p><img src="/docs/assets/img/f2.651ca88e.png" alt="Figure 2"><br> <em>Chunk replication</em></p> <h3 id="master"><a href="#master" class="header-anchor">#</a> Master</h3> <p>Master server is the coordinator of a GFS cluster and is responsible for
keeping track of filesystem metadata:</p> <ol><li>The metadata stored at the master includes:
<ul><li>Name and directory of each file</li> <li>Mapping of each file to its chunks</li> <li>Current locations of chunks</li> <li>Access control information</li></ul></li> <li>The master also controls system-wide activities such as chunk lease
management (locks on chunks with expiration), garbage collection of
orphaned chunks, and chunk migration between ChunkServers. Master
assigns chunk handle to chunks at time of chunk creation.</li> <li>The master periodically communicates with each ChunkServer in
HeartBeat messages to give it instructions and collect its state.</li> <li>For performance and fast random access, all metadata is stored in the
master’s main memory. This includes the entire filesystem namespace
as well as all the name-to-chunk mappings.</li> <li>For fault tolerance and to handle a master crash, all metadata changes
are written to the disk onto an operation log. This operation log is also
replicated onto remote machines. The operation log is similar to a
journal. Every operation to the file system is logged into this file.</li> <li>The master is a single point of failure, hence, it replicates its data onto
several remote machines so that the master can be readily restored on
failure.</li> <li>The benefit of having a single, centralized master is that it has a global
view of the file system, and hence, it can make optimum management
decisions, for example, related to chunk placement.</li></ol> <h3 id="client"><a href="#client" class="header-anchor">#</a> Client</h3> <p>Client is an entity that makes a read or write request to GSF. GFS client
library is linked into each application that uses GFS. This library
communicates with the master for all metadata-related operations like
creating or deleting files, looking up files, etc. To read or write data, the
client interacts directly with the ChunkServers that hold the data</p> <p>Neither the client nor the ChunkServer caches file data. Client caches offer
little benefit because most applications stream through huge files or have
working sets too large to be cached. ChunkServers rely on the buffer cache
in Linux to maintain frequently accessed data in memory.</p> <h2 id="single-master-and-large-chunk-size"><a href="#single-master-and-large-chunk-size" class="header-anchor">#</a> Single Master and Large Chunk Size</h2> <h3 id="single-master"><a href="#single-master" class="header-anchor">#</a> Single Master</h3> <p>Having a single master vastly simplifies GFS design and enables the master
to make sophisticated chunk placement and replication decisions using
global knowledge. However, GFS minimizes the master’s involvement in
reads and writes, so that it does not become a bottleneck. Clients never read
or write file data through the master. Instead, a client asks the master which
ChunkServers it should contact. The client caches this information for a
limited time and interacts with the ChunkServers directly for many
subsequent operations.</p> <p><img src="/docs/assets/img/f3.20c54b7a.png" alt="Figure 3"><br> <em>GFS's high-level architecture</em></p> <h3 id="chunk-size"><a href="#chunk-size" class="header-anchor">#</a> Chunk size</h3> <p>Chunk size is one of the key design parameters. GFS has chosen 64 MB,
which is much larger than typical filesystem block sizes (which are often
around 4KB). Here are the advantages of using a large chunk size:</p> <ol><li>Since GFS was designed to handle huge files, small chunk sizes would
not make a lot of sense, as each file would then have a map of a huge
number of chunks.</li> <li>As the master holds the metadata and manages file distribution, it is
involved whenever chunks are read, modified, or deleted. A small
chunk size would significantly increase the amount of data a master
would need to manage, and also, increase the amount of data that
would need to be communicated to a client, resulting in extra network
traffic.</li> <li>A large chunk size reduces the size of the metadata stored on the
master, which enables the master to keep all the metadata in memory,
thus significantly decreasing the latency for control operations.</li> <li>By using a large chunk size, GFS reduces the need for frequent
communication with the master to get chunk location information. It
becomes feasible for a client to cache all information related to chunk
locations of a large file. Client metadata caches have timeouts to reduce
the risk of caching stale data.</li> <li>A large chunk size also makes it possible to keep a TCP connection open
to a ChunkServer for an extended time, amortizing the time of setting
up a TCP connection.</li> <li>A large chunk size simplifies ChunkServer management, i.e., to check
which ChunkServers are near capacity or which are overloaded.</li> <li>Large chunk size provides highly efficient sequential reads and appends
of large amounts of data.</li></ol> <h3 id="lazy-space-allocation"><a href="#lazy-space-allocation" class="header-anchor">#</a> Lazy space allocation</h3> <p>Each chunk replica is stored as a plain Linux file on a ChunkServer. GFS does
not allocate the whole 64MB of disk space when creating a chunk. Instead, as
the client appends data, the ChunkServer, lazily extends the chunk. This lazy
space allocation avoids wasting space due to internal fragmentation. Internal
fragmentation refers to having unused portions of the 64 MB chunk. For
example, if we allocate a 64 MB chunk and only fill up 20MB, the remaining
space is unused</p> <p>One disadvantage of having a large chunk size is the handling of small files.
Since a small file will have one or a few chunks, the ChunkServers storing
those chunks can become hotspots if a lot of clients access the same file. To
handle this scenario, GFS stores such files with a higher replication factor
and also adds a random delay in the start times of the applications accessing
these files.</p> <h2 id="metadata"><a href="#metadata" class="header-anchor">#</a> Metadata</h2> <p>The master stores three types of metadata:</p> <ol><li>The file and chunk namespaces (i.e., directory hierarchy).</li> <li>The mapping from files to chunks.</li> <li>The locations of each chunk’s replicas.</li></ol> <p>There are three aspects of how master manages the metadata:</p> <ol><li>Master keeps all this metadata in memory.</li> <li>The first two types (i.e., namespaces and file-to-chunk mapping) are also
persisted on the master’s local disk.</li> <li>The third (i.e., chunk replicas’ locations) is not persisted.</li></ol> <h3 id="storing-metadata-in-memory"><a href="#storing-metadata-in-memory" class="header-anchor">#</a> Storing metadata in memory</h3> <p>Since metadata is stored in memory, the master operates very quickly.
Additionally, it is easy and efficient for the master to periodically scan
through its entire state in the background. This periodic scanning is used to
implement three functions:</p> <ol><li>Chunk garbage collection</li> <li>Re-replication in the case of ChunkServer failures</li> <li>Chunk migration to balance load and disk-space usage across ChunkServers</li></ol> <p>As discussed above, one potential concern for this memory-only approach is
that the number of chunks, and hence the capacity of the whole system, is
limited by how much memory the master has. This is not a serious problem
in practice. The master maintains less than 64 bytes of metadata for each 64
MB chunk. Most chunks are full because most files contain many chunks,
only the last of which may be partially filled. Similarly, the file namespace
data typically requires less than 64 bytes per file because the master stores
file names compactly using <strong>prefix compression.</strong></p> <p>If the need for supporting an even larger file system arises, the cost of adding
extra memory to the master is a small price to pay for the simplicity,
reliability, performance, and flexibility gained by storing the metadata in
memory.</p> <h3 id="chunk-location"><a href="#chunk-location" class="header-anchor">#</a> Chunk location</h3> <p>The master does not keep a persistent record of which ChunkServers have a
replica of a given chunk; instead, the master asks each chunk server about
its chunks at master startup, and whenever a ChunkServer joins the cluster.
The master can keep itself up-to-date after that because it controls all chunk
placements and monitors ChunkServer status with regular HeartBeat
messages.</p> <p>By having the ChunkServer as the ultimate source of truth of each chunk’s
location, GFS eliminates the problem of keeping the master and
ChunkServers in sync. It is not beneficial to maintain a consistent view of
chunk locations on the master, because errors on a ChunkServer may cause
chunks to vanish spontaneously (e.g., a disk may go bad and be disabled, or
ChunkServer is renamed or failed, etc.) In a cluster with hundreds of servers,
these events happen all too often.</p> <h3 id="operation-log"><a href="#operation-log" class="header-anchor">#</a> Operation log</h3> <p>The master maintains an operation log that contains the namespace and fileto-chunk mappings and stores it on the local disk. Specifically, this log stores
a historical record of all the metadata changes. Operation log is very
important to GFS. It contains the persistent record of metadata and serves as
a logical timeline that defines the order of concurrent operations.</p> <p>For fault tolerance and reliability, this operation log is replicated on multiple
remote machines, and changes to the metadata are not made visible to
clients until they have been persisted on all replicas. The master batches
several log records together before flushing, thereby reducing the impact of
flushing and replicating on overall system throughput.</p> <p>Upon restart, the master can restore its file-system state by replaying the
operation log. This log must be kept small to minimize the startup time, and
that is achieved by periodically checkpointing it.</p> <h3 id="checkpointing"><a href="#checkpointing" class="header-anchor">#</a> Checkpointing</h3> <p>Master’s state is periodically serialized to disk and then replicated, so that on
recovery, a master may load the checkpoint into memory, replay any
subsequent operations from the operation log, and be available again very
quickly. To further speed up the recovery and improve availability, GFS
stores the checkpoint in a compact B-tree like format that can be directly
mapped into memory and used for namespace lookup without extra parsing.</p> <p>The checkpoint process can take time, therefore, to avoid delaying incoming
mutations, the master switches to a new log file and creates the new
checkpoint in a separate thread. The new checkpoint includes all mutations
before the switch.</p> <h2 id="master-operations"><a href="#master-operations" class="header-anchor">#</a> Master Operations</h2> <p>The master executes all namespace operations. Furthermore, it manages
chunk replicas throughout the system. It is responsible for:</p> <ul><li>Making replica placement decisions</li> <li>Creating new chunks and hence replicas</li> <li>Making sure that chunks are fully replicated according to the replication factor</li> <li>Balancing the load across all the ChunkServers</li> <li>Reclaim unused storage</li></ul> <h3 id="namespace-management-and-locking"><a href="#namespace-management-and-locking" class="header-anchor">#</a> Namespace management and locking</h3> <p>The master acquires locks over a namespace region to ensure proper
serialization and to allow multiple operations at the master. GFS does not
have an i-node like tree structure for directories and files. Instead, it has a
hash-map that maps a filename to its metadata, and reader-writer locks are
applied on each node of the hash table for synchronization.</p> <ul><li>Each absolute file name or absolute directory name has an associated</li> <li>read-write lock.</li> <li>Each master operation acquires a set of locks before it runs.</li> <li>To make operation on <code>/dir1/dir2/leaf</code> , it first needs the following locks:
<ul><li>Reader lock on <code>/dir1</code></li> <li>Reader lock on <code>/dir1/dir2</code></li> <li>Reader or Writer lock on <code>/dir21/dir2/leaf</code></li></ul></li> <li>Following this scheme, concurrent writes on the same leaf are prevented right away. However, at the same time, concurrent modifications in the same directory are allowed.</li> <li>File creation does not require write-lock on the parent directory; a readlock on its name is sufficient to protect the parent directory from deletion, rename, or snapshot.</li> <li>Write-lock on a file name stops attempts to create multiple files with the same name.</li> <li>Locks are acquired in a consistent order to prevent <strong>deadlock</strong>:
<ul><li>First ordered by level in the namespace tree</li> <li>Lexicographically ordered within the same level</li></ul></li></ul> <h3 id="replica-placement"><a href="#replica-placement" class="header-anchor">#</a> Replica placement</h3> <p>To ensure maximum data availability and integrity, the master distributes
replicas on different racks, so that clients can still read or write in case of a
rack failure. As the in and out bandwidth of a rack may be less than the sum
of the bandwidths of individual machines, placing the data in various racks
can help clients exploit reads from multiple racks. For ‘write’ operations,
multiple racks are actually disadvantageous as data has to travel longer
distances. It is an intentional tradeoff that GFS made.</p> <h4 id="replica-creation-and-re-replication"><a href="#replica-creation-and-re-replication" class="header-anchor">#</a> Replica creation and re-replication</h4> <p>The goals of a master are to place replicas on servers with less-than-average
disk utilization, spread replicas across racks, and reduce the number of
‘recent’ creations on each ChunkServer (even though writes are cheap, they
are followed by heavy write traffic) which might create additional load.</p> <p>Chunks need to be re-replicated as soon as the number of available replicas
falls (due to data corruption on a server or a replica being unavailable)
below the user-specified replication factor. Instead of re-replicating all of
such chunks at once, the master prioritizes re-replication to prevent these
cloning operations from becoming bottlenecks. Restrictions are placed on the
bandwidth of each server for re-replication so that client requests are not
compromised.</p> <p><strong>How are chunks prioritized for re-replication?</strong></p> <ul><li>A chunk is prioritized based on how far it is from its replication goal.
For example, a chunk that has lost two replicas will be given priority on
a chuck that has lost only one replica.</li> <li>GFS prioritizes chunks of live files as opposed to chunks that belong to
recently deleted files. Deleted files are not removed
immediately; instead, they are renamed temporarily and garbage collected after a few days. Replicas of deleted files can exist for a few
days as well.</li></ul> <h4 id="replica-rebalancing"><a href="#replica-rebalancing" class="header-anchor">#</a> Replica rebalancing</h4> <p>Master rebalances replicas regularly to achieve load balancing and better
disk space usage. It may move replicas from one ChunkServer to another to
bring disk usage in a server closer to the average. Any new ChunkServer
added to the cluster is filled up gradually by the master rather than flooding
it with a heavy traffic of write operations.</p> <h3 id="stale-replica-detection"><a href="#stale-replica-detection" class="header-anchor">#</a> Stale replica detection</h3> <p>Chunk replicas may become stale if a ChunkServer fails and misses
mutations to the chunk while it is down. For each chunk, the master
maintains a chunk Version Number to distinguish between up-to-date and
stale replicas. The master increments the chunk version every time it grants
a lease (more on this later) and informs all up-to-date replicas. The master
and these replicas all record the new version number in their persistent
state. If the ChunkServer hosting a chunk replica is down during a mutation,
the chunk replica will become stale and will have an older version number.
The master will detect this when the ChunkServer restarts and reports its set
of chunks and their associated version numbers. Master removes stale
replicas during regular garbage collection.</p> <p>Stale replicas are not given to clients when they ask the master for a chunk
location, and they are not involved in mutations either. However, because a
client caches a chunk’s location, it may read from a stale replica before the
data is resynced. The impact of this is low due to the fact that most
operations to a chunk are append-only. This means that a stale replica
usually returns a premature end of a chunk rather than outdated data for a
value.</p> <h2 id="anatomy-of-a-read-operation"><a href="#anatomy-of-a-read-operation" class="header-anchor">#</a> Anatomy of a Read Operation</h2> <p>A typical read interaction with a GFS cluster by a client application goes like
this:</p> <ol><li>First, the client translates the file name and byte offset specified by the
application into a chunk index within the file. Given the fixed chunk
size, this can be computed easily.</li> <li>The client then sends the master an RPC request containing the file
name and chunk index.</li> <li>The master replies with the chunk handle and the location of replicas
holding the chunk. The client caches this metadata using the file name
and chunk-index as the key. This information is subsequently used to
access the data.</li> <li>The  client then sends a request to one of the replicas (the closest one). The request specifies the chunk handle and a byte range within that chunk.
<ul><li>Further reads of the same chunk require no more client-master interaction until the cached information expires or the file is reopened.</li> <li>In fact, the client typically asks for multiple chunks in the same request, and the master can also include the information for chunks immediately following those requested.</li></ul></li> <li>The replica ChunkServer replies with the requested data.</li> <li>As evident from the above workflow, the master is involved at the start
and is then completely out of the loop, implementing a separation of
control and data flows – a separation that is crucial for maintaining high
performance of file accesses.</li></ol> <p><img src="/docs/assets/img/f4.81407ff3.png" alt="Figure 4"><br> <em>The anatomy of a read operation</em></p> <h2 id="anatomy-of-a-write-operation"><a href="#anatomy-of-a-write-operation" class="header-anchor">#</a> Anatomy of a Write Operation</h2> <h3 id="what-is-a-chunk-lease"><a href="#what-is-a-chunk-lease" class="header-anchor">#</a> What is a chunk lease?</h3> <p>To safeguard against concurrent writes at two different replicas of a chunk,
GFS makes use of chunk lease. When a mutation (i.e., a write, append or
delete operation) is requested for a chunk, the master finds the
ChunkServers which hold that chunk and grants a chunk lease (for 60
seconds) to one of them. The server with the lease is called the primary and
is responsible for providing a serial order for all the currently pending
concurrent mutations to that chunk. There is only one lease per chunk at any
time, so that if two write requests go to the master, both see the same lease
denoting the same primary.</p> <p>Thus, a global ordering is provided by the ordering of the chunk leases
combined with the order determined by that primary. The primary can
request lease extensions if needed. When the master grants the lease, it
increments the chunk version number and informs all replicas containing
that chunk of the new version number.</p> <h3 id="data-writing"><a href="#data-writing" class="header-anchor">#</a> Data writing</h3> <p>The actual writing of data is split into two phases:</p> <ul><li><strong>Sending</strong>: First, the client is given a list of replicas that identifies the
primary ChunkServer and secondaries. The client sends the data to the
closest replica. Then replicas send the data in chain to all other replicas
to maximize bandwidth and throughput. Eventually, all the replicas get
the data, which is not yet written to a file but sits in a cache.</li> <li><strong>Writing</strong>: When the client gets an acknowledgment from all replicas that
the data has been received, it then sends a write request to the primary,
identifying the data that was sent in the previous phase. The primary is
responsible for the serialization of writes. It assigns consecutive serial
numbers to all write requests that it has received, applies the writes to
the file in serial-number order, and forwards the write requests in that
order to the secondaries. Once the primary gets acknowledgments from
all the secondaries, the primary responds back to the client, and the
write operation is complete. Any errors at any stage in this process are
met with retries and eventual failure. On failure, an error is returned to
the client.</li></ul> <p>Following is the stepwise breakdown of the data transfer:</p> <ol><li>Client asks master which chunk server holds the current lease of chunk
and locations of other replicas.</li> <li>Master replies with the identity of primary and locations of the
secondary replicas.</li> <li>Client pushes data to the closest replica. Then replicas send the data in
chain to all other replicas.</li> <li>Once all replicas have acknowledged receiving the data, the client sends
the write request to the primary. The primary assigns consecutive serial
numbers to all the mutations it receives, providing serialization. It
applies mutations in serial number order.</li> <li>Primary forwards the write request to all secondary replicas. They
apply mutations in the same serial number order.</li> <li>Secondary replicas reply to primary indicating they have completed
operation.</li> <li>Primary replies to the client with success or error message</li></ol> <p><img src="/docs/assets/img/f5.e6eefade.png" alt="Figure 5"><br> <em>The anatomy of a write operation</em></p> <p>The key point to note is that the data flow is different from the control flow.
The data flows from the client to a ChunkServer and then from that
ChunkServer to another ChunkServer, until all ChunkServers that store
replicas for that chunk have received the data. The control (the write
request) flow goes from the client to the primary ChunkServer for that
chunk. The primary then forwards the request to all the secondaries. This
ensures that the primary controls the order of writes even if it receives
multiple concurrent write requests. All replicas will have data written in the
same sequence. Chunk version numbers are used to detect if any replica has
stale data which has not been updated because that ChunkServer was down
during some update.</p> <h2 id="anatomy-of-an-append-operation"><a href="#anatomy-of-an-append-operation" class="header-anchor">#</a> Anatomy of an Append Operation</h2> <p>Record append operation is optimized in a unique way that distinguishes
GFS from other distributed file systems. In a normal write, the client
specifies the offset at which data is to be written. Concurrent writes to the
same region can experience race conditions, and the region may end up
containing data fragments from multiple clients. In a record append,
however, the client specifies only the data. GFS appends it to the file at least
once atomically (i.e., as one continuous sequence of bytes) at an offset of
GFS’s choosing and returns that offset to the client. This process is similar to
the append operation on a file opened with <a href="https://man7.org/linux/man-pages/man2/open.2.html" target="_blank" rel="noopener noreferrer">O_APPEND<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> mode on a POSIXcompliant file system but without the race conditions when multiple writers
do so concurrently.</p> <p>Record Append is a kind of mutation that changes the contents of the
metadata of a chunk. When an application tries to append data on a chunk
by sending a request to the client, the client pushes the data to all replicas of
the last chunk of the file just like the write operation. When the client
forwards the request to the master, the primary checks whether appending
the record to the existing chunk will increase the chunk’s size more than its
limit (maximum size of a chunk is 64MB). If this happens, it pads the chunk
to the maximum limit, commands the secondary to do the same, and
requests the clients to try to append to the next chunk. If the record fits
within the maximum size, the primary appends the data to its replica, tells
the secondary to write the data at the exact offset where it has, and finally
replies success to the client</p> <p>If an append operation fails at any replica, the client retries the operation.
Due to this reason, replicas of the same chunk may contain different data,
possibly including duplicates of the same record in whole or in part. GFS
does not guarantee that all replicas are byte-wise identical; instead, it only
ensures that the data is written at-least-once as an atomic unit.</p> <h2 id="gfs-consistency-model-and-snapshotting"><a href="#gfs-consistency-model-and-snapshotting" class="header-anchor">#</a> GFS Consistency Model and Snapshotting</h2> <h3 id="gfs-consistency-model"><a href="#gfs-consistency-model" class="header-anchor">#</a> GFS consistency model</h3> <p>To keep things simple and efficient, GFS has a relaxed consistency model.</p> <p>Metadata operations (e.g., file creation) are atomic. They are handled
exclusively by the master. Namespace locking guarantees atomicity and
correctness, whereas the master’s operation log defines a global total order
of these operations.</p> <p>In data mutations, there is an important distinction between <code>write</code> and
<code>append</code> operations. <code>Write</code> operations specify an offset at which mutations
should occur, whereas <code>appends</code> are always applied at the end of the file. This
means that for the <code>write</code> operation, the offset in the chunk is
predetermined, whereas for <code>append</code> , the system decides. Concurrent <code>writes</code>
to the same location are not serializable and may result in corrupted regions
of the file. With <code>append</code> operations, GFS guarantees the <code>append</code> will happen
at-least-once and atomically (that is, as a contiguous sequence of bytes). The
system does not guarantee that all copies of the chunk will be identical (some
may have duplicate data).</p> <h3 id="snapshotting"><a href="#snapshotting" class="header-anchor">#</a> Snapshotting</h3> <p>A snapshot is a copy of some subtree of the global namespace as it exists at a
given point in time. GFS clients use snapshotting to efficiently branch two
versions of the same data. Snapshots in GFS are initially <strong>zero-copy</strong>. This
means that data copies are made only when clients make a request to modify
the chunks. This scheme is known as <strong>copy-on-write</strong>.</p> <p>When the master receives a snapshot request, it first revokes any
outstanding leases on the chunks in the files to snapshot. It waits for leases
to be revoked or expired and logs the snapshot operation to the operation
log. The snapshot is then made by duplicating the metadata for the source
directory tree. Newly created snapshot files still point to the original chunks.</p> <p>When a client makes a request to write to one of these chunks, the master
detects that it is a copy-on-write chunk by examining its reference count
(which will be more than one). At this point, the master asks each
ChunkServer holding the replica to make a copy of the chunk and store it
locally. These local copies are made to avoid copying the chunk over the
network. Once the copy is complete, the master issues a lease for the new
copy, and the write proceeds.</p> <h2 id="fault-tolerance-high-availability-and-data-integrity"><a href="#fault-tolerance-high-availability-and-data-integrity" class="header-anchor">#</a> Fault Tolerance, High Availability, and Data Integrity</h2> <h3 id="fault-tolerance"><a href="#fault-tolerance" class="header-anchor">#</a> Fault tolerance</h3> <p>To make the system fault-tolerant and available, GFS makes use of two
simple strategies:</p> <ol><li><strong>Fast recovery</strong> in case of component failures.</li> <li><strong>Replication</strong> for high availability.</li></ol> <p>Let’s first see how GFS recovers from master or replica failure:</p> <ul><li><strong>On master failure</strong>: The Master being a single point of failure, can make the entire system unavailable in a short time. To handle this, all operations applied on master are saved in an <strong>operation log</strong>. This log is checkpointed and replicated on multiple remote machines, so that on recovery, a master may load the checkpoint into memory, replay any subsequent operations from the operation log, and be available again in a short amount of time. GFS relies on an external monitoring infrastructure to detect the master failure and switch the traffic to the backup master server.
<ul><li><strong>Shadow masters</strong> are replicas of master and provide read-only
access to the file system even when the primary is down. All
shadow masters keep themselves updated by applying the same
sequence of updates exactly as the primary master does by reading
its operation log. Shadow masters may lag the primary slightly, but
they enhance read availability for files that are not being actively
changed or applications that do not mind getting slightly stale
metadata. Since file contents are read from the ChunkServers,
applications do not observe stale file contents.</li></ul></li> <li><strong>On primary replica failure</strong>: If an active primary replica fails (or there
is a network partition), the master detects this failure (as there will be
no heartbeat), and waits for the current lease to expire (in case the
primary replica is still serving traffic from clients directly), and then
assigns the lease to a new node. When the old primary replica recovers,
the master will detect it as ‘stale’ by checking the version number of the
chunks. The master node will pick new nodes to replace the stale node
and garbage-collect it before it can join the group again.</li> <li><strong>On secondary replica failure</strong>: If there is a replica failure, all client
operations will start failing on it. When this happens, the client retries a
few times; if all of the retries fail, it reports failure to the master. This
can leave the secondary replica inconsistent because it misses some
mutations. As described above, stale nodes will be replaced by new
nodes picked by the master, and eventually garbage-collected.</li></ul> <div class="custom-block tip"><p class="custom-block-title">TIP</p> <p>Stale replicas might be exposed to clients. It depends on the
application programmer to deal with these stale reads. GFS does not
guarantee strong consistency on chunk reads.</p></div> <h3 id="high-availability-through-chunk-replication"><a href="#high-availability-through-chunk-replication" class="header-anchor">#</a> High availability through Chunk replication</h3> <p>As discussed earlier, each chunk is replicated on multiple ChunkServers on
different racks. Users can specify different replication levels for different
parts of the file namespace. The default is three. The master clones the
existing replicas to keep each chunk fully replicated as ChunkServers go
offline or when the master detects corrupted replicas through checksum
verification.</p> <p>A chunk is lost irreversibly only if all its replicas are lost before GFS can
react. Even in this case, the data becomes unavailable, not corrupted, which
means applications receive clear errors rather than corrupt data.</p> <h3 id="data-integrity-through-checksum"><a href="#data-integrity-through-checksum" class="header-anchor">#</a> Data integrity through checksum</h3> <p>Checksumming is used by each ChunkServer to detect the corruption of
stored data. The chunk is broken down into 64 KB blocks. Each has a
corresponding 32-bit checksum. Like other metadata, checksums are kept in
memory and stored persistently with logging, separate from user data.</p> <ol><li><strong>For reads</strong>, the ChunkServer verifies the checksum of data blocks that
overlap the read range before returning any data to the requester,
whether a client or another ChunkServer. Therefore, ChunkServers will
not propagate corruptions to other machines. If a block does not match
the recorded checksum, the ChunkServer returns an error to the
requestor and reports the mismatch to the master. In response, the
requestor will read from other replicas, and the master will clone the
chunk from another replica. After a valid new replica is in place, the master instructs the ChunkServer that reported the mismatch to delete
its replica.</li> <li><strong>For writes</strong>, ChunkServer verifies the checksum of first and last data
blocks that overlap the write range before performing the write. Then, it
computes and records the new checksums. For a corrupted block, the
ChunkServer returns an error to the requestor and reports the
mismatch to the master.</li> <li><strong>For appends</strong>, checksum computation is optimized as there is no
checksum verification on the last block; instead, just incrementally
update the checksum for the last partial block and compute new
checksums for any brand-new blocks filed by the append. This way, if
the last partial block is already corrupted (and GFS fails to detect it
now), the new checksum value will not match the stored data, and the
corruption will be detected as usual when the block is next read.</li></ol> <p>During idle periods, ChunkServers can scan and verify the contents of
inactive chunks (prevents an inactive but corrupted chunk replica from
fooling the master into thinking that it has enough valid replicas of a chunk).</p> <p>Checksumming has little effect on read performance for the following
reasons:</p> <ul><li>Since most of the reads span at least a few blocks, GFS needs to read and
checksum only a relatively small amount of extra data for verification.
GFS client code further reduces this overhead by trying to align reads at
checksum block boundaries.</li> <li>Checksum lookups and comparisons on the ChunkServer are done
without any I/O.</li> <li>Checksum calculation can often be overlapped with I/Os.</li></ul> <h2 id="garbage-collection"><a href="#garbage-collection" class="header-anchor">#</a> Garbage Collection</h2> <h3 id="garbage-collection-through-lazy-deletion"><a href="#garbage-collection-through-lazy-deletion" class="header-anchor">#</a> Garbage collection through lazy deletion</h3> <p>When a file is deleted, GFS does not immediately reclaim the physical space
used by that file. Instead, it <strong>follows a lazy garbage collection strategy.</strong>
When the client issues a delete file operation, GFS does two things:</p> <ol><li>The master logs the deletion operation just like other changes.</li> <li>The deleted file is renamed to a hidden name that also includes a
deletion timestamp.</li></ol> <p>The file can still be read under the new, special name and can also be
undeleted by renaming it back to normal. To reclaim the physical storage,
the master, while performing regular scans of the file system, removes any
such hidden files if they have existed for more than three days (this interval
is configurable) and also deletes its in-memory metadata. This lazy deletion
scheme provides a window of opportunity to a user who deleted a file by
mistake to recover the file.</p> <p>The master, while performing regular scans of chunk namespace, deletes the
metadata of all chunks that are not part of any file. Also, during the
exchange of regular HeartBeat messages with the master, each ChunkServer
reports a subset of the chunks it has, and the master replies with a list of
chunks from that subset that are no longer present in the master’s database;
such chunks are then deleted from the ChunkServer.</p> <h3 id="advantages-of-lazy-deletion"><a href="#advantages-of-lazy-deletion" class="header-anchor">#</a> Advantages of lazy deletion</h3> <p>Here are the advantages of lazy deletion.</p> <ul><li>Simple and reliable. If the chunk deletion message is lost, the master
does not have to retry. The ChunkServer can perform the garbage
collection with the subsequent heartbeat messages.</li> <li>GFS merges storage reclamation into regular background activities of
the master, such as the regular scans of the filesystem or the exchange
of HeartBeat messages. Thus, it is done in batches, and the cost is
amortized.</li> <li>Garbage collection takes place when the master is relatively free.</li> <li>Lazy deletion provides safety against accidental, irreversible deletions.</li></ul> <h3 id="disadvantages-of-lazy-deletion"><a href="#disadvantages-of-lazy-deletion" class="header-anchor">#</a> Disadvantages of lazy deletion</h3> <p>As we know, after deletion, storage space does not become available
immediately. Applications that frequently create and delete files may not be
able to reuse the storage right away. To overcome this, GFS provides
following options:</p> <ul><li>If a client deletes a deleted file again, GFS expedites the storage
reclamation.</li> <li>Users can specify directories that are to be stored without replication</li> <li>Users can also specify directories where deletion takes place
immediately.</li></ul> <h2 id="criticism-on-gfs"><a href="#criticism-on-gfs" class="header-anchor">#</a> Criticism on GFS</h2> <h3 id="problems-associated-with-single-master"><a href="#problems-associated-with-single-master" class="header-anchor">#</a> Problems associated with single master</h3> <p>As GFS has grown in usage, Google has started to see the following problems
with the centralized master scheme:</p> <ul><li>Despite the separation of control flow (i.e., metadata operations) and
data flow, the master is emerging as a bottleneck in the design. As the
number of clients grows, a single master could not serve them because
it does not have enough CPU power.</li> <li>Despite the reduced amount of metadata (because of the large chunk
size), the amount of metadata stored by the master is increasing to a
level where it is getting difficult to keep all the metadata in the main
memory.</li></ul> <h3 id="problems-associated-with-large-chunk-size"><a href="#problems-associated-with-large-chunk-size" class="header-anchor">#</a> Problems associated with large chunk size</h3> <p>Large chunk size (64MB) in GFS has its disadvantages while reading. Since a
small file will have one or a few chunks, the ChunkServers storing those
chunks can become hotspots if a lot of clients are accessing the same file. As
a workaround for this problem, GFS stores extra copies of small files for
distributing the load to multiple ChunkServers. Furthermore, GFS adds a
random delay in the start times of the applications accessing such files</p> <h2 id="summary"><a href="#summary" class="header-anchor">#</a> Summary</h2> <ul><li>GFS is a scalable distributed file storage system for large data-intensive applications.</li> <li>GFS uses commodity hardware to reduce infrastructure costs.</li> <li>GFS was designed with the understanding that system/hardware failures can and do occur.</li> <li>Reading workload consists of large streaming reads and small random reads. Writing workloads consists of many large, sequential writes that append data to files.</li> <li>GFS provides APIs for usual file operations like create, delete, open, close, read, and write. Additionally, GFS supports snapshot and record append operations. Snapshot creates a copy of the file or directory tree. Record append allows multiple clients to append data to the same file concurrently while guaranteeing atomicity.</li> <li>A GFS cluster consists of a <strong>single master</strong> and <strong>multiple ChunkServers</strong> and is accessed by multiple clients.</li> <li><strong>Chunk</strong>: Files are broken into fixed-size chunks where each chunk is 64 megabytes in size. Each chunk is identified by an immutable and globally unique <strong>64-bit chunk handle</strong> assigned by the master at the time of chunk creation.</li> <li>ChunkServers store chunks on the local disk as Linux files.</li> <li>For reliability, each chunk is replicated on multiple ChunkServers.</li> <li>Master server is the coordinator of a GFS cluster and is responsible for keeping track of all the filesystem metadata. This includes namespace, authorization, mapping of files to chunks, and the current location of chunks.</li> <li>Master keeps all metadata in memory for faster operations. For fault tolerance and to handle a master crash, all metadata changes are written to the disk onto an <strong>operation log.</strong> This operation log is also replicated onto remote machines.</li> <li>The master does not keep a persistent record of which ChunkServers have a replica of a given chunk. Instead, the master asks each
ChunkServer about what chunks it holds at master startup or whenever a ChunkServer joins the cluster.</li> <li><strong>Checkpointing</strong>: The master's state is periodically serialized to disk and then replicated so that on recovery, a master may load the checkpoint into memory, replay any subsequent operations from the operation log, and be available again very quickly.</li> <li><strong>HeartBeat</strong>: The master communicates with each ChunkServer through
Heartbeat messages to pass instructions to it and collects its state.</li> <li><strong>Client</strong>: GFS client code which is linked into each application, implements filesystem APIs, and communicates with the cluster. Clients interact with the master for metadata, but all data transfers happen directly between the client and ChunkServers.</li> <li><strong>Data Integrity</strong>: Each ChunkServer uses checksumming to detect the corruption of stored data.</li> <li><strong>Garbage Collection</strong>: After a file is deleted, GFS does not immediately reclaim the available physical storage. It does so only lazily during regular garbage collection at both the file and chunk levels.</li> <li><strong>Consistency</strong>: Master guarantees data consistency by ensuring the order of mutations on all replicas and using chunk version numbers. If a replica has an incorrect version, it is garbage collected.</li> <li>GFS guarantees <strong>at-least-once</strong> writes for writers. This means that records could be written more than once as well (although rarely). It is the responsibility of the readers to deal with these duplicate chunks.
This is achieved by having checksums and serial numbers in the chunks, which help readers to filter and discard duplicate data.</li> <li><strong>Cache</strong>: Neither the client nor the ChunkServer caches file data. Client caches offer little benefit because most applications stream through huge files or have working sets too large to be cached. However, clients do cache metadata.</li></ul> <h3 id="system-design-patterns"><a href="#system-design-patterns" class="header-anchor">#</a> System design patterns</h3> <p>Here is a summary of system design patterns used in GFS.</p> <ul><li><strong>Write-Ahead Log</strong>: For fault-tolerance and in the event of a master crash, all metadata changes are written to the disk onto an operation log which is a write-ahead log.</li> <li><strong>HeartBeat</strong>: The GFS master periodically communicates with each
ChunkServer in HeartBeat messages to give it instructions and collect its state.</li> <li><strong>Checksum</strong>: Each ChunkServer uses checksumming to detect the corruption of stored data.</li></ul></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/docs/kungfu/case-study/s3/" class="prev">
        S3-like object storage
      </a></span> <span class="next"><a href="/docs/kungfu/case-study/kafka/">
        Design Kafka
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"><!----><!----><!----></div></div>
    <script src="/docs/assets/js/app.8b37fff2.js" defer></script><script src="/docs/assets/js/2.d4412a2d.js" defer></script><script src="/docs/assets/js/54.9280baf7.js" defer></script><script src="/docs/assets/js/78.542f7f90.js" defer></script>
  </body>
</html>
